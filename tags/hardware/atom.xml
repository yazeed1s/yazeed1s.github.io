<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Yazeed&#x27;s Blog - Hardware</title>
    <subtitle>Notes on systems and low-level software.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://yazeed1s.github.io/tags/hardware/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://yazeed1s.github.io"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2026-02-05T00:00:00+00:00</updated>
    <id>https://yazeed1s.github.io/tags/hardware/atom.xml</id>
    <entry xml:lang="en">
        <title>CXL: Why Datacenter Memory is Getting a New Tier</title>
        <published>2026-02-05T00:00:00+00:00</published>
        <updated>2026-02-05T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/cxl/"/>
        <id>https://yazeed1s.github.io/posts/cxl/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/cxl/">&lt;p&gt;DRAM can be 50% of a server&#x27;s cost. And most of it sits idle. One machine thrashes while the one next to it uses 30% of its memory. CXL is supposed to fix this.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-problem&quot;&gt;the problem&lt;&#x2F;h2&gt;
&lt;p&gt;Memory in datacenters is expensive and wasted at the same time. DRAM can be 50% of server cost, and research keeps showing that utilization is terrible. One machine is thrashing because it ran out of memory while another machine next to it is sitting at 30% usage. Some papers claim 70% of aggregate memory is underutilized across a cluster.&lt;&#x2F;p&gt;
&lt;p&gt;The obvious thought: why not share memory across machines, like we do with storage. Pool it. If one server has idle memory another server could use it.&lt;&#x2F;p&gt;
&lt;p&gt;But the issue is that storage can tolerate milliseconds of latency. Memory can&#x27;t. A cache miss to local DRAM is around 100ns. Go over the network with RDMA and you&#x27;re at 1-5 microseconds. That&#x27;s 10-50x slower. For memory access patterns that&#x27;s a lot.&lt;&#x2F;p&gt;
&lt;p&gt;CXL is supposed to bridge this gap.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-it-actually-is&quot;&gt;what it actually is&lt;&#x2F;h2&gt;
&lt;p&gt;CXL stands for Compute Express Link. It runs on the PCIe physical layer, same cables and slots, but with a different protocol on top.&lt;&#x2F;p&gt;
&lt;p&gt;The interesting thing is it&#x27;s cache-coherent. The CPU can do normal load&#x2F;store to CXL-attached memory. No special APIs. No verbs like RDMA. No memory registration. The memory controller just handles it like it&#x27;s another region of memory. A different NUMA node basically.&lt;&#x2F;p&gt;
&lt;p&gt;There are three protocols in the spec. CXL.io is basically just PCIe, for device discovery and config, boring stuff. CXL.cache lets devices cache host memory, useful for accelerators. CXL.mem is the interesting one, it lets the host access device-attached memory with load&#x2F;store.&lt;&#x2F;p&gt;
&lt;p&gt;CXL 1.0 and 1.1 are mostly local expansion. You plug a CXL card with DRAM into a PCIe slot and your system sees more memory. Latency is higher than native DIMMs, maybe 200-300ns instead of 100ns, but it&#x27;s still memory, not storage. CXL 2.0 adds switching so multiple hosts can share a memory pool. CXL 3.0 goes further with fabric and shared memory semantics.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mixing-memory-types&quot;&gt;mixing memory types&lt;&#x2F;h2&gt;
&lt;p&gt;Normally your CPU&#x27;s memory controller dictates what DRAM you can use. If it&#x27;s a DDR5 system, all your DIMMs have to be DDR5. Same speed, same density rules, same timing specs. You can&#x27;t just plug DDR4 into a DDR5 slot.&lt;&#x2F;p&gt;
&lt;p&gt;CXL breaks this because the CXL device has its own memory controller. It can use whatever DRAM it wants. DDR4, DDR5, older cheaper stuff, slower but denser modules. The CPU doesn&#x27;t care. It just sees CXL memory at some address range.&lt;&#x2F;p&gt;
&lt;p&gt;So you could have local DDR5 for hot data and a CXL card with cheaper DDR4 as a slower tier. Or use high-capacity modules that wouldn&#x27;t fit your motherboard&#x27;s timing requirements. From a cost perspective this is interesting. You&#x27;re not locked into whatever generation your motherboard supports.&lt;&#x2F;p&gt;
&lt;p&gt;The tradeoff is latency. CXL adds overhead. But if you&#x27;re using it for capacity expansion rather than latency-critical paths, maybe that&#x27;s acceptable.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;why-not-just-use-rdma&quot;&gt;why not just use RDMA&lt;&#x2F;h2&gt;
&lt;p&gt;RDMA is a different model. You need explicit verbs, post work requests, poll completions. It&#x27;s not transparent load&#x2F;store. You have to register memory, pin pages, exchange keys. One-sided operations are async so you don&#x27;t know when remote writes land unless you add signaling. And latency is around 1-5μs which is fast for networking but slow for memory access patterns.&lt;&#x2F;p&gt;
&lt;p&gt;CXL at 200-500ns for pooled memory is closer to local DRAM territory. And it&#x27;s transparent to software. Your malloc can return CXL memory and the application doesn&#x27;t know the difference.&lt;&#x2F;p&gt;
&lt;p&gt;That&#x27;s the promise anyway. The hardware shipping today is mostly local expansion cards, not pooled memory. The pooling stuff is still coming hopefully.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-latency-thing&quot;&gt;the latency thing&lt;&#x2F;h2&gt;
&lt;p&gt;Local DRAM is ~100ns. CXL local expansion is ~200-300ns. CXL through a switch to a shared pool is ~500-1000ns.&lt;&#x2F;p&gt;
&lt;p&gt;So pooled CXL is 5-10x slower than local. That&#x27;s not nothing. For tight loops constantly hitting memory, that seems expensive. The pitch is that it&#x27;s still way better than swapping to SSD (100μs) and you get more capacity. Which is true.&lt;&#x2F;p&gt;
&lt;p&gt;I think the mental model is supposed to be tiering. Hot data lives in local DRAM. Warm data lives in CXL pool. Cold data goes to SSD. The kernel or some runtime migrates pages between tiers based on access patterns.&lt;&#x2F;p&gt;
&lt;p&gt;Linux already has machinery for this. NUMA balancing, DAMON for access pattern detection, tiered memory support that got merged recently. Whether this works well in practice with real workloads, I don&#x27;t know yet. The theory sounds reasonable but there will be edge cases.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shared-memory-across-hosts&quot;&gt;shared memory across hosts&lt;&#x2F;h2&gt;
&lt;p&gt;CXL 3.0 talks about multiple hosts accessing the same memory with hardware-maintained cache coherence.&lt;&#x2F;p&gt;
&lt;p&gt;This sounds amazing and also scary at the same time.&lt;&#x2F;p&gt;
&lt;p&gt;Cache coherence doesn&#x27;t scale. The distributed shared memory people learned this in the 90s. Beyond a few nodes the coherence traffic overwhelms everything.&lt;&#x2F;p&gt;
&lt;p&gt;The CXL spec people know this. The scope is limited, maybe a rack, maybe a pod, maybe less. The vision isn&#x27;t coherent memory across the whole datacenter. It&#x27;s more like, within a small group of machines you can have shared memory semantics. Beyond that you&#x27;re back to message passing or RDMA.&lt;&#x2F;p&gt;
&lt;p&gt;Even rack-scale shared memory is interesting though. Databases that want to share buffer caches across replicas. ML training jobs that share model weights. There are use cases.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-s-actually-shipping&quot;&gt;what&#x27;s actually shipping&lt;&#x2F;h2&gt;
&lt;p&gt;CXL 1.1 memory expanders exist today from Samsung, SK Hynix and others. Intel Sapphire Rapids supports CXL. These are mostly used to add capacity to memory-hungry workloads.&lt;&#x2F;p&gt;
&lt;p&gt;CXL switches are not really production-ready yet. Some prototypes. I&#x27;d guess pooled CXL deployments are 2-3 years out.&lt;&#x2F;p&gt;
&lt;p&gt;So when papers say &quot;CXL will enable this,&quot; they&#x27;re often talking about future hardware. The concepts are solid but the ecosystem is young. Worth understanding now because it&#x27;s coming, but don&#x27;t expect to deploy pooled CXL next month.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-i-m-still-uncertain-about&quot;&gt;what I&#x27;m still uncertain about&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Latency tradeoffs.&lt;&#x2F;strong&gt; 5-10x slower than local is real overhead. Better than SSD, yes. But memory-intensive applications might just thrash the CXL tier and make things worse. Tiering policies need to actually work.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Ecosystem maturity.&lt;&#x2F;strong&gt; RDMA took years to get right. CXL is newer. Drivers, kernel support, allocation policies, debugging tools, all of this needs to catch up.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Who benefits.&lt;&#x2F;strong&gt; Big cloud providers with massive memory imbalance probably see value. Smaller deployments might not see ROI at current hardware costs.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;CXL consortium includes Intel, AMD, ARM, NVIDIA, Samsung and others&lt;&#x2F;li&gt;
&lt;li&gt;Built on PCIe 5.0&#x2F;6.0 physical layer, same slots and cables&lt;&#x2F;li&gt;
&lt;li&gt;Latency numbers vary by source and topology&lt;&#x2F;li&gt;
&lt;li&gt;Linux kernel has CXL support in drivers&#x2F;cxl&#x2F;, device enumeration works, memory tiering is evolving&lt;&#x2F;li&gt;
&lt;li&gt;Related specs: Gen-Z (seems dead), CCIX (absorbed into CXL)&lt;&#x2F;li&gt;
&lt;li&gt;Good starting point: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;www.computeexpresslink.org&#x2F;&quot;&gt;CXL Consortium&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;For context on memory disaggregation: Aguilera et al., &quot;Memory disaggregation: why now and what are the challenges&quot;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
</feed>
