<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Yazeed&#x27;s Blog - RDMA</title>
    <subtitle>Notes on systems and low-level software.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://yazeed1s.github.io/tags/rdma/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://yazeed1s.github.io"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2026-01-18T00:00:00+00:00</updated>
    <id>https://yazeed1s.github.io/tags/rdma/atom.xml</id>
    <entry xml:lang="en">
        <title>Infiniswap: Remote Memory Paging Over RDMA</title>
        <published>2026-01-18T00:00:00+00:00</published>
        <updated>2026-01-18T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/infiniswap/"/>
        <id>https://yazeed1s.github.io/posts/infiniswap/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/infiniswap/">&lt;hr &#x2F;&gt;
&lt;p&gt;I came across this paper while looking into memory disaggregation. The idea is deceptively simple: when a machine runs out of RAM, instead of paging to disk, page to another machine&#x27;s unused memory over the network.&lt;&#x2F;p&gt;
&lt;p&gt;What caught my attention is &lt;em&gt;how&lt;&#x2F;em&gt; they pulled it off; no application changes, no core kernel patching. It&#x27;s a kernel module that hooks into Linux&#x27;s swap path and uses remote RAM as the fast tier, with disk as fallback.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-core-idea&quot;&gt;the core idea&lt;&#x2F;h2&gt;
&lt;p&gt;Production clusters waste a lot of memory. Some machines are memory-starved while others sit idle. The 99th percentile machine often uses 2-3× more memory than the median. Meanwhile, over half the cluster&#x27;s aggregate memory goes unused.&lt;&#x2F;p&gt;
&lt;p&gt;When applications can&#x27;t fit their working set in RAM, performance falls off a cliff. VoltDB drops from 95K TPS to 4K TPS. Memcached&#x27;s tail latency shoots up 21×. Disk is too slow to help (1000× slower than memory).&lt;&#x2F;p&gt;
&lt;p&gt;Infiniswap&#x27;s insight: RDMA networks give you single-digit microsecond latencies. That&#x27;s fast enough to make remote memory a viable swap target. Instead of page -&amp;gt; disk, you do page -&amp;gt; remote RAM over RDMA. The remote CPU stays out of the data movement; the RNIC does the DMA.&lt;&#x2F;p&gt;
&lt;p&gt;The result: swap that looks normal to Linux, but is backed by slabs of remote memory scattered across the cluster.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-s-clever&quot;&gt;what&#x27;s clever&lt;&#x2F;h2&gt;
&lt;p&gt;A few design choices stood out:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Using swap as the integration point.&lt;&#x2F;strong&gt; Instead of modifying the kernel&#x27;s page fault handler or remapping virtual memory, Infiniswap plugs into Linux&#x27;s existing swap subsystem. The kernel already knows how to page out and page back in. Infiniswap just changes where those swapped pages live. The trade-off is you&#x27;re still going through the swap path (page faults, context switches, the whole thing) but you get deployment simplicity.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;One-sided RDMA.&lt;&#x2F;strong&gt; Traditional network block devices (like Mellanox&#x27;s nbdX) use send&#x2F;recv semantics. The remote CPU has to wake up, copy data, respond. Infiniswap uses RDMA_READ and RDMA_WRITE (the RNIC directly accesses remote memory without running remote code on the critical path). The paper shows nbdX burns multiple vCPUs on the remote side; Infiniswap largely avoids that.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Slab-based design.&lt;&#x2F;strong&gt; Pages are grouped into 1GB slabs. Each slab maps to one remote machine. This keeps the metadata manageable (tracking millions of individual 4KB pages across the cluster would be expensive). When a slab gets &quot;hot&quot; (&amp;gt;20 page I&#x2F;O ops&#x2F;sec), it gets mapped to remote memory. Cold slabs stay on disk.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;where-it-works&quot;&gt;where it works&lt;&#x2F;h2&gt;
&lt;p&gt;Memory-bound workloads see big wins. Memcached stays nearly flat even when only 50% of the working set fits in memory. PowerGraph runs 6.5× faster. VoltDB, while CPU-heavy, still sees 15× throughput improvement over disk.&lt;&#x2F;p&gt;
&lt;p&gt;The cluster memory utilization goes from 40% to 60% (that&#x27;s 47% more effective use of RAM, with minimal network overhead (&amp;lt;1% of capacity)).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;where-it-doesn-t&quot;&gt;where it doesn&#x27;t&lt;&#x2F;h2&gt;
&lt;p&gt;CPU-bound workloads don&#x27;t benefit as much. VoltDB and Spark already run at high CPU utilization. Adding paging overhead (context switches, TLB flushes, page table walks) eats into that. Spark at 50% memory thrashes so badly it doesn&#x27;t complete.&lt;&#x2F;p&gt;
&lt;p&gt;The fundamental limit: this isn&#x27;t local memory. Page faults still happen. You&#x27;re masking latency, not eliminating it. For workloads where microseconds matter deterministically, that&#x27;s a problem.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Paper: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;conference&#x2F;nsdi17&#x2F;nsdi17-gu.pdf&quot;&gt;Gu et al., &quot;Efficient Memory Disaggregation with Infiniswap&quot;, NSDI 2017&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Tested on 32 machines, 56 Gbps Infiniband, 64GB RAM each&lt;&#x2F;li&gt;
&lt;li&gt;Slab placement uses &quot;power of two choices&quot; (pick two random machines, query free memory, map to the one with more headroom)&lt;&#x2F;li&gt;
&lt;li&gt;Slab eviction queries E+5 machines, evicts the coldest from that set (~363μs median)&lt;&#x2F;li&gt;
&lt;li&gt;Page-out: synchronous RDMA_WRITE + async disk write (disk is fallback if remote crashes)&lt;&#x2F;li&gt;
&lt;li&gt;Page-in: check bitmap -&amp;gt; RDMA_READ if remote, else read from disk&lt;&#x2F;li&gt;
&lt;li&gt;Slab remapping after failure takes ~54ms (Infiniband memory registration)&lt;&#x2F;li&gt;
&lt;li&gt;Default headroom threshold: 8GB per machine&lt;&#x2F;li&gt;
&lt;li&gt;Hot slab threshold: 20 page I&#x2F;O ops&#x2F;sec (EWMA, α=0.2)&lt;&#x2F;li&gt;
&lt;li&gt;Compared to: nbdX (Mellanox), Fastswap (kernel modification), LegoOS (full OS redesign)&lt;&#x2F;li&gt;
&lt;li&gt;Code available on GitHub &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;SymbioticLab&#x2F;Infiniswap&quot;&gt;SymbioticLab&#x2F;infiniswap&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>RDMA: Bypassing the Kernel for Network I&#x2F;O</title>
        <published>2025-12-28T00:00:00+00:00</published>
        <updated>2025-12-28T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/rdma/"/>
        <id>https://yazeed1s.github.io/posts/rdma/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/rdma/">&lt;hr &#x2F;&gt;
&lt;p&gt;I&#x27;ve been reading about memory disaggregation and kept running into RDMA as the enabling technology. The pitch is simple: what if your application could read and write memory on a remote machine without dragging the remote CPU into the loop?&lt;&#x2F;p&gt;
&lt;p&gt;The core concept is kernel bypass. In traditional TCP, every packet goes through the kernel&#x27;s network stack. Data gets copied (or at least touched) as it crosses user&#x2F;kernel boundaries, checksums happen, packets get queued, interrupts fire. The CPU is in the middle of basically everything.&lt;&#x2F;p&gt;
&lt;p&gt;RDMA flips this. The network card (called an RNIC) reads directly from your application&#x27;s memory buffer and sends it over the wire. The remote RNIC DMA-writes directly into the destination application&#x27;s buffer.&lt;&#x2F;p&gt;
&lt;p&gt;No kernel on the data path, no socket buffers, and (for one-sided ops) no remote syscall&#x2F;interrupt just to move bytes. (You still pay CPU to post work and handle completions, but you stop burning cycles on the kernel stack and copies.)&lt;&#x2F;p&gt;
&lt;p&gt;This is why systems like Infiniswap and modern distributed databases obsess over RDMA. Often single-digit microsecond latencies instead of tens or hundreds.&lt;&#x2F;p&gt;
&lt;p&gt;The difference is clear when you visualize the stack:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;rdma.png&quot; alt=&quot;TCP vs RDMA&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#3C3836, #D3C6AA); background-color: light-dark(#151515, #202020);&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                   TCP&#x2F;IP                                      RDMA&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;+-------------------+  +-------------------+     +-------------------+  +-------------------+&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|      SERVER       |  |      SERVER       |     |      SERVER       |  |      SERVER       |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|                   |  |                   |     |                   |  |                   |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   +-----------+   |  |   +-----------+   |     |   +-----------+   |  |   +-----------+   |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   |    App    |   |  |   |    App    |   |     |   |    App    |   |  |   |    App    |   |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   +-----+-----+   |  |   +-----+-----+   |     |   +-----+-----+   |  |   +-----+-----+   |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|         |         |  |         |         |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|         v         |  |         v         |     |         v         |  |         v         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   +-----------+   |  |   +-----------+   |     |   +-----------+   |  |   +-----------+   |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   |  Buffer   |   |  |   |  Buffer   |   |     |   |  Buffer   |   |  |   |  Buffer   |   |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   +-----+-----+   |  |   +-----+-----+   |     |   +-----+-----+   |  |   +-----+-----+   |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|         |         |  |         |         |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|         v         |  |         v         |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   +-----------+   |  |   +-----------+   |     |   (kernel bypass) |  |   (kernel bypass) |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   |  Sockets  |   |  |   |  Sockets  |   |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   +-----+-----+   |  |   +-----+-----+   |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|         |         |  |         |         |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|         v         |  |         v         |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   +-----------+   |  |   +-----------+   |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   | Transport |   |  |   | Transport |   |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   +-----+-----+   |  |   +-----+-----+   |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|         |         |  |         |         |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|         v         |  |         v         |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   +-----------+   |  |   +-----------+   |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   | NIC Drvr  |   |  |   | NIC Drvr  |   |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|   +-----+-----+   |  |   +-----+-----+   |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;|         |         |  |         |         |     |         |         |  |         |         |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;+---------+---------+  +---------+---------+     +---------+---------+  +---------+---------+&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;          |                      |                         |                      |&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;          v                      v                         v                      v&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;       [NIC]&amp;lt;-----------------&amp;gt;[NIC]                    [RNIC]&amp;lt;----------------&amp;gt;[RNIC]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-hardware&quot;&gt;the hardware&lt;&#x2F;h2&gt;
&lt;p&gt;You need an RNIC (RDMA Network Interface Card). This isn&#x27;t your regular NIC. The RNIC implements RDMA protocols in hardware—it knows how to access memory regions, validate permissions, and move data without running the kernel network stack on every packet.&lt;&#x2F;p&gt;
&lt;p&gt;Two common protocol families:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;InfiniBand (IB):&lt;&#x2F;strong&gt; The original RDMA technology. Requires special switches and cabling. Common in HPC clusters and supercomputers. Latencies under 1μs.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;RoCE (RDMA over Converged Ethernet):&lt;&#x2F;strong&gt; Runs over Ethernet. Easier to deploy if you already have Ethernet infrastructure, but in practice you usually end up treating the fabric as &quot;almost lossless&quot; (PFC&#x2F;ECN&#x2F;DCB tuning) or performance gets ugly under loss&#x2F;congestion. Slightly higher latency than InfiniBand, but close enough for most use cases.&lt;&#x2F;p&gt;
&lt;p&gt;There&#x27;s also iWARP (RDMA over TCP). It avoids the whole &quot;make Ethernet lossless&quot; dance, but it&#x27;s less common in modern deployments and often doesn&#x27;t match RoCE&#x2F;IB latency.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-programming-model&quot;&gt;the programming model&lt;&#x2F;h2&gt;
&lt;p&gt;RDMA exposes a different mental model than sockets. Instead of &lt;code&gt;send()&lt;&#x2F;code&gt; and &lt;code&gt;recv()&lt;&#x2F;code&gt;, you work with queues and memory regions. The main abstractions from &lt;code&gt;libibverbs&lt;&#x2F;code&gt; (the userspace library from &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;linux-rdma&#x2F;rdma-core&quot;&gt;rdma-core&lt;&#x2F;a&gt;):&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Queue Pair (QP):&lt;&#x2F;strong&gt; Your connection to the hardware. A QP has two queues (a send queue and a receive queue). You post work requests to these queues, and the RNIC processes them asynchronously.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#3C3836, #D3C6AA); background-color: light-dark(#151515, #202020);&quot;&gt;&lt;code data-lang=&quot;c&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;struct&lt;&#x2F;span&gt;&lt;span&gt; ibv_qp &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#707943, #A9B665);&quot;&gt;ibv_create_qp&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;struct&lt;&#x2F;span&gt;&lt;span&gt; ibv_pd &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;pd&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;                             struct&lt;&#x2F;span&gt;&lt;span&gt; ibv_qp_init_attr &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;qp_init_attr&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Creating a QP isn&#x27;t enough. Fresh QPs start in a &lt;code&gt;RESET&lt;&#x2F;code&gt; state. You have to walk them through a state machine: &lt;code&gt;RESET&lt;&#x2F;code&gt; -&amp;gt; &lt;code&gt;INIT&lt;&#x2F;code&gt; -&amp;gt; &lt;code&gt;RTR&lt;&#x2F;code&gt; (ready to receive) -&amp;gt; &lt;code&gt;RTS&lt;&#x2F;code&gt; (ready to send). Only then can data actually flow. This trips up everyone the first time.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Completion Queue (CQ):&lt;&#x2F;strong&gt; How you know an operation finished. When the RNIC completes a work request, it posts a completion entry to the CQ. You poll the CQ or wait for an interrupt.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#3C3836, #D3C6AA); background-color: light-dark(#151515, #202020);&quot;&gt;&lt;code data-lang=&quot;c&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;struct&lt;&#x2F;span&gt;&lt;span&gt; ibv_cq &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#707943, #A9B665);&quot;&gt;ibv_create_cq&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;struct&lt;&#x2F;span&gt;&lt;span&gt; ibv_context &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;context&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt; int&lt;&#x2F;span&gt;&lt;span&gt; cqe&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;                             void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span&gt;cq_context&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;                             struct&lt;&#x2F;span&gt;&lt;span&gt; ibv_comp_channel &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;channel&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;                             int&lt;&#x2F;span&gt;&lt;span&gt; comp_vector&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Two modes for processing completions:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Polling:&lt;&#x2F;strong&gt; Call &lt;code&gt;ibv_poll_cq&lt;&#x2F;code&gt; in a loop. Lowest latency, highest throughput, burns CPU.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Interrupt-based:&lt;&#x2F;strong&gt; Use &lt;code&gt;ibv_req_notify_cq&lt;&#x2F;code&gt; to arm the CQ for events. Lower CPU usage, higher latency.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;A single CQ can serve multiple QPs. This is useful for consolidating completion processing.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Memory Region (MR):&lt;&#x2F;strong&gt; Before the RNIC can touch your memory, you must register it. Registration does two things: pins the memory so it can&#x27;t be swapped to disk, and gives you keys.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#3C3836, #D3C6AA); background-color: light-dark(#151515, #202020);&quot;&gt;&lt;code data-lang=&quot;c&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;struct&lt;&#x2F;span&gt;&lt;span&gt; ibv_mr &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#707943, #A9B665);&quot;&gt;ibv_reg_mr&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;struct&lt;&#x2F;span&gt;&lt;span&gt; ibv_pd &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;pd&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt; void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span&gt;addr&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;                          size_t&lt;&#x2F;span&gt;&lt;span&gt; length&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt; int&lt;&#x2F;span&gt;&lt;span&gt; access&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The &lt;code&gt;access&lt;&#x2F;code&gt; flags matter. &lt;code&gt;IBV_ACCESS_LOCAL_WRITE&lt;&#x2F;code&gt; lets the RNIC write into this MR (e.g., receives). &lt;code&gt;IBV_ACCESS_REMOTE_READ&lt;&#x2F;code&gt; and &lt;code&gt;IBV_ACCESS_REMOTE_WRITE&lt;&#x2F;code&gt; let remote machines access it directly. The registration returns an &lt;code&gt;lkey&lt;&#x2F;code&gt; (used by the local RNIC to validate local work requests) and an &lt;code&gt;rkey&lt;&#x2F;code&gt; (presented by remote peers). You share the rkey with the other side.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Protection Domain (PD):&lt;&#x2F;strong&gt; A security boundary. QPs and MRs belong to a PD. The RNIC checks that any operation matches—you can&#x27;t use an MR with a QP from a different PD. This replaces the safety checks we lost by bypassing the kernel.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#3C3836, #D3C6AA); background-color: light-dark(#151515, #202020);&quot;&gt;&lt;code data-lang=&quot;c&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;struct&lt;&#x2F;span&gt;&lt;span&gt; ibv_pd &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#707943, #A9B665);&quot;&gt;ibv_alloc_pd&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;struct&lt;&#x2F;span&gt;&lt;span&gt; ibv_context &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#9C4641, #EA6962);&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;context&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#685C52, #9C8B7C);&quot;&gt;;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;one-sided-vs-two-sided&quot;&gt;one-sided vs two-sided&lt;&#x2F;h2&gt;
&lt;p&gt;RDMA supports both:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Two-sided (send&#x2F;receive):&lt;&#x2F;strong&gt; Traditional messaging. One side posts a send, the other posts a receive. Both CPUs are aware of the transfer. Simpler to reason about, similar to sockets.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;One-sided (RDMA read&#x2F;write):&lt;&#x2F;strong&gt; The magic. RDMA_WRITE shoves data into remote memory without the remote CPU knowing. RDMA_READ pulls data out. The remote application keeps running, oblivious. This is how you get no remote CPU involvement in the data movement.&lt;&#x2F;p&gt;
&lt;p&gt;One-sided operations need the remote memory region&#x27;s address and rkey. You typically exchange these out-of-band during connection setup.&lt;&#x2F;p&gt;
&lt;p&gt;The gotcha: &quot;oblivious&quot; doesn&#x27;t mean &quot;safe.&quot; You&#x27;re writing memory via DMA, not calling a function on the remote core. You still need a synchronization protocol (and careful ordering) so the remote side knows when it can read that memory.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;who-validates-operations&quot;&gt;who validates operations?&lt;&#x2F;h2&gt;
&lt;p&gt;If the kernel isn&#x27;t inspecting every packet, what stops bad memory accesses?&lt;&#x2F;p&gt;
&lt;p&gt;The RNIC hardware takes over. The setup&#x2F;transfer split:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Setup (kernel involved):&lt;&#x2F;strong&gt; When you call &lt;code&gt;ibv_reg_mr&lt;&#x2F;code&gt;, the kernel tells the hardware exactly which addresses are valid and what permissions apply. The hardware stores this.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Transfer (kernel asleep):&lt;&#x2F;strong&gt; During data movement, the RNIC checks every operation against those rules. If you mess up local registration&#x2F;keys&#x2F;permissions, you&#x27;ll see local errors like &lt;code&gt;IBV_WC_LOC_PROT_ERR&lt;&#x2F;code&gt;. If the remote rejects the request (bad rkey&#x2F;permissions or invalid remote address), you&#x27;ll see a remote error like &lt;code&gt;IBV_WC_REM_ACCESS_ERR&lt;&#x2F;code&gt; or &lt;code&gt;IBV_WC_REM_INV_REQ_ERR&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The hardware also handles corruption. It checks CRCs, drops bad packets, requests retries. You get reliable delivery without TCP&#x27;s overhead.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;where-rdma-wins&quot;&gt;where rdma wins&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Latency-critical paths:&lt;&#x2F;strong&gt; Sub-microsecond matters when you&#x27;re doing millions of small operations per second.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;High-throughput bulk transfers:&lt;&#x2F;strong&gt; The CPU isn&#x27;t the bottleneck when the RNIC handles everything.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Memory disaggregation:&lt;&#x2F;strong&gt; Systems like Infiniswap swap pages to remote memory over RDMA. Works because you&#x27;re paying microseconds over the fabric instead of milliseconds to disk.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Distributed databases and KV stores:&lt;&#x2F;strong&gt; RAMCloud, FaRM, and others build on RDMA for fast replication and reads.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;where-it-hurts&quot;&gt;where it hurts&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Programming complexity:&lt;&#x2F;strong&gt; The queue&#x2F;completion model is harder than sockets. State machines, pinned memory, keys—there&#x27;s a lot to get right.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Debugging is painful:&lt;&#x2F;strong&gt; Problems manifest as cryptic completion status codes. No tcpdump. Tools are improving but still rough.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Hardware cost:&lt;&#x2F;strong&gt; RNICs and InfiniBand switches aren&#x27;t cheap. RoCE helps if you already have decent Ethernet.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Not worth it for large, infrequent transfers:&lt;&#x2F;strong&gt; If you&#x27;re sending a few big blobs over slow intervals, TCP&#x27;s overhead is negligible and the simplicity wins.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Paper: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;nsdi22-paper-reda_1.pdf&quot;&gt;RDMA is Turing complete, we just did not know it yet!&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;RDMA&#x27;s &quot;zero-copy&quot; means the RNIC reads directly from your user-space buffer. No kernel buffer, no extra copy. The CPU does zero memory-copy work for the transfer itself.&lt;&#x2F;li&gt;
&lt;li&gt;Memory registration (&lt;code&gt;ibv_reg_mr&lt;&#x2F;code&gt;) pins pages via the kernel&#x2F;driver so the RNIC can DMA safely. Large registrations can hit &lt;code&gt;ulimit -l&lt;&#x2F;code&gt; &#x2F; &lt;code&gt;RLIMIT_MEMLOCK&lt;&#x2F;code&gt; limits.&lt;&#x2F;li&gt;
&lt;li&gt;QP state transitions are required because each state enables different capabilities and checks. You can&#x27;t skip steps.&lt;&#x2F;li&gt;
&lt;li&gt;Scatter&#x2F;gather elements (SGEs) let you describe non-contiguous memory in a single work request. They always point to local memory.&lt;&#x2F;li&gt;
&lt;li&gt;One application can have multiple QPs and CQs. CQs aren&#x27;t 1:1 with queues—flexibility for different polling strategies.&lt;&#x2F;li&gt;
&lt;li&gt;Atomic operations (compare-and-swap, fetch-and-add) are also available for lock-free distributed algorithms.&lt;&#x2F;li&gt;
&lt;li&gt;libibverbs man pages: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;linux-rdma&#x2F;rdma-core&#x2F;blob&#x2F;master&#x2F;libibverbs&#x2F;man&#x2F;ibv_create_qp.3&quot;&gt;ibv_create_qp&lt;&#x2F;a&gt;, &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;linux-rdma&#x2F;rdma-core&#x2F;blob&#x2F;master&#x2F;libibverbs&#x2F;man&#x2F;ibv_reg_mr.3&quot;&gt;ibv_reg_mr&lt;&#x2F;a&gt;, &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;linux-rdma&#x2F;rdma-core&#x2F;blob&#x2F;master&#x2F;libibverbs&#x2F;man&#x2F;ibv_create_cq.3&quot;&gt;ibv_create_cq&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
</feed>
