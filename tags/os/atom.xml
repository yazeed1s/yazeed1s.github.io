<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Yazeed&#x27;s Blog - OS</title>
    <subtitle>Notes on systems and low-level software.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://yazeed1s.github.io/tags/os/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://yazeed1s.github.io"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2026-02-16T00:00:00+00:00</updated>
    <id>https://yazeed1s.github.io/tags/os/atom.xml</id>
    <entry xml:lang="en">
        <title>Should malloc Know About Tiered Memory?</title>
        <published>2026-02-16T00:00:00+00:00</published>
        <updated>2026-02-16T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/malloc-to-teired/"/>
        <id>https://yazeed1s.github.io/posts/malloc-to-teired/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/malloc-to-teired/">&lt;p&gt;When you call &lt;code&gt;malloc()&lt;&#x2F;code&gt;, the allocator gives you a pointer. It doesn&#x27;t know or care whether the physical page behind it sits in fast local DRAM or slower CXL-attached memory. From user space, memory still looks flat. But it isn&#x27;t anymore. Machines now have 2â€“3x latency differences between memory tiers, and the allocator is completely blind to that.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;how-glibc-malloc-sees-the-world&quot;&gt;how glibc malloc sees the world&lt;&#x2F;h2&gt;
&lt;p&gt;glibc&#x27;s allocator (ptmalloc2) was designed in a mostly uniform DRAM world. It manages arenas, splits and coalesces chunks, decides when to use &lt;code&gt;brk&lt;&#x2F;code&gt; and when to use &lt;code&gt;mmap&lt;&#x2F;code&gt;, and tries to reduce lock contention between threads. But it doesn&#x27;t care about which NUMA node backs an allocation unless the application explicitly asks for it. In the common case, it just requests virtual memory and leaves physical placement to the kernel.&lt;&#x2F;p&gt;
&lt;p&gt;So from the allocator&#x27;s perspective, memory is virtual address space. It doesn&#x27;t know whether the physical pages will come from local DRAM, remote NUMA, CXL-attached memory, or something else. That blindness was perfectly reasonable when latency differences were small and mostly about bandwidth balancing. The allocator could afford to ignore placement because the hardware was close to uniform.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-tiered-memory-changes&quot;&gt;what tiered memory changes&lt;&#x2F;h2&gt;
&lt;p&gt;In tiered memory systems, the kernel often treats slow memory as another NUMA node. It may demote cold pages to slow memory and promote hot pages back to fast DRAM. Research systems like TPP migrate pages based on observed access frequency, and Memtis tries to improve classification by looking at access distribution and even splitting huge pages when access inside them is skewed.&lt;&#x2F;p&gt;
&lt;p&gt;But the pattern is always the same: allocate first, observe later, migrate if needed. The allocator places data somewhere, the kernel watches page faults or samples accesses, then corrects the placement. We&#x27;re always reacting.&lt;&#x2F;p&gt;
&lt;p&gt;Migration isn&#x27;t free. It involves copying 4KB pages, updating page tables, invalidating TLB entries, and potentially disturbing caches. Work like M5 shows that misclassification and migration overhead can actually hurt performance if not handled carefully. So you&#x27;re paying a correction cost because the initial allocation was blind. I keep wondering how much of this cost could be avoided if the allocator had any information at all about what&#x27;s hot.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-allocator-knows-nothing-about-temperature&quot;&gt;the allocator knows nothing about temperature&lt;&#x2F;h2&gt;
&lt;p&gt;Consider a simple program:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;c&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span&gt;hot_table &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt; malloc&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; &amp;lt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 20&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt;   &#x2F;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt; frequently accessed&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span&gt;log_buffer &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt; malloc&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; &amp;lt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 20&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt;  &#x2F;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt; rarely accessed&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span&gt;archive &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt; malloc&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;100&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; &amp;lt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 20&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt;   &#x2F;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt; mostly cold&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;From glibc&#x27;s perspective, these are identical calls. Same API, same path. But their temperature is completely different. The allocator has no way to express or detect that difference.&lt;&#x2F;p&gt;
&lt;p&gt;The application often already knows which data is critical. A database knows its buffer pool is hot. A web server knows which structures sit in the request path. A compiler knows which tables are heavily reused. Yet we force the kernel to guess using access bits and heuristics.&lt;&#x2F;p&gt;
&lt;p&gt;That naturally leads to the question: are we solving the problem too late?&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;should-malloc-become-tier-aware&quot;&gt;should malloc become tier-aware?&lt;&#x2F;h2&gt;
&lt;p&gt;One idea, not that exotic: let the allocator express intent.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;c&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;malloc_hot&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;size_t&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7D4242, #D4D4C0);&quot;&gt; size&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;malloc_cold&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;size_t&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7D4242, #D4D4C0);&quot;&gt; size&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Internally, &lt;code&gt;malloc_hot()&lt;&#x2F;code&gt; could bind memory to the fast NUMA node using mechanisms that already exist, like &lt;code&gt;mbind()&lt;&#x2F;code&gt; or &lt;code&gt;set_mempolicy()&lt;&#x2F;code&gt;. &lt;code&gt;malloc_cold()&lt;&#x2F;code&gt; could allocate directly on the slow tier. Instead of allocate -&amp;gt; detect -&amp;gt; migrate, you&#x27;d allocate correctly from the start.&lt;&#x2F;p&gt;
&lt;p&gt;This avoids some migration entirely. Fewer TLB shootdowns, less page copying. Placement becomes a proactive decision rather than a reactive correction.&lt;&#x2F;p&gt;
&lt;p&gt;But now the deeper question comes up.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;is-os-transparency-still-sacred&quot;&gt;is OS transparency still sacred?&lt;&#x2F;h2&gt;
&lt;p&gt;Virtual memory was designed to hide physical placement. That abstraction is powerful because developers don&#x27;t need to care where bytes live. They just allocate and use.&lt;&#x2F;p&gt;
&lt;p&gt;Tiered memory challenges that. When latency differences become large enough, placement starts to matter again.&lt;&#x2F;p&gt;
&lt;p&gt;You can keep full transparency. The kernel observes access patterns and tries to infer temperature. Developers stay insulated. The system grows more complex internally, with more sampling and migration.&lt;&#x2F;p&gt;
&lt;p&gt;Or you can leak some abstraction. Let developers label allocations as hot or cold. Trust applications to express intent, and let the allocator participate in placement.&lt;&#x2F;p&gt;
&lt;p&gt;I honestly don&#x27;t know which is better. The second approach sounds cleaner until you think about what happens when developers misclassify. What if everything gets labeled &quot;fast&quot;? Do you override their hints? Ignore them? Once you expose placement, you also expose responsibility, and most application developers probably don&#x27;t want that.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;huge-pages-make-it-worse&quot;&gt;huge pages make it worse&lt;&#x2F;h2&gt;
&lt;p&gt;Memtis shows that access inside a 2MB huge page can be highly skewed. Promoting an entire huge page to fast memory because a small region is hot wastes precious capacity. The allocator doesn&#x27;t know how its allocations align with huge pages. The kernel may split or merge them later.&lt;&#x2F;p&gt;
&lt;p&gt;So page size, allocation strategy, and tier placement are all interacting and I&#x27;m not sure anyone has a clean model for how they should interact. The original layering between allocator and kernel assumed these things were independent. They&#x27;re not anymore.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;a-possible-middle-ground&quot;&gt;a possible middle ground&lt;&#x2F;h2&gt;
&lt;p&gt;I don&#x27;t think we should abandon OS transparency completely. It&#x27;s still valuable, especially for most applications that don&#x27;t care about deep performance tuning. But maybe transparency should become adjustable.&lt;&#x2F;p&gt;
&lt;p&gt;By default, memory stays abstract. The kernel handles tiering. But for performance-critical systems, the allocator could expose controlled hints, and the kernel could enforce limits so that misclassification doesn&#x27;t destabilize things.&lt;&#x2F;p&gt;
&lt;p&gt;I don&#x27;t know if this would hold up in real systems. It depends on how well the kernel can override bad hints, and on whether developers will bother annotating allocations. My guess is most people ignore it and a small group gets real value out of it.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>What a Process Really Owns</title>
        <published>2026-01-05T00:00:00+00:00</published>
        <updated>2026-01-05T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/what-a-process-owns/"/>
        <id>https://yazeed1s.github.io/posts/what-a-process-owns/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/what-a-process-owns/">&lt;p&gt;I wanted to think through what a process actually &quot;owns&quot; because I realized I had a fuzzy picture of it. We throw around words like &quot;process&quot;, &quot;thread&quot;, &quot;address space&quot; but I wanted to be more concrete about what the kernel actually tracks.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-short-version&quot;&gt;the short version&lt;&#x2F;h2&gt;
&lt;p&gt;A process owns:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Its own virtual address space&lt;&#x2F;li&gt;
&lt;li&gt;File descriptors&lt;&#x2F;li&gt;
&lt;li&gt;Signal handlers&lt;&#x2F;li&gt;
&lt;li&gt;Resource limits&lt;&#x2F;li&gt;
&lt;li&gt;A process ID and various metadata&lt;&#x2F;li&gt;
&lt;li&gt;One or more threads of execution&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;When you fork(), the child gets copies of most of these. When you exec(), most of it gets replaced with fresh stuff for the new program.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;address-space&quot;&gt;address space&lt;&#x2F;h2&gt;
&lt;p&gt;This is the big one. Every process gets its own virtual address space. On a 64-bit system that&#x27;s a huge range of addresses (48 bits usable on x86-64, so 256 TB of virtual space).&lt;&#x2F;p&gt;
&lt;p&gt;The address space contains:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Text segment&lt;&#x2F;strong&gt;: the executable code, usually read-only&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Data segment&lt;&#x2F;strong&gt;: initialized global&#x2F;static variables&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;BSS&lt;&#x2F;strong&gt;: uninitialized globals (zeroed on startup)&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Heap&lt;&#x2F;strong&gt;: dynamic allocations (malloc, new)&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Stack&lt;&#x2F;strong&gt;: function call frames, local variables&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Memory-mapped regions&lt;&#x2F;strong&gt;: shared libraries, mmap&#x27;d files, anonymous mappings&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The kernel tracks all this in data structures (on Linux, the &lt;code&gt;mm_struct&lt;&#x2F;code&gt; and a tree of &lt;code&gt;vm_area_struct&lt;&#x2F;code&gt;). Each region has permissions (read&#x2F;write&#x2F;execute) and backing (file, anonymous, shared, private).&lt;&#x2F;p&gt;
&lt;p&gt;You can see your process&#x27;s memory map in &lt;code&gt;&#x2F;proc&#x2F;self&#x2F;maps&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;shellscript&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;$&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; cat&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; &#x2F;proc&#x2F;self&#x2F;maps&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;55a4c8a00000-55a4c8a02000&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; r--p&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 00000000&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; 08:01&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 1234567&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt;  &#x2F;usr&#x2F;bin&#x2F;cat&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;55a4c8a02000-55a4c8a06000&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; r-xp&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 00002000&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; 08:01&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 1234567&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt;  &#x2F;usr&#x2F;bin&#x2F;cat&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;55a4c8a06000-55a4c8a09000&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; r--p&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 00006000&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; 08:01&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 1234567&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt;  &#x2F;usr&#x2F;bin&#x2F;cat&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;7f8c12000000-7f8c12021000&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; rw-p&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 00000000&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; 00:00&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 0&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;7ffd5c9e0000-7ffd5ca01000&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; rw-p&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 00000000&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; 00:00&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 0&lt;&#x2F;span&gt;&lt;span&gt;        [stack&lt;&#x2F;span&gt;&lt;span&gt;]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Each line is a region with its address range, permissions, offset, device, inode, and path.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;file-descriptors&quot;&gt;file descriptors&lt;&#x2F;h2&gt;
&lt;p&gt;A process has a table of open file descriptors. These are just small integers (0, 1, 2, 3, ...) that refer to open files, pipes, sockets, devices, whatever.&lt;&#x2F;p&gt;
&lt;p&gt;By convention:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;0 = stdin&lt;&#x2F;li&gt;
&lt;li&gt;1 = stdout&lt;&#x2F;li&gt;
&lt;li&gt;2 = stderr&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;When you open() a file, you get the lowest available fd number. When you fork(), the child inherits copies of the parent&#x27;s fd table (pointing to the same underlying file objects, so they share the file offset).&lt;&#x2F;p&gt;
&lt;p&gt;The kernel tracks this in a &lt;code&gt;files_struct&lt;&#x2F;code&gt;. Each fd points to a &lt;code&gt;file&lt;&#x2F;code&gt; object which points to an inode.&lt;&#x2F;p&gt;
&lt;p&gt;You can see a process&#x27;s fds in &lt;code&gt;&#x2F;proc&#x2F;[pid]&#x2F;fd&#x2F;&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;shellscript&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;$&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; ls&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; -&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;la&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; &#x2F;proc&#x2F;self&#x2F;fd&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;lrwx------&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; user&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; user&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 64&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; Jan&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;  1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; 00:00&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 0&lt;&#x2F;span&gt;&lt;span&gt; -&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; &#x2F;dev&#x2F;pts&#x2F;0&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;lrwx------&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; user&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; user&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 64&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; Jan&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;  1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; 00:00&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 1&lt;&#x2F;span&gt;&lt;span&gt; -&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; &#x2F;dev&#x2F;pts&#x2F;0&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;lrwx------&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; user&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; user&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 64&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; Jan&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;  1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; 00:00&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 2&lt;&#x2F;span&gt;&lt;span&gt; -&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; &#x2F;dev&#x2F;pts&#x2F;0&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;lr-x------&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; user&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; user&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 64&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; Jan&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;  1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; 00:00&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 3&lt;&#x2F;span&gt;&lt;span&gt; -&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; &#x2F;proc&#x2F;12345&#x2F;fd&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;credentials-and-ids&quot;&gt;credentials and IDs&lt;&#x2F;h2&gt;
&lt;p&gt;A process has:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PID&lt;&#x2F;strong&gt;: unique process ID&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;PPID&lt;&#x2F;strong&gt;: parent process ID&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;UID&#x2F;GID&lt;&#x2F;strong&gt;: user and group IDs (real, effective, saved)&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Process group and session IDs&lt;&#x2F;strong&gt;: for job control&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The UID&#x2F;GID stuff is more complicated than I expected. There&#x27;s the real UID (who you actually are), effective UID (what permissions you&#x27;re currently using), and saved UID (so you can drop and regain privileges). This is how setuid programs work.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;signal-handling&quot;&gt;signal handling&lt;&#x2F;h2&gt;
&lt;p&gt;A process has a table of signal handlers. Each signal (SIGINT, SIGTERM, SIGSEGV, etc.) can have:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Default action (terminate, ignore, stop, etc.)&lt;&#x2F;li&gt;
&lt;li&gt;A custom handler function&lt;&#x2F;li&gt;
&lt;li&gt;Ignored&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Plus there&#x27;s a signal mask (which signals are currently blocked) and pending signals (delivered but not yet handled).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;resource-limits&quot;&gt;resource limits&lt;&#x2F;h2&gt;
&lt;p&gt;Every process has limits on things like:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Maximum file size it can create&lt;&#x2F;li&gt;
&lt;li&gt;Number of open files&lt;&#x2F;li&gt;
&lt;li&gt;Stack size&lt;&#x2F;li&gt;
&lt;li&gt;CPU time&lt;&#x2F;li&gt;
&lt;li&gt;Memory&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;You can see these with &lt;code&gt;ulimit -a&lt;&#x2F;code&gt; in bash. The kernel enforces these.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;threads&quot;&gt;threads&lt;&#x2F;h2&gt;
&lt;p&gt;Here&#x27;s where it gets a bit confusing. On Linux, threads are really just processes that share stuff. When you create a thread (via clone() with the right flags), the new &quot;thread&quot; shares the address space, file descriptors, signal handlers, etc. with the parent.&lt;&#x2F;p&gt;
&lt;p&gt;So a &quot;process&quot; might have multiple threads, and they all share most of the resources I listed above. But each thread has its own:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Thread ID&lt;&#x2F;li&gt;
&lt;li&gt;Stack&lt;&#x2F;li&gt;
&lt;li&gt;Register state&lt;&#x2F;li&gt;
&lt;li&gt;Signal mask&lt;&#x2F;li&gt;
&lt;li&gt;errno&lt;&#x2F;li&gt;
&lt;li&gt;Thread-local storage&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The kernel calls these &quot;tasks&quot; internally. A process is really a group of tasks that share an address space.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;pcb-and-tcb&quot;&gt;PCB and TCB&lt;&#x2F;h2&gt;
&lt;p&gt;So all this stuff I&#x27;ve been talking about, the kernel has to store it somewhere. That&#x27;s where the Process Control Block (PCB) and Thread Control Block (TCB) come in.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Process Control Block (PCB)&lt;&#x2F;strong&gt; is a kernel data structure that holds everything about a process. On Linux this is the &lt;code&gt;task_struct&lt;&#x2F;code&gt; (confusingly named since it&#x27;s used for both processes and threads). It contains:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Process state (running, sleeping, stopped, zombie)&lt;&#x2F;li&gt;
&lt;li&gt;PID, PPID, credentials&lt;&#x2F;li&gt;
&lt;li&gt;Pointers to the memory management structures (&lt;code&gt;mm_struct&lt;&#x2F;code&gt;)&lt;&#x2F;li&gt;
&lt;li&gt;Pointer to the file descriptor table (&lt;code&gt;files_struct&lt;&#x2F;code&gt;)&lt;&#x2F;li&gt;
&lt;li&gt;Signal handling info&lt;&#x2F;li&gt;
&lt;li&gt;Scheduling info (priority, time slice, CPU affinity)&lt;&#x2F;li&gt;
&lt;li&gt;Accounting info (CPU time used, etc.)&lt;&#x2F;li&gt;
&lt;li&gt;Pointers to parent, children, siblings in the process tree&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;When the scheduler picks a process to run, it reads from the PCB to restore the process&#x27;s context. When the kernel needs to check permissions or deliver a signal, it looks at the PCB.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Thread Control Block (TCB)&lt;&#x2F;strong&gt; stores the execution context for a specific thread. On Linux this is also in &lt;code&gt;task_struct&lt;&#x2F;code&gt; since threads and processes use the same structure. But the thread-specific stuff includes:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Register state (saved during context switch)&lt;&#x2F;li&gt;
&lt;li&gt;Stack pointer&lt;&#x2F;li&gt;
&lt;li&gt;Thread ID&lt;&#x2F;li&gt;
&lt;li&gt;Signal mask (which signals this thread blocks)&lt;&#x2F;li&gt;
&lt;li&gt;Thread-local storage pointer&lt;&#x2F;li&gt;
&lt;li&gt;errno location&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The key difference is that multiple TCBs (threads) can point to the same memory management structures and file descriptor tables, while separate PCBs (processes) have their own.&lt;&#x2F;p&gt;
&lt;p&gt;When you do a context switch, the kernel saves the current thread&#x27;s register state into its TCB, then loads the next thread&#x27;s state from its TCB. If the threads belong to different processes, it also has to switch page tables (which is the expensive part).&lt;&#x2F;p&gt;
&lt;p&gt;You can think of it like this: the PCB holds &quot;what resources does this process own&quot; while the TCB holds &quot;where is this thread in its execution&quot;. A single-threaded process has one PCB and effectively one TCB. A multi-threaded process has one PCB (shared resources) and multiple TCBs (one per thread).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-happens-on-fork&quot;&gt;what happens on fork()&lt;&#x2F;h2&gt;
&lt;p&gt;When you fork():&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Address space is copied (or copy-on-write, so it&#x27;s cheap until you modify things)&lt;&#x2F;li&gt;
&lt;li&gt;File descriptors are copied (but point to the same file objects)&lt;&#x2F;li&gt;
&lt;li&gt;Signal handlers are copied&lt;&#x2F;li&gt;
&lt;li&gt;The child gets a new PID&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The child is almost identical to the parent at the moment of fork. This is why fork+exec is the traditional Unix way to spawn programs: fork copies everything, then exec replaces it all with a new program.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-i-find-interesting&quot;&gt;what I find interesting&lt;&#x2F;h2&gt;
&lt;p&gt;The address space isolation is the core thing. That&#x27;s what makes a process a process. Everything else (fds, credentials, signals) is just bookkeeping.&lt;&#x2F;p&gt;
&lt;p&gt;Threads share the address space but are otherwise separate schedulable entities. This is why data races are possible with threads but not between processes (unless you explicitly share memory).&lt;&#x2F;p&gt;
&lt;p&gt;The &#x2F;proc filesystem is amazing for introspection. You can see almost everything about a running process without any special tools. Just cat files.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;On Linux, look at &lt;code&gt;task_struct&lt;&#x2F;code&gt; in the kernel source to see what the kernel tracks per task&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;mm_struct&lt;&#x2F;code&gt; holds the memory management info&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;files_struct&lt;&#x2F;code&gt; holds the fd table&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;signal_struct&lt;&#x2F;code&gt; and &lt;code&gt;sighand_struct&lt;&#x2F;code&gt; handle signals&lt;&#x2F;li&gt;
&lt;li&gt;The clone() syscall lets you pick exactly what to share between parent and child, which is how threads are implemented&lt;&#x2F;li&gt;
&lt;li&gt;cgroups and namespaces add more layers of isolation on top of this (for containers)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;If you want to poke around a running process, check &lt;code&gt;&#x2F;proc&#x2F;[pid]&#x2F;status&lt;&#x2F;code&gt; (state, memory, threads), &lt;code&gt;&#x2F;proc&#x2F;[pid]&#x2F;maps&lt;&#x2F;code&gt; (memory map), &lt;code&gt;&#x2F;proc&#x2F;[pid]&#x2F;fd&lt;&#x2F;code&gt; (open file descriptors), &lt;code&gt;&#x2F;proc&#x2F;[pid]&#x2F;limits&lt;&#x2F;code&gt; (resource limits), &lt;code&gt;&#x2F;proc&#x2F;[pid]&#x2F;task&#x2F;&lt;&#x2F;code&gt; (threads). Also &lt;code&gt;ulimit -a&lt;&#x2F;code&gt; for your shell&#x27;s limits and &lt;code&gt;grep ctxt &#x2F;proc&#x2F;self&#x2F;status&lt;&#x2F;code&gt; for context switch counts.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>A Buffer Pool Is Just Paging in User Space</title>
        <published>2025-12-02T00:00:00+00:00</published>
        <updated>2025-12-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/buffer-pools/"/>
        <id>https://yazeed1s.github.io/posts/buffer-pools/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/buffer-pools/">&lt;p&gt;A database buffer pool manages fixed-size pages in memory, decides which ones to keep and which to evict, tracks dirty pages, and writes them back to disk on its own schedule.&lt;&#x2F;p&gt;
&lt;p&gt;That&#x27;s what the OS virtual memory system does. Page frames, page tables, eviction policies, dirty bit tracking, write-back. The database reimplements all of it. In user space.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-os-already-does-this&quot;&gt;the OS already does this&lt;&#x2F;h2&gt;
&lt;p&gt;The kernel manages physical memory in page frames (4KB). It maps virtual pages to physical frames through page tables. When memory is full, it evicts cold pages to disk. When a process touches an evicted page, it faults and the kernel loads it back. It tracks which pages are dirty and writes them back when it needs to.&lt;&#x2F;p&gt;
&lt;p&gt;This is the exact same problem a database has. The database has pages on disk. Some of them need to be in memory. Not all of them fit. The database needs to decide which pages to keep, which to evict, and when to write dirty ones back.&lt;&#x2F;p&gt;
&lt;p&gt;So why not just let the OS handle it? Map the database file with mmap and let the kernel manage everything. Some databases tried this. &lt;a href=&quot;https:&#x2F;&#x2F;yazeed1s.github.io&#x2F;posts&#x2F;mmap-databases&#x2F;&quot;&gt;It didn&#x27;t go well&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;why-databases-reimplement-it&quot;&gt;why databases reimplement it&lt;&#x2F;h2&gt;
&lt;p&gt;The OS page cache is general purpose. It has no concept of index pages vs temporary sort pages, no awareness that a range scan is about to need the next 50 pages, and no understanding that a dirty page has to hit the WAL before it hits the data file.&lt;&#x2F;p&gt;
&lt;p&gt;A buffer pool knows all of that.&lt;&#x2F;p&gt;
&lt;p&gt;The database builds its own page table: a hash map from &lt;code&gt;(file_id, page_number)&lt;&#x2F;code&gt; to a frame in the buffer pool. When a query needs a page, it looks up the hash map. If the page is there, it returns the pointer. If not, it picks a frame to evict, reads the page from disk into that frame, and updates the map.&lt;&#x2F;p&gt;
&lt;p&gt;Page fault, but in user space. Controlled entirely by the database.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-anatomy-of-a-buffer-pool&quot;&gt;the anatomy of a buffer pool&lt;&#x2F;h2&gt;
&lt;p&gt;The structure is simple:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Frame array&lt;&#x2F;strong&gt;: a fixed-size array of page-sized slots in memory (the &quot;RAM&quot; of the buffer pool).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Page table&lt;&#x2F;strong&gt;: a hash map from page ID to frame index (how the database translates a logical page reference into a memory location).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Eviction policy&lt;&#x2F;strong&gt;: decides which frame to reclaim when the pool is full (LRU, clock, LRU-K).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Dirty flag&lt;&#x2F;strong&gt;: each frame tracks whether its contents have been modified since it was read from disk.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Pin count&lt;&#x2F;strong&gt;: tracks how many operations are currently using a frame. A pinned page can&#x27;t be evicted (same idea as the kernel&#x27;s page reference count).&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;When a page is requested:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Check the page table. If the page is already in a frame, pin it and return the pointer.&lt;&#x2F;li&gt;
&lt;li&gt;If not, find a victim frame (eviction policy). If the victim is dirty, write it to disk first.&lt;&#x2F;li&gt;
&lt;li&gt;Read the requested page from disk into the victim frame.&lt;&#x2F;li&gt;
&lt;li&gt;Update the page table. Return the pointer.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;That&#x27;s page fault, find victim, write back if dirty, read page, update mapping. Same flow as an OS page fault handler, different layer.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;eviction-the-database-knows-more&quot;&gt;eviction: the database knows more&lt;&#x2F;h2&gt;
&lt;p&gt;The OS uses something like clock or a modified LRU. It works across all processes, all files, all pages. It has no application-level knowledge.&lt;&#x2F;p&gt;
&lt;p&gt;A database can do better because it knows the access patterns.&lt;&#x2F;p&gt;
&lt;p&gt;A sequential scan will touch every page once. An LRU policy would fill the cache with scan pages and evict hot index pages. PostgreSQL handles this by using a small ring buffer for sequential scans, so scan pages cycle through a handful of frames instead of polluting the whole pool.&lt;&#x2F;p&gt;
&lt;p&gt;A B+tree lookup traverses root, internal, then leaf. The root page is accessed on every lookup. It should basically never be evicted. LRU handles this naturally, but a database can also pin critical pages explicitly.&lt;&#x2F;p&gt;
&lt;p&gt;Prefetching works better too. The database knows it&#x27;s doing a range scan on a B+tree. It can issue async reads for the next few leaf pages before it needs them. The OS page cache can&#x27;t do this because it only sees physical file offsets, not logical access patterns.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;dirty-pages-and-write-back&quot;&gt;dirty pages and write-back&lt;&#x2F;h2&gt;
&lt;p&gt;This is where the difference matters most.&lt;&#x2F;p&gt;
&lt;p&gt;The OS can flush a dirty page to disk whenever it wants. That&#x27;s fine for normal files. For a database, it&#x27;s dangerous. If a modified data page hits disk before the corresponding WAL record, crash recovery breaks. This is the write-ahead logging rule: log first, data page second.&lt;&#x2F;p&gt;
&lt;p&gt;A buffer pool enforces this. Before writing a dirty page back to disk, it checks that the WAL has been flushed up to the page&#x27;s last modification LSN (Log Sequence Number). The page doesn&#x27;t go to disk until its log records are safe.&lt;&#x2F;p&gt;
&lt;p&gt;This is impossible with mmap. The kernel has no concept of WAL ordering or LSNs. It flushes when it feels like it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;o-direct-bypassing-the-os-page-cache&quot;&gt;O_DIRECT: bypassing the OS page cache&lt;&#x2F;h2&gt;
&lt;p&gt;Most serious databases open their files with &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt;. This tells the kernel to skip its own page cache entirely. Reads and writes go straight between the database&#x27;s buffer pool and the disk.&lt;&#x2F;p&gt;
&lt;p&gt;Without &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt;, you&#x27;d have the data in two places: once in the database&#x27;s buffer pool and once in the OS page cache. Double the memory usage for no benefit. The database already manages caching. The OS cache is redundant.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;O_DIRECT&lt;&#x2F;code&gt; also gives the database precise control over I&#x2F;O timing, no surprises from kernel write-back threads or memory pressure from the kernel evicting buffer pool pages.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;O_DIRECT&lt;&#x2F;code&gt; isn&#x27;t free to use. It requires buffers to be aligned to the filesystem block size (usually 512 bytes or 4KB), and I&#x2F;O sizes must also be aligned. If you get the alignment wrong, the syscall fails with EINVAL. This is why most databases that use &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt; implement their own aligned allocation routines.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;PostgreSQL is an exception. It uses the OS page cache (buffered I&#x2F;O) rather than &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt;, and relies on &lt;code&gt;fsync&lt;&#x2F;code&gt; to force data to disk. It simplifies some things but means PostgreSQL competes with the OS for memory management control.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;it-s-the-same-problem-at-a-different-layer&quot;&gt;it&#x27;s the same problem at a different layer&lt;&#x2F;h2&gt;
&lt;p&gt;The parallel is almost exact:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;OS Virtual Memory&lt;&#x2F;th&gt;&lt;th&gt;Database Buffer Pool&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Physical page frame&lt;&#x2F;td&gt;&lt;td&gt;Buffer pool frame&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Page table (virtual to physical)&lt;&#x2F;td&gt;&lt;td&gt;Page table (page ID to frame)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Page fault handler&lt;&#x2F;td&gt;&lt;td&gt;Buffer pool miss handler&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Dirty bit in PTE&lt;&#x2F;td&gt;&lt;td&gt;Dirty flag per frame&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Reference count&lt;&#x2F;td&gt;&lt;td&gt;Pin count&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;kswapd (page reclaim)&lt;&#x2F;td&gt;&lt;td&gt;Eviction policy (LRU, clock)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Swap file&lt;&#x2F;td&gt;&lt;td&gt;Data file on disk&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;code&gt;write-back&lt;&#x2F;code&gt; flush&lt;&#x2F;td&gt;&lt;td&gt;WAL-ordered write-back&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The database takes this responsibility away from the OS because general-purpose policies don&#x27;t work for database workloads. Eviction needs access-pattern awareness. Write-back needs WAL ordering. Prefetching needs query-plan knowledge. The OS has none of this context.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;InnoDB (MySQL) uses a buffer pool with an LRU that splits into &quot;young&quot; and &quot;old&quot; sublists. New pages enter the old sublist, and only move to the young sublist if accessed again. This handles the scan-pollution problem.&lt;&#x2F;li&gt;
&lt;li&gt;PostgreSQL&#x27;s shared_buffers is its buffer pool. It uses a clock-sweep eviction policy.&lt;&#x2F;li&gt;
&lt;li&gt;SQLite in WAL mode maintains its own page cache but sits on top of the OS page cache (no O_DIRECT). It works because SQLite targets small-to-medium databases where double-caching isn&#x27;t expensive.&lt;&#x2F;li&gt;
&lt;li&gt;The buffer pool is one of the first things a database student builds. It&#x27;s simple in concept and brutal in the details (concurrency, latch ordering, I&#x2F;O scheduling).&lt;&#x2F;li&gt;
&lt;li&gt;Some databases are experimenting with letting the buffer pool manage allocation at finer granularity than pages. But pages have stuck around because they align with disk I&#x2F;O boundaries.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Context Switches</title>
        <published>2025-04-03T00:00:00+00:00</published>
        <updated>2025-04-03T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/context-switches/"/>
        <id>https://yazeed1s.github.io/posts/context-switches/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/context-switches/">&lt;p&gt;A CPU can only run one thing at a time. When the OS needs to switch from one process to another, it saves everything about the current one and loads everything about the next. That&#x27;s a context switch.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-gets-saved&quot;&gt;what gets saved&lt;&#x2F;h2&gt;
&lt;p&gt;&quot;Everything&quot; means the program counter (where the process was in its code), the register values, the stack pointer, and some other CPU state. This is called the process context.&lt;&#x2F;p&gt;
&lt;p&gt;The reason we need this is simple: we have more processes than CPUs. On my laptop I might have hundreds of processes but only 8 cores. They all need to run somehow, so they take turns. The OS gives each process a slice of time on a CPU, and when the slice is up (or something else happens), it switches to another process.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;when-do-context-switches-happen&quot;&gt;when do context switches happen&lt;&#x2F;h2&gt;
&lt;p&gt;A few situations:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Timer interrupt.&lt;&#x2F;strong&gt; The scheduler sets a timer (maybe 1-10ms depending on the OS and config). When it fires, the kernel gets control and can decide to switch to another process. This is called preemption.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Process blocks.&lt;&#x2F;strong&gt; If a process does something that has to wait (reading from disk, waiting for network, waiting for a lock), it makes no sense to keep it on the CPU. The kernel switches to something that can actually run.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Explicit yield.&lt;&#x2F;strong&gt; A process can voluntarily give up the CPU. This is rare in practice, most of the time the kernel just preempts.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Higher priority process wakes up.&lt;&#x2F;strong&gt; If something more important becomes runnable (like a real-time task or an interactive process that was waiting for input), the kernel might preempt the current process immediately.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-actually-happens-during-a-switch&quot;&gt;what actually happens during a switch&lt;&#x2F;h2&gt;
&lt;p&gt;When the kernel decides to switch from process A to process B:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Save A&#x27;s registers to memory (usually in some kernel data structure associated with A, like the task_struct in Linux)&lt;&#x2F;li&gt;
&lt;li&gt;Save A&#x27;s stack pointer&lt;&#x2F;li&gt;
&lt;li&gt;Update page tables or TLB if the processes have different address spaces (this is where it gets expensive)&lt;&#x2F;li&gt;
&lt;li&gt;Load B&#x27;s registers from memory&lt;&#x2F;li&gt;
&lt;li&gt;Load B&#x27;s stack pointer&lt;&#x2F;li&gt;
&lt;li&gt;Jump to wherever B left off&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;If A and B are threads in the same process, step 3 is simpler because they share the same address space. This is one reason thread switches are cheaper than process switches.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is the common case, but not always true. With Spectre mitigations like KPTI enabled, the kernel uses separate page tables for user and kernel space. Even a thread switch within the same process pays for the kernel page table switch on entry and exit.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;why-context-switches-are-expensive&quot;&gt;why context switches are expensive&lt;&#x2F;h2&gt;
&lt;p&gt;The direct cost isn&#x27;t that bad. Saving and restoring registers takes maybe hundreds of nanoseconds to a few microseconds. The bigger costs are indirect:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;TLB flush.&lt;&#x2F;strong&gt; The TLB caches virtual-to-physical address translations. If you switch to a process with a different address space, those cached translations are useless (or worse, wrong). The TLB gets flushed and the new process starts cold. Every memory access causes a TLB miss until the cache warms up again.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Cache pollution.&lt;&#x2F;strong&gt; The new process touches different memory. The L1&#x2F;L2&#x2F;L3 caches fill up with its data, evicting the old process&#x27;s data. When you switch back, you start with cold caches again.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Pipeline flush.&lt;&#x2F;strong&gt; Modern CPUs have deep pipelines with speculative execution. A context switch might flush all of that.&lt;&#x2F;p&gt;
&lt;p&gt;So the direct cost is maybe 1-10 microseconds depending on the hardware. But the indirect costs from cache&#x2F;TLB warming can add hundreds of microseconds or more to the next stretch of execution.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;why-this-matters&quot;&gt;why this matters&lt;&#x2F;h2&gt;
&lt;p&gt;If you&#x27;re doing I&#x2F;O-bound work, context switches are probably fine. You&#x27;re waiting on disk or network anyway, the overhead is noise compared to the wait time.&lt;&#x2F;p&gt;
&lt;p&gt;If you&#x27;re doing CPU-bound work and switching a lot, you&#x27;re wasting cycles. This is why things like busy-waiting or spinning on a lock can sometimes outperform blocking (at least for short waits). You avoid the context switch overhead.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s also why async I&#x2F;O and event loops (like epoll, io_uring, kqueue) are popular. Instead of blocking and switching, you check if something is ready and move on. You stay on the CPU. Fewer switches, warmer caches.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;some-numbers&quot;&gt;some numbers&lt;&#x2F;h2&gt;
&lt;p&gt;These are rough and depend heavily on hardware and workload:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Direct context switch cost: ~1-5 Î¼s on modern hardware&lt;&#x2F;li&gt;
&lt;li&gt;TLB flush + warmup: can add 10-100+ Î¼s depending on working set size&lt;&#x2F;li&gt;
&lt;li&gt;Thread switch (same process): cheaper, maybe 0.5-2 Î¼s since no page table switch&lt;&#x2F;li&gt;
&lt;li&gt;Typical scheduler timeslice: 1-10 ms&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The kernel tries to be smart about this. Linux&#x27;s CFS scheduler considers cache locality when picking where to run a task. It tries to keep tasks on the same CPU they ran on last time (CPU affinity) to preserve cache warmth.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;You can measure context switches on Linux with &lt;code&gt;perf stat -e context-switches .&#x2F;your_program&lt;&#x2F;code&gt; or by reading &lt;code&gt;&#x2F;proc&#x2F;[pid]&#x2F;status&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;vmstat 1&lt;&#x2F;code&gt; shows system-wide context switches per second&lt;&#x2F;li&gt;
&lt;li&gt;Voluntary vs involuntary: voluntary means the process blocked or yielded, involuntary means the scheduler preempted it&lt;&#x2F;li&gt;
&lt;li&gt;In user-space threading (green threads, goroutines), &quot;context switches&quot; are much cheaper because you&#x27;re not going through the kernel and not touching page tables&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Interrupts, Traps, and the Kernel Boundary</title>
        <published>2025-02-18T00:00:00+00:00</published>
        <updated>2025-02-18T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/interrupts-traps/"/>
        <id>https://yazeed1s.github.io/posts/interrupts-traps/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/interrupts-traps/">&lt;p&gt;Your app might call a syscall, a packet might arrive on the network card, or the timer might fire; all of these interrupt normal execution, but they&#x27;re not the same thing. The terminology gets confusing because people use interrupt, trap, and exception loosely.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;user-space-vs-kernel-space&quot;&gt;user space vs kernel space&lt;&#x2F;h2&gt;
&lt;p&gt;First the basics: modern CPUs have privilege levels called rings on x86, where ring 0 is the most privileged (kernel) and ring 3 is least privileged (user applications).&lt;&#x2F;p&gt;
&lt;p&gt;Your application runs in ring 3. It can execute normal instructions, access its own memory, do math. But it cannot:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Access hardware directly&lt;&#x2F;li&gt;
&lt;li&gt;Read&#x2F;write arbitrary memory addresses&lt;&#x2F;li&gt;
&lt;li&gt;Execute privileged instructions (like changing page tables or disabling interrupts)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The kernel runs in ring 0. It can do all of those things.&lt;&#x2F;p&gt;
&lt;p&gt;When you call &lt;code&gt;read()&lt;&#x2F;code&gt; to read from a file, your code can&#x27;t just talk to the disk controller. It has to ask the kernel. The CPU has to switch from ring 3 to ring 0, do the privileged work, then switch back.&lt;&#x2F;p&gt;
&lt;p&gt;This switch is the kernel boundary, and crossing it costs you: register save&#x2F;restore, privilege change, and sometimes cache disruption. That&#x27;s why syscalls aren&#x27;t free.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;interrupts&quot;&gt;interrupts&lt;&#x2F;h2&gt;
&lt;p&gt;An interrupt is a signal from hardware that says &quot;stop what you&#x27;re doing, I need attention.&quot;&lt;&#x2F;p&gt;
&lt;p&gt;Examples:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Keyboard: &quot;a key was pressed&quot;&lt;&#x2F;li&gt;
&lt;li&gt;Network card: &quot;a packet arrived&quot;&lt;&#x2F;li&gt;
&lt;li&gt;Timer: &quot;your time slice is up&quot;&lt;&#x2F;li&gt;
&lt;li&gt;Disk controller: &quot;that read you asked for is done&quot;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Interrupts are asynchronous. They happen whenever the hardware needs attention, regardless of what the CPU is currently doing. You could be in the middle of a for loop and suddenly an interrupt fires.&lt;&#x2F;p&gt;
&lt;p&gt;When an interrupt happens:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;CPU stops executing current instruction stream&lt;&#x2F;li&gt;
&lt;li&gt;Saves current state (registers, instruction pointer, flags)&lt;&#x2F;li&gt;
&lt;li&gt;Looks up the interrupt handler in the Interrupt Descriptor Table (IDT)&lt;&#x2F;li&gt;
&lt;li&gt;Jumps to that handler (now in ring 0)&lt;&#x2F;li&gt;
&lt;li&gt;Handler does its work&lt;&#x2F;li&gt;
&lt;li&gt;Handler returns, CPU restores state, continues where it left off&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;The key thing: the currently running process doesn&#x27;t trigger this. It just happens to it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;traps&quot;&gt;traps&lt;&#x2F;h2&gt;
&lt;p&gt;A trap is a synchronous exception triggered by the currently running code. It&#x27;s intentional.&lt;&#x2F;p&gt;
&lt;p&gt;The main example: syscalls. When you call &lt;code&gt;read()&lt;&#x2F;code&gt;, the C library eventually executes a special instruction (&lt;code&gt;syscall&lt;&#x2F;code&gt; on x86-64, &lt;code&gt;int 0x80&lt;&#x2F;code&gt; on older x86) that deliberately triggers a trap.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;User code calls read()&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;  -&amp;gt; libc wrapper&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    -&amp;gt; syscall instruction (trap into kernel)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;      -&amp;gt; kernel syscall handler&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        -&amp;gt; returns to user space&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The difference from interrupts: you asked for this. The code executing triggered it. It happens at a specific point in your instruction stream, not randomly.&lt;&#x2F;p&gt;
&lt;p&gt;Other traps&#x2F;exceptions:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Page fault â€” you accessed memory that isn&#x27;t mapped. Could be a bug, or could be demand paging doing its job.&lt;&#x2F;li&gt;
&lt;li&gt;Division by zero â€” arithmetic error&lt;&#x2F;li&gt;
&lt;li&gt;Invalid opcode â€” tried to execute garbage&lt;&#x2F;li&gt;
&lt;li&gt;Breakpoint â€” debugger trap (int 3)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Some of these are errors (division by zero kills your process). Some are handled and execution continues (page fault loads the page, then your load instruction retries).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-naming-confusion&quot;&gt;the naming confusion&lt;&#x2F;h2&gt;
&lt;p&gt;Different sources use these terms differently. Here&#x27;s how I think about it:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interrupt&lt;&#x2F;strong&gt; â€” external, async, from hardware&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Trap&lt;&#x2F;strong&gt; â€” internal, sync, intentional (syscalls)&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Exception&lt;&#x2F;strong&gt; â€” internal, sync, usually an error (page fault, div by zero)&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Fault&lt;&#x2F;strong&gt; â€” exception that can be corrected (page fault) â€” instruction retries&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Abort&lt;&#x2F;strong&gt; â€” unrecoverable error&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Some people use &quot;exception&quot; as the umbrella term for everything. Some use &quot;interrupt&quot; for everything. The Intel manual has its own definitions. It&#x27;s messy.&lt;&#x2F;p&gt;
&lt;p&gt;What matters: understand whether the trigger is external (hardware) or internal (executing code), and whether it&#x27;s expected (syscall) or unexpected (error).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-interrupt-descriptor-table&quot;&gt;the interrupt descriptor table&lt;&#x2F;h2&gt;
&lt;p&gt;The CPU needs to know where to jump for each interrupt&#x2F;exception. This is stored in the IDT, a table in memory. The kernel sets this up at boot.&lt;&#x2F;p&gt;
&lt;p&gt;Each entry has:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Handler address&lt;&#x2F;li&gt;
&lt;li&gt;What privilege level can trigger it&lt;&#x2F;li&gt;
&lt;li&gt;Gate type (interrupt gate, trap gate)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;For hardware interrupts, the entries point to kernel interrupt handlers. For the syscall trap, it points to the syscall entry point.&lt;&#x2F;p&gt;
&lt;p&gt;When an interrupt fires, the CPU:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Uses the interrupt number as an index into IDT&lt;&#x2F;li&gt;
&lt;li&gt;Checks privilege&lt;&#x2F;li&gt;
&lt;li&gt;Switches to ring 0 if needed&lt;&#x2F;li&gt;
&lt;li&gt;Jumps to the handler address&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;hardware-interrupts-in-more-detail&quot;&gt;hardware interrupts in more detail&lt;&#x2F;h2&gt;
&lt;p&gt;When a device needs attention, it signals an interrupt request (IRQ). On modern systems this goes through an interrupt controller (APIC).&lt;&#x2F;p&gt;
&lt;p&gt;The kernel has to:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Acknowledge the interrupt&lt;&#x2F;li&gt;
&lt;li&gt;Figure out which device caused it&lt;&#x2F;li&gt;
&lt;li&gt;Call the right driver&#x27;s handler&lt;&#x2F;li&gt;
&lt;li&gt;Tell the interrupt controller we&#x27;re done&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Handling needs to be fast because interrupts are disabled (or that IRQ is masked) while you&#x27;re in the handler. If you take too long, you miss other interrupts.&lt;&#x2F;p&gt;
&lt;p&gt;Linux splits this into top half and bottom half:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Top half&lt;&#x2F;strong&gt;: runs in interrupt context, does minimum work, schedules bottom half&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Bottom half&lt;&#x2F;strong&gt;: runs later with interrupts enabled, does the real work (softirqs, tasklets, workqueues)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;For example, network card interrupt:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Top half: grab the packet from hardware, queue it, schedule bottom half&lt;&#x2F;li&gt;
&lt;li&gt;Bottom half: process the packet up the network stack&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;syscall-cost&quot;&gt;syscall cost&lt;&#x2F;h2&gt;
&lt;p&gt;Crossing the kernel boundary isn&#x27;t free. You pay for:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Saving&#x2F;restoring registers&lt;&#x2F;li&gt;
&lt;li&gt;Switching stacks (user stack -&amp;gt; kernel stack)&lt;&#x2F;li&gt;
&lt;li&gt;TLB and cache effects&lt;&#x2F;li&gt;
&lt;li&gt;Spectre mitigations on modern kernels (KPTI, retpolines)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;On a modern system, a syscall might take a few hundred nanoseconds. Doesn&#x27;t sound like much, but if you&#x27;re doing thousands per second, it adds up.&lt;&#x2F;p&gt;
&lt;p&gt;That cost is why people use:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Batching (fewer syscalls, more work per call)&lt;&#x2F;li&gt;
&lt;li&gt;io_uring (submit many I&#x2F;O requests with one syscall)&lt;&#x2F;li&gt;
&lt;li&gt;mmap (access files without read() syscalls)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;On x86-64, &lt;code&gt;syscall&lt;&#x2F;code&gt;&#x2F;&lt;code&gt;sysret&lt;&#x2F;code&gt; are faster than the old &lt;code&gt;int 0x80&lt;&#x2F;code&gt; method&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;&#x2F;proc&#x2F;interrupts&lt;&#x2F;code&gt; shows interrupt counts per CPU&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;perf stat&lt;&#x2F;code&gt; can count context switches and syscalls&lt;&#x2F;li&gt;
&lt;li&gt;NMI (Non-Maskable Interrupt) can&#x27;t be disabled, used for profiling and panic&lt;&#x2F;li&gt;
&lt;li&gt;The timer interrupt is what makes preemptive multitasking work&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
</feed>
