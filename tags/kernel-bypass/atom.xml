<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Yazeed&#x27;s Blog - Kernel Bypass</title>
    <subtitle>Notes on systems and low-level software.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://yazeed1s.github.io/tags/kernel-bypass/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://yazeed1s.github.io"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2026-01-16T00:00:00+00:00</updated>
    <id>https://yazeed1s.github.io/tags/kernel-bypass/atom.xml</id>
    <entry xml:lang="en">
        <title>RDMA: Bypassing the Kernel for Network I&#x2F;O</title>
        <published>2026-01-16T00:00:00+00:00</published>
        <updated>2026-01-16T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/rdma/"/>
        <id>https://yazeed1s.github.io/posts/rdma/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/rdma/">&lt;hr &#x2F;&gt;
&lt;p&gt;RDMA lets one machine read and write another machine&#x27;s memory. The network card handles it. The remote CPU doesn&#x27;t even know it happened.&lt;&#x2F;p&gt;
&lt;p&gt;That sounded wrong to me at first. I knew the slogan, &quot;skip the kernel, go fast,&quot; but I didn&#x27;t understand what it meant at the hardware level.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-s-wrong-with-normal-networking&quot;&gt;what&#x27;s wrong with normal networking&lt;&#x2F;h2&gt;
&lt;p&gt;When you use TCP, kernel is always involved.&lt;&#x2F;p&gt;
&lt;p&gt;The app calls &lt;code&gt;send()&lt;&#x2F;code&gt;, which is a syscall, so execution enters the kernel; your data gets copied from the app buffer to a kernel buffer, TCP runs its state machine (checksum, segmentation, queueing), and eventually the driver sends it to the NIC.&lt;&#x2F;p&gt;
&lt;p&gt;Receive side same thing. NIC gets packet, interrupt, kernel wakes up, copies to socket buffer. App calls &lt;code&gt;recv()&lt;&#x2F;code&gt;, another copy into app buffer.&lt;&#x2F;p&gt;
&lt;p&gt;So you end up with two copies, multiple syscalls, and context switches every time, and the CPU stays busy with all of it.&lt;&#x2F;p&gt;
&lt;p&gt;If you&#x27;re moving big files, it&#x27;s OK. Overhead doesn&#x27;t matter much. But if you want millions of small operations per second, like key-value gets, this overhead is too much.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-rdma-does-different&quot;&gt;what rdma does different&lt;&#x2F;h2&gt;
&lt;p&gt;With RDMA, the network card reads and writes directly to your app memory.&lt;&#x2F;p&gt;
&lt;p&gt;When you send, the NIC reads from your buffer via DMA, and when you receive, it writes into your buffer via DMA, so the kernel is off the fast path and extra copies disappear.&lt;&#x2F;p&gt;
&lt;p&gt;There&#x27;s also one-sided operations: RDMA_WRITE puts bytes into remote memory and RDMA_READ pulls bytes from it, while the remote CPU keeps running because nobody wakes it up.&lt;&#x2F;p&gt;
&lt;p&gt;First time I saw this it looked strange. You&#x27;re writing to memory on different machine, through network card, and that machine doesn&#x27;t know it happened.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&quot;Doesn&#x27;t know&quot; means the remote CPU isn&#x27;t interrupted and doesn&#x27;t execute any code. But the NIC is still doing DMA over PCIe, which consumes memory bandwidth on the remote machine. At high throughput, one-sided RDMA operations can noticeably affect remote-side performance even though no software runs there.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;rdma.png&quot; alt=&quot;TCP vs RDMA&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;setup-vs-data-path&quot;&gt;setup vs data path&lt;&#x2F;h2&gt;
&lt;p&gt;OK but the kernel is still there. Just not on data path.&lt;&#x2F;p&gt;
&lt;p&gt;Before you send anything, you have to set things up by opening the device, creating queues, registering memory, and connecting to the remote side; all of this goes through syscalls and kernel checks.&lt;&#x2F;p&gt;
&lt;p&gt;Only after setup does the fast path work, where you post work directly to hardware and poll completions without syscalls for that part.&lt;&#x2F;p&gt;
&lt;p&gt;So the trade is: expensive setup, cheap operations after. Good if you do many operations. Not good for connections that don&#x27;t last.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;queue-pairs&quot;&gt;queue pairs&lt;&#x2F;h2&gt;
&lt;p&gt;RDMA doesn&#x27;t use sockets. Uses queues instead.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Queue Pair&lt;&#x2F;strong&gt; is your connection. Has send queue and receive queue. You put work requests there, saying what to do (send this buffer, read from that address). NIC processes them when it can.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Completion Queue&lt;&#x2F;strong&gt; is how you know things finished. NIC puts entries. You poll or wait.&lt;&#x2F;p&gt;
&lt;p&gt;The annoying thing is that queue pairs start in RESET state and must move through RESET → INIT → RTR → RTS; if you miss one transition, nothing works and you usually don&#x27;t get a useful error, which took me a while to learn the first time.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;memory-registration&quot;&gt;memory registration&lt;&#x2F;h2&gt;
&lt;p&gt;Before NIC can touch your memory, you register it.&lt;&#x2F;p&gt;
&lt;p&gt;What registration does:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Pins the pages. Memory can&#x27;t go to swap. Physical addresses stay valid because hardware will DMA there.&lt;&#x2F;li&gt;
&lt;li&gt;Builds translation in NIC. Hardware needs to know where virtual address X is in physical memory.&lt;&#x2F;li&gt;
&lt;li&gt;Gives you keys. lkey for your own ops, rkey to share with remote. They need your rkey to access your memory.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Registration is a syscall. This is where kernel checks permissions.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;who-checks-if-not-kernel&quot;&gt;who checks if not kernel&lt;&#x2F;h2&gt;
&lt;p&gt;This part confused me for a while.&lt;&#x2F;p&gt;
&lt;p&gt;With normal networking kernel validates everything. Bad pointer? SIGSEGV. Wrong permission? Error. Kernel is the one checking.&lt;&#x2F;p&gt;
&lt;p&gt;But RDMA kernel is not in data path. So how bad accesses get stopped?&lt;&#x2F;p&gt;
&lt;p&gt;Answer is hardware.&lt;&#x2F;p&gt;
&lt;p&gt;When you register memory, kernel tells the NIC: these addresses valid, these permissions, this protection domain. NIC stores all this in its memory protection tables.&lt;&#x2F;p&gt;
&lt;p&gt;Then during transfers NIC checks every operation:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Is address in registered region?&lt;&#x2F;li&gt;
&lt;li&gt;Permissions OK?&lt;&#x2F;li&gt;
&lt;li&gt;Key matches?&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;If something wrong, operation fails. Error shows in completion queue. Not SIGSEGV because NIC caught it, not CPU.&lt;&#x2F;p&gt;
&lt;p&gt;Hardware does what kernel would do, just at wire speed.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;protection-domains&quot;&gt;protection domains&lt;&#x2F;h2&gt;
&lt;p&gt;You can&#x27;t access anyone&#x27;s memory.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Protection Domain&lt;&#x2F;strong&gt; is security boundary. When you make queue pair and register memory, you put them in a PD. Operations only work on memory in same PD.&lt;&#x2F;p&gt;
&lt;p&gt;This is like kernel process isolation but for RDMA. Different apps get different PDs.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;one-sided-and-two-sided&quot;&gt;one-sided and two-sided&lt;&#x2F;h2&gt;
&lt;p&gt;Two kinds of operations.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Two-sided.&lt;&#x2F;strong&gt; Both sides do something. Receiver posts buffer first. Sender posts send. Both CPUs involved.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;One-sided.&lt;&#x2F;strong&gt; Only you do something. RDMA_WRITE pushes data to remote memory. RDMA_READ pulls data. Remote CPU not involved. Not even aware.&lt;&#x2F;p&gt;
&lt;p&gt;One-sided is where RDMA is powerful. But also more work. If remote app needs to know you wrote, you have to tell it. Usually you write a flag that it polls. Or use atomics. Synchronization is your problem to solve.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;atomics&quot;&gt;atomics&lt;&#x2F;h2&gt;
&lt;p&gt;There are atomic operations:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Compare-and-swap&lt;&#x2F;li&gt;
&lt;li&gt;Fetch-and-add&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;They run at remote memory, atomically. Remote CPU not involved.&lt;&#x2F;p&gt;
&lt;p&gt;Good for locks, counters. But slower than regular read&#x2F;write. And some NICs implement in firmware so even slower. Depends on card.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-hardware&quot;&gt;the hardware&lt;&#x2F;h2&gt;
&lt;p&gt;You need special NIC. Normal ones don&#x27;t do RDMA.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;InfiniBand.&lt;&#x2F;strong&gt; The original. Needs its own switches and cables. Very low latency, under microsecond. HPC clusters use this.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;RoCE.&lt;&#x2F;strong&gt; RDMA over Ethernet. Works on regular switches. But Ethernet drops packets and RDMA really doesn&#x27;t like that. So you configure switches for &quot;lossless&quot; mode. Priority flow control and so on. It gets complicated.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;iWARP.&lt;&#x2F;strong&gt; RDMA over TCP, which is the most compatible option, but TCP adds latency.&lt;&#x2F;p&gt;
&lt;p&gt;I think most datacenters use RoCE v2 now.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;some-numbers&quot;&gt;some numbers&lt;&#x2F;h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;What&lt;&#x2F;th&gt;&lt;th&gt;How long&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;TCP round trip&lt;&#x2F;td&gt;&lt;td&gt;10-50 μs&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;RDMA round trip&lt;&#x2F;td&gt;&lt;td&gt;1-5 μs&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Local memory&lt;&#x2F;td&gt;&lt;td&gt;~100 ns&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;RDMA is maybe 10x faster than TCP. But still 10x slower than local RAM. This is important when you think about memory disaggregation. You&#x27;re replacing local memory with remote. Microseconds add up.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-s-hard-about-it&quot;&gt;what&#x27;s hard about it&lt;&#x2F;h2&gt;
&lt;p&gt;Debugging is not fun. Problems are completion queue errors with codes you have to look up. No tcpdump. When something breaks you look at hardware counters and guess.&lt;&#x2F;p&gt;
&lt;p&gt;Before RDMA works, both sides exchange info. Queue pair numbers, memory keys, addresses. Usually you do this over TCP first. Extra complexity.&lt;&#x2F;p&gt;
&lt;p&gt;Registered memory is pinned. Big registrations hit ulimit.&lt;&#x2F;p&gt;
&lt;p&gt;For two-sided, receiver must post buffers before sender sends. If receiver runs out of buffers, sender operations fail.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Verbs API is standard interface. libibverbs on Linux.&lt;&#x2F;li&gt;
&lt;li&gt;Watch &lt;code&gt;ulimit -l&lt;&#x2F;code&gt; for locked memory limit.&lt;&#x2F;li&gt;
&lt;li&gt;One app can have many queue pairs and completion queues. Common pattern is one QP per thread.&lt;&#x2F;li&gt;
&lt;li&gt;Atomic ops: compare-and-swap, fetch-and-add. Support varies by hardware.&lt;&#x2F;li&gt;
&lt;li&gt;Paper: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;nsdi22-paper-reda_1.pdf&quot;&gt;RDMA is Turing complete&lt;&#x2F;a&gt; (yes really)&lt;&#x2F;li&gt;
&lt;li&gt;Man pages: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;linux-rdma&#x2F;rdma-core&#x2F;blob&#x2F;master&#x2F;libibverbs&#x2F;man&#x2F;ibv_create_qp.3&quot;&gt;ibv_create_qp&lt;&#x2F;a&gt;, &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;linux-rdma&#x2F;rdma-core&#x2F;blob&#x2F;master&#x2F;libibverbs&#x2F;man&#x2F;ibv_reg_mr.3&quot;&gt;ibv_reg_mr&lt;&#x2F;a&gt;, &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;linux-rdma&#x2F;rdma-core&#x2F;blob&#x2F;master&#x2F;libibverbs&#x2F;man&#x2F;ibv_post_send.3&quot;&gt;ibv_post_send&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Nvidia docs: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;docs.nvidia.com&#x2F;networking&#x2F;display&#x2F;RDMAAwareProgrammingv17&quot;&gt;RDMA Aware Networks Programming User Manual&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
</feed>
