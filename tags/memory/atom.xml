<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Yazeed&#x27;s Blog - Memory</title>
    <subtitle>Notes on systems and low-level software.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://yazeed1s.github.io/tags/memory/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://yazeed1s.github.io"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2026-02-05T00:00:00+00:00</updated>
    <id>https://yazeed1s.github.io/tags/memory/atom.xml</id>
    <entry xml:lang="en">
        <title>CXL: Why Datacenter Memory is Getting a New Tier</title>
        <published>2026-02-05T00:00:00+00:00</published>
        <updated>2026-02-05T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/cxl/"/>
        <id>https://yazeed1s.github.io/posts/cxl/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/cxl/">&lt;hr &#x2F;&gt;
&lt;p&gt;CXL keeps appearing everywhere I read about memory disaggregation. Every paper mentions it as &lt;em&gt;&quot;the future&quot;&lt;&#x2F;em&gt; of remote memory access. And I think I finally sat down to understand what it actually is.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-problem-it-s-solving&quot;&gt;the problem it&#x27;s solving&lt;&#x2F;h2&gt;
&lt;p&gt;Memory is expensive, especially in datacenters. DRAM can be 50% of server cost. And it&#x27;s wasted constantly. One machine thrashes while another sits at 30% usage. Many research papers have shown that memory usage is highly imbalanced across servers in a datacenter, and memory underutilization is 70% in some cases. So what do we do?&lt;&#x2F;p&gt;
&lt;p&gt;The obvious fix is to let machines share memory. Pool it like storage. So instead of the memory setting idle in one server, it can be used by another server.&lt;&#x2F;p&gt;
&lt;p&gt;But memory access is different from storage. Storage can tolerate milliseconds. Memory access happens in nanoseconds. A cache miss to DRAM is ~100ns. Going over the network adds orders of magnitude. RDMA gets you down to single-digit microseconds. Still 10-50x slower than local memory.&lt;&#x2F;p&gt;
&lt;p&gt;CXL is supposed to close that gap.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-cxl-actually-is&quot;&gt;what cxl actually is&lt;&#x2F;h2&gt;
&lt;p&gt;CXL stands for Compute Express Link. It&#x27;s a standard built on top of PCIe physical layer. You use the same cables and slots.&lt;&#x2F;p&gt;
&lt;p&gt;The key thing: it&#x27;s cache-coherent. The CPU can do normal load&#x2F;store to CXL-attached memory. The memory controller handles it. No special APIs. No registration like RDMA. Just... memory.&lt;&#x2F;p&gt;
&lt;p&gt;Three protocols in CXL:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;CXL.io&lt;&#x2F;strong&gt; — basically PCIe. For discovery, configuration, interrupts. Boring stuff.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;CXL.cache&lt;&#x2F;strong&gt; — lets devices cache host memory. Useful for accelerators that want to share data with the CPU without explicit copies.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;CXL.mem&lt;&#x2F;strong&gt; — the interesting one. Lets the host access device-attached memory. The CPU sees it as another memory region. Different NUMA node, slower latency, but addressable like regular RAM.&lt;&#x2F;p&gt;
&lt;p&gt;CXL 1.0&#x2F;1.1 is mostly expansion cards. Plug a CXL card with extra DRAM into PCIe slot. Your system gets more memory. Latency is worse than local DIMMs (maybe 200-300ns instead of 100ns), but it&#x27;s still memory, not storage.&lt;&#x2F;p&gt;
&lt;p&gt;CXL 2.0 adds switching. Multiple hosts can connect to a shared memory pool. CXL 3.0 pushes this further with fabric.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;mixing-memory-types&quot;&gt;mixing memory types&lt;&#x2F;h2&gt;
&lt;p&gt;This is the part that took me a while to get.&lt;&#x2F;p&gt;
&lt;p&gt;Normally, your CPU&#x27;s memory controller dictates what DRAM you use. DDR5 system? All your DIMMs are DDR5. Same speed, same density rules. You can&#x27;t just plug DDR4 into a DDR5 slot.&lt;&#x2F;p&gt;
&lt;p&gt;CXL breaks this. The CXL device has its own memory controller. It can use whatever DRAM it wants. DDR4, DDR5, older stuff, slower but denser chips. The CPU doesn&#x27;t care. It just sees CXL memory at some address range.&lt;&#x2F;p&gt;
&lt;p&gt;So you could have a system with local DDR5 for hot data, and a CXL card with cheaper DDR4 as a slower tier. Or high-capacity, high-density modules that wouldn&#x27;t work in your native DIMM slots.&lt;&#x2F;p&gt;
&lt;p&gt;This is interesting from a cost angle. You&#x27;re not stuck with whatever generation your motherboard supports. Want to keep using old DRAM you already bought? Stick it behind CXL. Want the newest high-density stuff that doesn&#x27;t fit your memory controller&#x27;s timing specs? CXL device can handle it.&lt;&#x2F;p&gt;
&lt;p&gt;The tradeoff is latency. CXL adds overhead. But if you&#x27;re using it for capacity expansion (not latency-critical paths), maybe that&#x27;s fine.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;why-not-just-use-rdma&quot;&gt;why not just use rdma&lt;&#x2F;h2&gt;
&lt;p&gt;I kept wondering this. RDMA already gives you fast remote memory access. Infiniswap uses it. Works today.&lt;&#x2F;p&gt;
&lt;p&gt;But RDMA is different:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;You need explicit verbs. Post work request, poll completion. Not load&#x2F;store.&lt;&#x2F;li&gt;
&lt;li&gt;Memory registration overhead. Pin pages, get keys, share keys.&lt;&#x2F;li&gt;
&lt;li&gt;One-sided ops are async. CPU doesn&#x27;t know when remote writes land unless you signal.&lt;&#x2F;li&gt;
&lt;li&gt;~1-5μs latency. Fast for network, slow for memory.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;CXL is supposed to be ~200-500ns for pooled memory. Still slower than local, but closer. And it&#x27;s transparent to software. Your malloc can return CXL memory. The app doesn&#x27;t know.&lt;&#x2F;p&gt;
&lt;p&gt;This is the promise anyway. Shipping hardware today is mostly local expansion, not pooled.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-latency-question&quot;&gt;the latency question&lt;&#x2F;h2&gt;
&lt;p&gt;This is where I&#x27;m not fully convinced.&lt;&#x2F;p&gt;
&lt;p&gt;Local DRAM: ~100ns. CXL local expansion: ~200-300ns. CXL pooled (through switch): ~500-1000ns.&lt;&#x2F;p&gt;
&lt;p&gt;Okay so CXL pool is 5-10x slower than local. That&#x27;s... a lot? For some workloads, maybe fine. For tight loops hitting memory constantly, this seems expensive.&lt;&#x2F;p&gt;
&lt;p&gt;The pitch is: better than swapping to SSD (100μs) or going over RDMA. And you get more capacity.&lt;&#x2F;p&gt;
&lt;p&gt;I think the mental model is tiering. Hot data in local DRAM. Warm data in CXL pool. Cold data in SSD or remote. The kernel or some runtime migrates pages between tiers based on access patterns.&lt;&#x2F;p&gt;
&lt;p&gt;Linux already has this machinery. NUMA balancing, DAMON, tiered memory support merged recently. Whether it works well in practice is another question.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;cache-coherence-across-hosts&quot;&gt;cache coherence across hosts&lt;&#x2F;h2&gt;
&lt;p&gt;CXL 3.0 talks about shared memory. Multiple hosts access same bytes. Hardware maintains coherence.&lt;&#x2F;p&gt;
&lt;p&gt;This sounds amazing and also scary.&lt;&#x2F;p&gt;
&lt;p&gt;Cache coherence doesn&#x27;t scale. We learned this from distributed shared memory systems in the 90s. Beyond a few nodes, the coherence traffic kills you.&lt;&#x2F;p&gt;
&lt;p&gt;CXL spec people know this. The scope is limited. Maybe a rack. Maybe less. The vision isn&#x27;t &quot;coherent memory across datacenter.&quot; It&#x27;s more like: within a pod or blade, you get shared memory semantics. Beyond that, you&#x27;re back to message passing or RDMA-style access.&lt;&#x2F;p&gt;
&lt;p&gt;Still, even rack-scale shared memory is interesting for some workloads. Databases that want to share buffer caches. ML jobs that share model weights.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-s-shipping&quot;&gt;what&#x27;s shipping&lt;&#x2F;h2&gt;
&lt;p&gt;CXL 1.1 devices exist. Samsung, SK Hynix, others have memory expanders. Mostly used to add capacity to memory-hungry workloads. Intel Sapphire Rapids supports CXL.&lt;&#x2F;p&gt;
&lt;p&gt;CXL switches are... not really there yet. Some prototypes. Production deployments of pooled CXL memory are probably 2-3 years out.&lt;&#x2F;p&gt;
&lt;p&gt;So when papers say &quot;CXL will enable this&quot;, they&#x27;re often talking about future hardware. The concepts matter but the ecosystem is young.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-i-m-still-unsure-about&quot;&gt;what i&#x27;m still unsure about&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Is the latency worth it?&lt;&#x2F;strong&gt; 5-10x slower than local. Yes better than SSD. But memory-intensive apps might just thrash the CXL tier. Need tiering policies that actually work.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Will the ecosystem mature?&lt;&#x2F;strong&gt; RDMA took years to get right. CXL is newer. Drivers, kernel support, allocation policies, debugging tools. All need to catch up.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Who actually benefits?&lt;&#x2F;strong&gt; Big cloud providers with massive memory imbalance probably. Smaller deployments might not see ROI with current hardware costs.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;CXL consortium: Intel, AMD, ARM, NVIDIA, Samsung, etc.&lt;&#x2F;li&gt;
&lt;li&gt;Built on PCIe 5.0&#x2F;6.0 physical layer. Same slots and cables.&lt;&#x2F;li&gt;
&lt;li&gt;Latency estimates vary by source. Local expansion seems around 200-300ns. Pooled depends heavily on topology.&lt;&#x2F;li&gt;
&lt;li&gt;Linux kernel CXL support: drivers&#x2F;cxl&#x2F;. Device enumeration works. Memory tiering still evolving.&lt;&#x2F;li&gt;
&lt;li&gt;Related specs: Gen-Z (dead?), CCIX (absorbed into CXL)&lt;&#x2F;li&gt;
&lt;li&gt;Good overview: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;www.computeexpresslink.org&#x2F;&quot;&gt;CXL Consortium spec&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;For memory disaggregation context: Aguilera et al., &quot;Memory disaggregation: why now and what are the challenges&quot;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Infiniswap</title>
        <published>2026-01-18T00:00:00+00:00</published>
        <updated>2026-01-18T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/infiniswap/"/>
        <id>https://yazeed1s.github.io/posts/infiniswap/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/infiniswap/">&lt;p&gt;I found this paper while reading about memory disaggregation. The idea is simple: when a machine runs out of RAM, page to another machine&#x27;s unused memory instead of disk.&lt;&#x2F;p&gt;
&lt;p&gt;What caught my attention is how they did it. No application changes. No core kernel patching. It&#x27;s a kernel module that hooks into Linux&#x27;s swap path. Remote RAM becomes the fast tier. Disk is just the fallback.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-problem-they-re-solving&quot;&gt;the problem they&#x27;re solving&lt;&#x2F;h2&gt;
&lt;p&gt;Production clusters waste a lot of memory. Some machines are memory-starved while others sit idle. The 99th percentile machine uses 2-3× more memory than the median. Over half the cluster&#x27;s aggregate memory goes unused.&lt;&#x2F;p&gt;
&lt;p&gt;When apps can&#x27;t fit their working set in RAM, performance falls off a cliff. VoltDB drops from 95K TPS to 4K TPS. Memcached&#x27;s tail latency shoots up 21×. Disk is just too slow (like 1000× slower than memory).&lt;&#x2F;p&gt;
&lt;p&gt;So they thought: RDMA gives single-digit microsecond latencies. That&#x27;s fast enough to make remote memory a viable swap target. Pages go to remote RAM over RDMA instead of disk. The remote CPU stays out of the data movement entirely since the RNIC does the DMA.&lt;&#x2F;p&gt;
&lt;p&gt;Result: swap that looks normal to Linux but is backed by slabs of remote memory scattered across the cluster.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-i-thought-was-clever&quot;&gt;what I thought was clever&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Using swap as the integration point.&lt;&#x2F;strong&gt; Instead of modifying the page fault handler or remapping virtual memory, they plug into Linux&#x27;s swap subsystem. The kernel already knows how to page out and page in. Infiniswap just changes where those pages live. The trade-off is you still go through the swap path (page faults, context switches). But you get deployment simplicity because everything else just works.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;One-sided RDMA.&lt;&#x2F;strong&gt; Traditional network block devices like Mellanox&#x27;s nbdX use send&#x2F;recv. Remote CPU wakes up, copies data, responds. Infiniswap uses RDMA_READ and RDMA_WRITE. The RNIC accesses remote memory directly without running any code on the remote side. nbdX burns multiple vCPUs on the remote machine. Infiniswap doesn&#x27;t touch the remote CPU at all.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Slab-based design.&lt;&#x2F;strong&gt; Pages are grouped into 1GB slabs. Each slab maps to one remote machine. This keeps metadata manageable. Tracking millions of 4KB pages across the cluster would be expensive. Hot slabs (more than 20 page I&#x2F;O ops&#x2F;sec) get mapped to remote memory. Cold slabs stay on disk.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;where-it-works-well&quot;&gt;where it works well&lt;&#x2F;h2&gt;
&lt;p&gt;Memory-bound workloads see big wins. Memcached stays nearly flat even when only 50% of the working set fits in memory. PowerGraph runs 6.5× faster. VoltDB sees 15× throughput improvement over disk.&lt;&#x2F;p&gt;
&lt;p&gt;Cluster memory utilization: goes from 40% to 60%. That&#x27;s 47% more effective use of RAM. Network overhead is less than 1% of capacity.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;where-it-doesn-t-work&quot;&gt;where it doesn&#x27;t work&lt;&#x2F;h2&gt;
&lt;p&gt;CPU-bound workloads don&#x27;t benefit much. VoltDB and Spark already run at high CPU utilization. Adding paging overhead (context switches, TLB flushes, page table walks) eats into that. Spark at 50% memory thrashes so badly it doesn&#x27;t complete.&lt;&#x2F;p&gt;
&lt;p&gt;There&#x27;s a fundamental limit here: this isn&#x27;t local memory. Page faults still happen. You&#x27;re masking latency, not eliminating it. For workloads where microseconds matter deterministically, that&#x27;s still a problem.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Paper: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;conference&#x2F;nsdi17&#x2F;nsdi17-gu.pdf&quot;&gt;Gu et al., &quot;Efficient Memory Disaggregation with Infiniswap&quot;, NSDI 2017&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Tested on 32 machines, 56 Gbps Infiniband, 64GB RAM each&lt;&#x2F;li&gt;
&lt;li&gt;Slab placement uses &quot;power of two choices&quot; (pick two random machines, query free memory, use the one with more headroom)&lt;&#x2F;li&gt;
&lt;li&gt;Slab eviction queries E+5 machines, evicts coldest (~363μs median)&lt;&#x2F;li&gt;
&lt;li&gt;Page-out: synchronous RDMA_WRITE + async disk write (disk is fallback if remote crashes)&lt;&#x2F;li&gt;
&lt;li&gt;Page-in: check bitmap → RDMA_READ if remote, else disk&lt;&#x2F;li&gt;
&lt;li&gt;Slab remapping after failure takes ~54ms (Infiniband memory registration)&lt;&#x2F;li&gt;
&lt;li&gt;Default headroom threshold: 8GB per machine&lt;&#x2F;li&gt;
&lt;li&gt;Hot slab threshold: 20 page I&#x2F;O ops&#x2F;sec (EWMA, α=0.2)&lt;&#x2F;li&gt;
&lt;li&gt;Compared against nbdX (Mellanox), Fastswap, LegoOS&lt;&#x2F;li&gt;
&lt;li&gt;Code: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;SymbioticLab&#x2F;Infiniswap&quot;&gt;SymbioticLab&#x2F;infiniswap&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Memory Disaggregation</title>
        <published>2026-01-05T00:00:00+00:00</published>
        <updated>2026-01-05T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/memory-disaggregation/"/>
        <id>https://yazeed1s.github.io/posts/memory-disaggregation/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/memory-disaggregation/">&lt;p&gt;I&#x27;ve been reading about memory disaggregation lately and wanted to write down what I understand so far. This is mostly from a VMware Research paper that asks a question I found interesting: why hasn&#x27;t memory disaggregation happened already?&lt;&#x2F;p&gt;
&lt;p&gt;The idea has been around since the 90s. Intel pushed Rack Scale Architecture in 2013. It didn&#x27;t take off. The paper argues two things finally align now: the economics are painful enough and the technology exists to actually do something about it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-basic-idea&quot;&gt;the basic idea&lt;&#x2F;h2&gt;
&lt;p&gt;Traditional servers bundle CPU, memory, storage into one box. Need more RAM? Buy a bigger box or add DIMMs if you haven&#x27;t hit the motherboard limit. Don&#x27;t use all your RAM? It sits idle. You can&#x27;t share it.&lt;&#x2F;p&gt;
&lt;p&gt;Memory disaggregation pulls memory out into separate pools that multiple servers can access. Think about how we went from local storage to SAN&#x2F;NAS, but for memory instead.&lt;&#x2F;p&gt;
&lt;p&gt;So what does this actually give you:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Capacity expansion.&lt;&#x2F;strong&gt; A server can use more memory than it physically contains by reaching into the pool. Similar to what Infiniswap does, but with hardware support instead of software paging tricks.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data sharing.&lt;&#x2F;strong&gt; Pool memory can be mapped into multiple hosts at once. They can load&#x2F;store to the same bytes without serializing everything into messages. You still need software to handle ownership and synchronization and failures. But the access itself looks like memory, not network messages.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;why-is-this-coming-up-now&quot;&gt;why is this coming up now&lt;&#x2F;h2&gt;
&lt;p&gt;The economics are getting painful. Memory is like 50% of server cost and 37% of TCO. Three companies control DRAM production. Demand is exploding from data centers and ML and in-memory databases. And here&#x27;s the frustrating part: clusters waste a lot of memory. Over 70% of the time, more than half of aggregate memory sits unused while some machines are paging to disk because they ran out.&lt;&#x2F;p&gt;
&lt;p&gt;The technology also finally exists. RDMA gives single-digit microsecond latencies. But the bigger thing is CXL which gives you cache-coherent load&#x2F;store access over PCIe. Plus theres a roadmap toward switches and pooling and shared memory fabrics.&lt;&#x2F;p&gt;
&lt;p&gt;The pool can even use cheaper denser slower DRAM since it&#x27;s already the &quot;slow tier&quot; compared to local DIMMs anyway.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-i-found-interesting&quot;&gt;what I found interesting&lt;&#x2F;h2&gt;
&lt;p&gt;It&#x27;s not just about capacity. Most remote memory systems like Infiniswap focus on paging to remote RAM. That&#x27;s useful but limited. CXL promises memory that actually looks like memory: load&#x2F;store access to a larger pool. With the right fabric features you could map the same bytes into multiple hosts. That&#x27;s different from shipping pages around.&lt;&#x2F;p&gt;
&lt;p&gt;The OS problems are hard though. The paper is mostly about what&#x27;s unsolved. Memory allocation at scale. Scheduling with memory locality. Pointer sharing across servers. Failure handling for &quot;optional&quot; memory. Security for hot-swappable pools. These need fundamental rethinking.&lt;&#x2F;p&gt;
&lt;p&gt;The timeline matches what happened with storage disaggregation. Start small with few hosts per pool. Add switches for rack-scale. Push the fabric boundary outward. Whether it ends up being &quot;CXL over something&quot; or something else is open. But the trajectory rhymes with how storage disaggregation went.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;where-it-fits-and-where-it-doesn-t&quot;&gt;where it fits and where it doesn&#x27;t&lt;&#x2F;h2&gt;
&lt;p&gt;Good for: data-intensive workloads like Spark or Ray or distributed DBs that spend cycles serializing and copying. Working sets that barely fit in local memory. Clusters with memory imbalance. Places where memory is already 50%+ of server cost.&lt;&#x2F;p&gt;
&lt;p&gt;Bad for: workloads that already fit in local memory (you&#x27;re just adding latency for no reason). Latency-sensitive apps that can&#x27;t handle hundreds of extra nanoseconds. Traditional apps that don&#x27;t share data across processes anyway.&lt;&#x2F;p&gt;
&lt;p&gt;Pool memory is slower than local (hundreds of ns vs ~100ns). But still way faster than SSD or disk. For workloads that currently page to disk, this could be big. For workloads that dont page at all, adding a slower tier might just make things worse.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Paper: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3606557.3606563&quot;&gt;Aguilera et al., &quot;Memory disaggregation: why now and what are the challenges&quot;, ACM SIGOPS, 2023&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Position paper, no benchmarks, just analysis of the problem space&lt;&#x2F;li&gt;
&lt;li&gt;CXL 1.0: local memory expansion cards (shipping now)&lt;&#x2F;li&gt;
&lt;li&gt;CXL 2.0&#x2F;3.0: fabric switches for pool memory (maybe 3-5 years out)&lt;&#x2F;li&gt;
&lt;li&gt;Latency estimates: local ~100ns, CXL local ~200-300ns, CXL pool ~500-1000ns, RDMA ~1-5μs, SSD ~100μs&lt;&#x2F;li&gt;
&lt;li&gt;Memory population rules (balanced channels, identical DIMMs) make upgrades nearly impossible in practice&lt;&#x2F;li&gt;
&lt;li&gt;Distributed shared memory from 90s taught us: cache coherence doesn&#x27;t scale beyond rack&lt;&#x2F;li&gt;
&lt;li&gt;Security: DRAM retains data after power-down and pool memory is hot-swappable so encryption matters&lt;&#x2F;li&gt;
&lt;li&gt;Related work: Infiniswap, LegoOS, The Machine (HPE, discontinued)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Swap and Paging: What Actually Happens When Memory Fills Up</title>
        <published>2025-12-05T00:00:00+00:00</published>
        <updated>2025-12-05T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/swap-paging/"/>
        <id>https://yazeed1s.github.io/posts/swap-paging/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/swap-paging/">&lt;hr &#x2F;&gt;
&lt;p&gt;I kept hitting concepts like &quot;page fault&quot; and &quot;swap&quot; while reading memory disaggregation papers. Figured I should actually understand what these mean at a low level before going further.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-swap-is&quot;&gt;what swap is&lt;&#x2F;h2&gt;
&lt;p&gt;Swap is disk space that acts as overflow for RAM. When physical memory fills up, the kernel moves some data to swap. Later, if that data is needed again, it gets loaded back.&lt;&#x2F;p&gt;
&lt;p&gt;That&#x27;s sort of it. The complication is in the details.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;pages&quot;&gt;pages&lt;&#x2F;h2&gt;
&lt;p&gt;The kernel doesn&#x27;t manage memory byte by byte. Too much bookkeeping. Instead it works in fixed-size chunks called &lt;strong&gt;pages&lt;&#x2F;strong&gt;. Usually 4KB.&lt;&#x2F;p&gt;
&lt;p&gt;When you allocate memory, you get pages. When data moves to disk, it moves as pages. The physical counterpart is called a &lt;strong&gt;frame&lt;&#x2F;strong&gt;. Same size, different name. Pages are virtual, frames are physical.&lt;&#x2F;p&gt;
&lt;p&gt;8GB of RAM = roughly 2 million frames. A process might think it has way more pages than that. Most aren&#x27;t backed by physical memory until used.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;page-tables-and-the-mmu&quot;&gt;page tables and the mmu&lt;&#x2F;h2&gt;
&lt;p&gt;The CPU doesn&#x27;t know about virtual addresses on its own. There&#x27;s a &lt;strong&gt;Memory Management Unit (MMU)&lt;&#x2F;strong&gt; that translates virtual addresses to physical ones.&lt;&#x2F;p&gt;
&lt;p&gt;How does it know the mapping? &lt;strong&gt;Page tables&lt;&#x2F;strong&gt;. Data structures the kernel maintains. The MMU walks these to find where a virtual page actually lives (which physical frame, or if it&#x27;s not in RAM at all).&lt;&#x2F;p&gt;
&lt;p&gt;Walking page tables on every memory access would be slow. So the MMU has a cache called the &lt;strong&gt;TLB (Translation Lookaside Buffer)&lt;&#x2F;strong&gt;. Recent translations are stored there. Hit the TLB = fast. Miss = pay for the walk.&lt;&#x2F;p&gt;
&lt;p&gt;Most accesses hit the TLB. That&#x27;s what makes virtual memory practical.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;page-faults&quot;&gt;page faults&lt;&#x2F;h2&gt;
&lt;p&gt;Program accesses a virtual address. MMU checks: is this page in RAM? If not, you get a &lt;strong&gt;page fault&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Not an error. Just the kernel saying &quot;hold on, I need to go get that.&quot;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Program accesses memory&lt;&#x2F;li&gt;
&lt;li&gt;MMU finds page isn&#x27;t resident&lt;&#x2F;li&gt;
&lt;li&gt;CPU traps to kernel&lt;&#x2F;li&gt;
&lt;li&gt;Kernel figures out where page lives (swap, file, or nowhere)&lt;&#x2F;li&gt;
&lt;li&gt;Kernel loads it into a frame&lt;&#x2F;li&gt;
&lt;li&gt;Page table updated&lt;&#x2F;li&gt;
&lt;li&gt;Program resumes&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Two kinds:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Minor fault.&lt;&#x2F;strong&gt; Page is already somewhere in memory (page cache, shared mapping). Kernel just fixes the page table. Fast.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Major fault.&lt;&#x2F;strong&gt; Page has to be read from disk. Slow. Really slow.&lt;&#x2F;p&gt;
&lt;p&gt;Some faults are expected:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lazy allocation.&lt;&#x2F;strong&gt; Kernel doesn&#x27;t back memory until you touch it. First access = minor fault.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Copy-on-write.&lt;&#x2F;strong&gt; Shared pages aren&#x27;t copied until someone writes.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Swapping.&lt;&#x2F;strong&gt; Page was evicted earlier and now needed again.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Others mean bugs. Accessing garbage address = SIGSEGV.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;paging-in-and-out&quot;&gt;paging in and out&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Page in&lt;&#x2F;strong&gt; = loading a page from disk into RAM.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Page out&lt;&#x2F;strong&gt; = moving a page from RAM to disk to free space.&lt;&#x2F;p&gt;
&lt;p&gt;RAM full. New page needed. Kernel picks a &lt;strong&gt;victim&lt;&#x2F;strong&gt; (some page not accessed recently). If it&#x27;s dirty (modified since loaded), kernel writes it to swap first. If it&#x27;s clean, kernel just drops it and reloads later if needed.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;Before:&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;RAM:  [A][B][C][D] ← full&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;Swap: [empty]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;Need page E. Pick B as victim.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;After:&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;RAM:  [A][E][C][D]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;Swap: [B]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If B is accessed again? Major fault. Load B. Evict something else. This happens constantly under memory pressure.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-dirty-bit&quot;&gt;the dirty bit&lt;&#x2F;h2&gt;
&lt;p&gt;Each page has a &lt;strong&gt;dirty bit&lt;&#x2F;strong&gt;. Set if the page has been modified since it was loaded.&lt;&#x2F;p&gt;
&lt;p&gt;Why it matters: clean pages can be dropped. Kernel can reload from file or wherever. Dirty pages can&#x27;t. Kernel has to write them somewhere first.&lt;&#x2F;p&gt;
&lt;p&gt;Anonymous memory (heap, stack) that&#x27;s dirty goes to swap. File-backed memory that&#x27;s modified goes back to the file (or swap, depends).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;why-disk-access-hurts&quot;&gt;why disk access hurts&lt;&#x2F;h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Access&lt;&#x2F;th&gt;&lt;th&gt;Latency&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;RAM&lt;&#x2F;td&gt;&lt;td&gt;~100 nanoseconds&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;SSD&lt;&#x2F;td&gt;&lt;td&gt;~100 microseconds&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;HDD&lt;&#x2F;td&gt;&lt;td&gt;~10 milliseconds&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;SSD is 1,000× slower than RAM. HDD is 100,000× slower.&lt;&#x2F;p&gt;
&lt;p&gt;Major fault = disk access = program stalls for eternity in CPU time.&lt;&#x2F;p&gt;
&lt;p&gt;One major fault, who cares. Hundred per second, app feels sluggish. Thousand, system unusable.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;thrashing&quot;&gt;thrashing&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Thrashing&lt;&#x2F;strong&gt; is what happens when working set doesn&#x27;t fit in RAM.&lt;&#x2F;p&gt;
&lt;p&gt;Working set = the memory you&#x27;re actively using right now. Bigger than RAM? Kernel constantly swapping pages in and out. Every page you load evicts something you need again soon.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Need page A -&amp;gt; fault, load A, evict B&lt;&#x2F;li&gt;
&lt;li&gt;Need page B -&amp;gt; fault, load B, evict A&lt;&#x2F;li&gt;
&lt;li&gt;Need page A -&amp;gt; fault, load A, evict B&lt;&#x2F;li&gt;
&lt;li&gt;Forever&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;System spends 99% of time moving data. 1% doing work. Disk light stuck on. Mouse barely moves.&lt;&#x2F;p&gt;
&lt;p&gt;This is the performance cliff. Not gradual slowdown. Death spiral.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;lru-picking-victims&quot;&gt;lru: picking victims&lt;&#x2F;h2&gt;
&lt;p&gt;How does kernel decide which page to evict?&lt;&#x2F;p&gt;
&lt;p&gt;Ideal: evict the one that won&#x27;t be needed soonest. Can&#x27;t predict future. So kernel approximates with &lt;strong&gt;LRU (Least Recently Used)&lt;&#x2F;strong&gt;. Pages not accessed in a while are good candidates.&lt;&#x2F;p&gt;
&lt;p&gt;Linux maintains active&#x2F;inactive lists. Pages accessed recently go to active. Cold pages drift to inactive. Reclaim takes from inactive first.&lt;&#x2F;p&gt;
&lt;p&gt;True LRU would track exact access times for every page. Too expensive. Linux settles for &quot;recently used&quot; vs &quot;not recently used.&quot;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;swappiness&quot;&gt;swappiness&lt;&#x2F;h2&gt;
&lt;p&gt;Linux doesn&#x27;t wait until RAM is completely full to start swapping. There&#x27;s a knob called &lt;strong&gt;swappiness&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;shellscript&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;$&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; cat&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;swappiness&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;60&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Range is 0-200. Affects how willing kernel is to swap anonymous memory vs reclaim file cache.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;0&lt;&#x2F;strong&gt; = Avoid swapping. Hold app memory, sacrifice cache.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;60&lt;&#x2F;strong&gt; = Default. Balanced.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;100+&lt;&#x2F;strong&gt; = Swap more. Keep cache warm.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;No universal right answer. Depends on workload.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-can-t-be-swapped&quot;&gt;what can&#x27;t be swapped&lt;&#x2F;h2&gt;
&lt;p&gt;Not everything can go to disk.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Kernel memory.&lt;&#x2F;strong&gt; Page tables, process descriptors, driver state. Must stay in RAM. If kernel got paged out, who pages it back in?&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Pinned memory.&lt;&#x2F;strong&gt; Memory explicitly locked by application (mlock). Used by RDMA, databases, etc.&lt;&#x2F;p&gt;
&lt;p&gt;Kernel memory leaks are dangerous. That memory is gone until reboot.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;overcommit&quot;&gt;overcommit&lt;&#x2F;h2&gt;
&lt;p&gt;Linux lets you allocate more memory than exists. &lt;code&gt;malloc(1GB)&lt;&#x2F;code&gt; succeeds even with 512MB free.&lt;&#x2F;p&gt;
&lt;p&gt;Intentional. Called &lt;strong&gt;overcommit&lt;&#x2F;strong&gt;. Most programs allocate more than they use. Sparse arrays. Forked processes before exec. Refusing would break software.&lt;&#x2F;p&gt;
&lt;p&gt;Downside: actually use all that memory? OOM killer fires. Allocation succeeded, using it didn&#x27;t.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;shellscript&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;$&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; cat&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4A6934, #CB8B8B);&quot;&gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;overcommit_memory&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt;  #&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt; heuristic (default)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt;#&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt; 0 = guess what&amp;#39;s safe&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt;#&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt; 1 = always allow (yolo)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt;#&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt; 2 = strict (refuse if exceeds limit)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Mode 2 is safer but breaks things. Mode 1 is living dangerously.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;why-this-matters-for-remote-memory&quot;&gt;why this matters for remote memory&lt;&#x2F;h2&gt;
&lt;p&gt;Main problem: disk is slow. 1,000-100,000× slower than RAM.&lt;&#x2F;p&gt;
&lt;p&gt;Systems like Infiniswap replace swap with network access to remote memory. RDMA gives single-digit microsecond latency. Still slower than local RAM. But 10-1000× faster than disk.&lt;&#x2F;p&gt;
&lt;p&gt;Keep the paging model. Replace the slow part. Cliff becomes slope.&lt;&#x2F;p&gt;
&lt;p&gt;The interesting thing: Linux has a &lt;strong&gt;frontswap&lt;&#x2F;strong&gt; interface. It&#x27;s a hook that lets you intercept pages before they go to disk. Implement a few callbacks and your module becomes an alternative swap backend. That&#x27;s how Infiniswap plugs into the kernel. Pages that would go to disk get redirected over the network instead.&lt;&#x2F;p&gt;
&lt;p&gt;I want to look at this interface in more detail later. How frontswap works, what the callbacks look like, and what&#x27;s involved in building something like Infiniswap. Different post.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Page size usually 4KB. Huge pages exist (2MB, 1GB) to reduce TLB pressure.&lt;&#x2F;li&gt;
&lt;li&gt;&quot;Anonymous memory&quot; = heap, stack (no backing file). &quot;File-backed&quot; = mmap&#x27;d files, page cache.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;vmstat&lt;&#x2F;code&gt;, &lt;code&gt;sar&lt;&#x2F;code&gt;, &lt;code&gt;&#x2F;proc&#x2F;meminfo&lt;&#x2F;code&gt; for monitoring paging activity.&lt;&#x2F;li&gt;
&lt;li&gt;Swap on SSD helps. Swap on HDD is pain.&lt;&#x2F;li&gt;
&lt;li&gt;zswap = compressed swap cache in RAM. Buys time before hitting disk.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
</feed>
