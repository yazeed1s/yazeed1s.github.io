<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Yazeed&#x27;s Blog - Hardware</title>
    <subtitle>Notes on systems and low-level software.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://yazeed1s.github.io/tags/hardware/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://yazeed1s.github.io"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2026-02-12T00:00:00+00:00</updated>
    <id>https://yazeed1s.github.io/tags/hardware/atom.xml</id>
    <entry xml:lang="en">
        <title>Tiered Memory</title>
        <published>2026-02-12T00:00:00+00:00</published>
        <updated>2026-02-12T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/memory-teiring/"/>
        <id>https://yazeed1s.github.io/posts/memory-teiring/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/memory-teiring/">&lt;p&gt;The OS always assumed memory is uniform. Every page frame is the same speed, same cost, same latency. With CXL and tiered memory that assumption breaks. You now have fast DRAM and slower memory in the same machine.&lt;&#x2F;p&gt;
&lt;p&gt;At first it sounded simple to me. Hot pages go in fast memory, cold pages in slow memory. But from an OS perspective it gets complicated fast.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;first-what-are-memory-pages&quot;&gt;first, what are memory pages&lt;&#x2F;h2&gt;
&lt;p&gt;Before talking about tiers, pages.&lt;&#x2F;p&gt;
&lt;p&gt;Operating systems manage memory in fixed-size chunks called &lt;strong&gt;pages&lt;&#x2F;strong&gt;. On x86 that&#x27;s 4KB. Sometimes you use huge pages (2MB or 1GB), but 4KB is the default.&lt;&#x2F;p&gt;
&lt;p&gt;Basically, the OS doesn&#x27;t think in bytes, it thinks in pages.&lt;&#x2F;p&gt;
&lt;p&gt;When a process allocates memory, the OS maps virtual pages to physical page frames. The page table stores this mapping. CPU sees a virtual address, walks the page tables, translates it to a physical frame.&lt;&#x2F;p&gt;
&lt;p&gt;If a page is not present? Page fault. Memory full? The OS evicts pages.&lt;&#x2F;p&gt;
&lt;p&gt;So when you talk about tiered memory, what you&#x27;re really asking is: which physical page frames should live in which kind of memory? That&#x27;s the core question.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;why-do-we-even-need-tiered-memory&quot;&gt;why do we even need tiered memory&lt;&#x2F;h2&gt;
&lt;p&gt;A server has DRAM directly attached to the CPU through memory channels. Fast, low latency, but expensive.&lt;&#x2F;p&gt;
&lt;p&gt;In large systems memory becomes a serious cost factor. In some cloud setups it&#x27;s a big portion of total server cost. And many applications allocate large heaps but only actively touch part of them.&lt;&#x2F;p&gt;
&lt;p&gt;Scaling DRAM isn&#x27;t trivial either. You&#x27;re limited by channels, DIMM slots, signal integrity.&lt;&#x2F;p&gt;
&lt;p&gt;So the idea behind tiered memory is simple: instead of making all memory equally fast and equally expensive, have a small fast tier and a larger slower tier. Put frequently used pages in fast memory. Put less active pages in slower memory.&lt;&#x2F;p&gt;
&lt;p&gt;Conceptually simple. Implementation is not.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;cxl-and-heterogeneous-memory&quot;&gt;CXL and heterogeneous memory&lt;&#x2F;h2&gt;
&lt;p&gt;With newer interconnects you can attach extra memory that&#x27;s cache-coherent but slower than local DRAM. From the OS perspective it looks like another NUMA node.&lt;&#x2F;p&gt;
&lt;p&gt;But latency is higher. Roughly:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Local DRAM: maybe around 100ns&lt;&#x2F;li&gt;
&lt;li&gt;Attached memory over fabric: maybe 2x or 3x that&lt;&#x2F;li&gt;
&lt;li&gt;Still much faster than SSD or disk&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;So now you have heterogeneous memory inside the same system. Fast tier is local DRAM. Slow tier is attached or remote memory. Same abstraction (page), different performance.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;The &quot;2x or 3x&quot; latency for CXL-attached memory is a rough estimate based on early CXL 1.1&#x2F;2.0 hardware. Actual latency depends on the CXL device type (Type 1, 2, or 3), the number of CXL hops, the controller implementation, and whether the access hits the device&#x27;s internal cache. Some CXL memory expanders report closer to 1.5x for cached accesses. These numbers will keep changing as the hardware matures.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;This breaks the old assumption that memory is uniform.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-core-problem-which-pages-go-where&quot;&gt;the core problem: which pages go where&lt;&#x2F;h2&gt;
&lt;p&gt;If hot pages sit in the fast tier, everything is fine. If hot pages end up in the slow tier, performance drops.&lt;&#x2F;p&gt;
&lt;p&gt;So you need to detect which pages are hot and move them.&lt;&#x2F;p&gt;
&lt;p&gt;The naive idea: count how many times each page is accessed. Most accessed pages are hot.&lt;&#x2F;p&gt;
&lt;p&gt;But it turns out that&#x27;s not enough.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;hotness-is-not-that-simple&quot;&gt;hotness is not that simple&lt;&#x2F;h2&gt;
&lt;p&gt;Frequency alone doesn&#x27;t always tell the full story.&lt;&#x2F;p&gt;
&lt;p&gt;Modern CPUs overlap memory accesses. If several cache misses happen at the same time, the effective stall per access can be smaller. Some accesses hurt more than others, depending on timing and overlap.&lt;&#x2F;p&gt;
&lt;p&gt;So a page can be frequently accessed but not necessarily performance-critical. Another page might be accessed less often but sit directly on the critical path.&lt;&#x2F;p&gt;
&lt;p&gt;Instead of just asking &quot;how many times was this page accessed?&quot;, you probably need to ask &quot;how much does this page slow down the program if it&#x27;s in slow memory?&quot;&lt;&#x2F;p&gt;
&lt;p&gt;That&#x27;s a harder question. I&#x27;m not sure how well current systems actually answer it. It connects OS policy with microarchitecture behavior, and I don&#x27;t think the abstractions we have right now are set up for that.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;page-granularity-mismatch&quot;&gt;page granularity mismatch&lt;&#x2F;h2&gt;
&lt;p&gt;The OS moves memory in 4KB pages. The hardware accesses memory in 64-byte cache lines.&lt;&#x2F;p&gt;
&lt;p&gt;Sometimes only a few cache lines inside a 4KB page are really hot. The rest is barely touched. If you migrate the entire page to the fast tier because a small region inside it is hot, you&#x27;re wasting precious fast memory.&lt;&#x2F;p&gt;
&lt;p&gt;Huge pages make this worse. A 2MB page may contain a small hot region and a lot of cold data. Promoting the whole thing seems expensive.&lt;&#x2F;p&gt;
&lt;p&gt;There&#x27;s a mismatch between OS abstraction (page-based) and real access behavior (cache-line based). Tiered memory just makes the mismatch more visible.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;migration-is-not-free&quot;&gt;migration is not free&lt;&#x2F;h2&gt;
&lt;p&gt;Moving a page between tiers isn&#x27;t just a pointer update. You need to allocate space in the target tier, copy 4KB of data, update page tables, possibly flush TLB entries, and coordinate across cores.&lt;&#x2F;p&gt;
&lt;p&gt;If you migrate too often, or migrate the wrong pages, you can hurt performance instead of improving it. I&#x27;ve seen papers where the migration overhead alone ate most of the benefit.&lt;&#x2F;p&gt;
&lt;p&gt;So it becomes a control problem. You need accurate detection, low tracking overhead, stable decisions, limited oscillation. It starts to feel like scheduling honestly. Continuously adapting to workload behavior, except the feedback signals are noisier.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;where-disaggregation-fits&quot;&gt;where disaggregation fits&lt;&#x2F;h2&gt;
&lt;p&gt;Tiered memory is closely related to disaggregated memory.&lt;&#x2F;p&gt;
&lt;p&gt;Disaggregation means memory can be separated from compute and accessed over a fabric. That memory naturally has higher latency than local DRAM, so it often becomes the slow tier.&lt;&#x2F;p&gt;
&lt;p&gt;At that point memory management isn&#x27;t just a local kernel concern. It interacts with cluster design, resource allocation, and even scheduling across machines. The boundary between OS and infrastructure gets thinner.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes-random-thoughts&quot;&gt;notes &#x2F; random thoughts&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Page abstraction worked well when memory was uniform. Now it feels slightly strained.&lt;&#x2F;li&gt;
&lt;li&gt;Counting accesses is easy. Understanding performance impact is harder. I&#x27;m not convinced anyone has a great solution for this yet.&lt;&#x2F;li&gt;
&lt;li&gt;Huge pages help TLB reach but can complicate tiering decisions.&lt;&#x2F;li&gt;
&lt;li&gt;Migration policy starts to look like a feedback controller.&lt;&#x2F;li&gt;
&lt;li&gt;There&#x27;s always tension between transparency and giving applications more control.&lt;&#x2F;li&gt;
&lt;li&gt;In some sense tiered memory is like swap inside RAM, but at nanosecond scale.&lt;&#x2F;li&gt;
&lt;li&gt;It looks like a small hardware change but from the OS side it touches a lot of assumptions.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>CXL: Why Datacenter Memory is Getting a New Tier</title>
        <published>2026-01-27T00:00:00+00:00</published>
        <updated>2026-01-27T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/cxl/"/>
        <id>https://yazeed1s.github.io/posts/cxl/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/cxl/">&lt;p&gt;DRAM can be half of server cost, and a lot of it still sits idle. One machine thrashes while the one next to it uses 30% of its memory. CXL is trying to fix that mismatch.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-problem&quot;&gt;the problem&lt;&#x2F;h2&gt;
&lt;p&gt;Memory in datacenters is expensive and wasted at the same time. DRAM can be 50% of server cost, and research keeps showing that utilization is terrible. One machine is thrashing because it ran out of memory while another machine next to it is sitting at 30% usage. Some papers claim 70% of aggregate memory is underutilized across a cluster.&lt;&#x2F;p&gt;
&lt;p&gt;The obvious thought: why not share memory across machines, like we do with storage. Pool it. If one server has idle memory another server could use it.&lt;&#x2F;p&gt;
&lt;p&gt;But the issue is that storage can tolerate milliseconds of latency. Memory can&#x27;t. A cache miss to local DRAM is around 100ns. Go over the network with RDMA and you&#x27;re at 1-5 microseconds. That&#x27;s 10-50x slower. For memory access patterns that&#x27;s a lot.&lt;&#x2F;p&gt;
&lt;p&gt;CXL is supposed to bridge this gap.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-it-actually-is&quot;&gt;what it actually is&lt;&#x2F;h2&gt;
&lt;p&gt;CXL stands for Compute Express Link. It runs on the PCIe physical layer, same cables and slots, but with a different protocol on top.&lt;&#x2F;p&gt;
&lt;p&gt;What matters is that it&#x27;s cache-coherent. The CPU can do normal load&#x2F;store to CXL-attached memory through the same memory path, without special APIs, RDMA-style verbs, or memory registration. The memory controller treats it like another memory region, basically another NUMA node.&lt;&#x2F;p&gt;
&lt;p&gt;There are three protocols in the spec. CXL.io is basically just PCIe, for device discovery and config, boring stuff. CXL.cache lets devices cache host memory, useful for accelerators. CXL.mem is the interesting one, it lets the host access device-attached memory with load&#x2F;store.&lt;&#x2F;p&gt;
&lt;p&gt;CXL 1.0 and 1.1 are mostly local expansion. You plug a CXL card with DRAM into a PCIe slot and your system sees more memory. Latency is higher than native DIMMs, maybe 200-300ns instead of 100ns, but it&#x27;s still memory, not storage. CXL 2.0 adds switching so multiple hosts can share a memory pool. CXL 3.0 goes further with fabric and shared memory semantics.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mixing-memory-types&quot;&gt;mixing memory types&lt;&#x2F;h2&gt;
&lt;p&gt;Normally your CPU&#x27;s memory controller dictates what DRAM you can use. If it&#x27;s a DDR5 system, all your DIMMs have to be DDR5. Same speed, same density rules, same timing specs. You can&#x27;t just plug DDR4 into a DDR5 slot.&lt;&#x2F;p&gt;
&lt;p&gt;CXL breaks this because the CXL device has its own memory controller. It can use whatever DRAM it wants. DDR4, DDR5, older cheaper stuff, slower but denser modules. The CPU doesn&#x27;t care. It just sees CXL memory at some address range.&lt;&#x2F;p&gt;
&lt;p&gt;So you could keep local DDR5 for hot data and use a CXL card with cheaper DDR4 as a slower tier, or use high-capacity modules that wouldn&#x27;t fit your motherboard timing rules. Cost-wise this is nice because you&#x27;re not locked to whatever generation the motherboard supports.&lt;&#x2F;p&gt;
&lt;p&gt;The tradeoff is latency. CXL adds overhead. But if you&#x27;re using it for capacity expansion rather than latency-critical paths, maybe that&#x27;s acceptable.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;why-not-just-use-rdma&quot;&gt;why not just use RDMA&lt;&#x2F;h2&gt;
&lt;p&gt;RDMA is a different model. You need explicit verbs, post work requests, poll completions. It&#x27;s not transparent load&#x2F;store. You have to register memory, pin pages, exchange keys. One-sided operations are async so you don&#x27;t know when remote writes land unless you add signaling. And latency is around 1-5μs which is fast for networking but slow for memory access patterns.&lt;&#x2F;p&gt;
&lt;p&gt;CXL at 200-500ns for pooled memory is closer to local DRAM territory. And it&#x27;s transparent to software. Your malloc can return CXL memory and the application doesn&#x27;t know the difference.&lt;&#x2F;p&gt;
&lt;p&gt;That&#x27;s the promise anyway. The hardware shipping today is mostly local expansion cards, not pooled memory. The pooling stuff is still coming hopefully.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-latency-thing&quot;&gt;the latency thing&lt;&#x2F;h2&gt;
&lt;p&gt;Local DRAM is ~100ns. CXL local expansion is ~200-300ns. CXL through a switch to a shared pool is ~500-1000ns.&lt;&#x2F;p&gt;
&lt;p&gt;So pooled CXL is 5-10x slower than local. That&#x27;s not nothing. For tight loops constantly hitting memory, that seems expensive. The pitch is that it&#x27;s still way better than swapping to SSD (100μs) and you get more capacity. Which is true.&lt;&#x2F;p&gt;
&lt;p&gt;The mental model is tiering: hot data in local DRAM, warm data in the CXL pool, cold data on SSD, and the kernel (or runtime) migrating pages based on access patterns.&lt;&#x2F;p&gt;
&lt;p&gt;Linux already has machinery for this. NUMA balancing, DAMON for access pattern detection, tiered memory support that got merged recently. Whether this works well in practice with real workloads, I don&#x27;t know yet. The theory sounds reasonable but there will be edge cases.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shared-memory-across-hosts&quot;&gt;shared memory across hosts&lt;&#x2F;h2&gt;
&lt;p&gt;CXL 3.0 talks about multiple hosts accessing the same memory with hardware-maintained cache coherence.&lt;&#x2F;p&gt;
&lt;p&gt;This sounds amazing and also scary at the same time.&lt;&#x2F;p&gt;
&lt;p&gt;Cache coherence doesn&#x27;t scale. The distributed shared memory people learned this in the 90s. Beyond a few nodes the coherence traffic overwhelms everything.&lt;&#x2F;p&gt;
&lt;p&gt;The CXL spec people know this. The scope is limited, maybe a rack, maybe a pod, maybe less. The vision isn&#x27;t coherent memory across the whole datacenter. It&#x27;s more like, within a small group of machines you can have shared memory semantics. Beyond that you&#x27;re back to message passing or RDMA.&lt;&#x2F;p&gt;
&lt;p&gt;Even rack-scale shared memory is interesting though. Databases that want to share buffer caches across replicas. ML training jobs that share model weights. There are use cases.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-s-actually-shipping&quot;&gt;what&#x27;s actually shipping&lt;&#x2F;h2&gt;
&lt;p&gt;CXL 1.1 memory expanders exist today from Samsung, SK Hynix and others. Intel Sapphire Rapids supports CXL. These are mostly used to add capacity to memory-hungry workloads.&lt;&#x2F;p&gt;
&lt;p&gt;CXL switches are not really production-ready yet. Some prototypes. I&#x27;d guess pooled CXL deployments are 2-3 years out.&lt;&#x2F;p&gt;
&lt;p&gt;So when papers say &quot;CXL will enable this,&quot; they&#x27;re often talking about future hardware. The concepts are solid but the ecosystem is young. Worth understanding now because it&#x27;s coming, but don&#x27;t expect to deploy pooled CXL next month.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-i-m-still-uncertain-about&quot;&gt;what I&#x27;m still uncertain about&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Latency tradeoffs.&lt;&#x2F;strong&gt; 5-10x slower than local is real overhead. Better than SSD, yes. But memory-intensive applications might just thrash the CXL tier and make things worse. Tiering policies need to actually work.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Ecosystem maturity.&lt;&#x2F;strong&gt; RDMA took years to get right. CXL is newer. Drivers, kernel support, allocation policies, debugging tools, all of this needs to catch up.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Who benefits.&lt;&#x2F;strong&gt; Big cloud providers with massive memory imbalance probably see value. Smaller deployments might not see ROI at current hardware costs.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;CXL consortium includes Intel, AMD, ARM, NVIDIA, Samsung and others&lt;&#x2F;li&gt;
&lt;li&gt;Built on PCIe 5.0&#x2F;6.0 physical layer, same slots and cables&lt;&#x2F;li&gt;
&lt;li&gt;Latency numbers vary by source and topology&lt;&#x2F;li&gt;
&lt;li&gt;Linux kernel has CXL support in drivers&#x2F;cxl&#x2F;, device enumeration works, memory tiering is evolving&lt;&#x2F;li&gt;
&lt;li&gt;Related specs: Gen-Z (seems dead), CCIX (absorbed into CXL)&lt;&#x2F;li&gt;
&lt;li&gt;Good starting point: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;www.computeexpresslink.org&#x2F;&quot;&gt;CXL Consortium&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;For context on memory disaggregation: Aguilera et al., &quot;Memory disaggregation: why now and what are the challenges&quot;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
</feed>
