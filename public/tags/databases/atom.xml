<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Yazeed&#x27;s Blog - Databases</title>
    <subtitle>Notes on systems and low-level software.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://yazeed1s.github.io/tags/databases/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://yazeed1s.github.io"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2025-12-18T00:00:00+00:00</updated>
    <id>https://yazeed1s.github.io/tags/databases/atom.xml</id>
    <entry xml:lang="en">
        <title>Why Databases Stopped Using mmap</title>
        <published>2025-12-18T00:00:00+00:00</published>
        <updated>2025-12-18T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/mmap-databases/"/>
        <id>https://yazeed1s.github.io/posts/mmap-databases/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/mmap-databases/">&lt;p&gt;mmap lets you map a file into your address space and access it like memory through pointer dereferences instead of &lt;code&gt;read()&lt;&#x2F;code&gt; calls and user-space buffers. The OS handles paging transparently.&lt;&#x2F;p&gt;
&lt;p&gt;For a database, this looks perfect at first: map data files, access pages through pointers, let the kernel decide what stays in memory, and skip writing a buffer pool. Several databases tried it, and most backed away from it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;why-it-s-tempting&quot;&gt;why it&#x27;s tempting&lt;&#x2F;h2&gt;
&lt;p&gt;A traditional DBMS maintains its own buffer pool where it tracks which pages are in memory, decides what to evict, and handles I&#x2F;O explicitly, which is a lot of code (thousands of lines just to manage what&#x27;s cached).&lt;&#x2F;p&gt;
&lt;p&gt;With mmap you skip all that because the kernel already has a page cache, already tracks access patterns, and already evicts pages under memory pressure, so the question becomes: why duplicate it?&lt;&#x2F;p&gt;
&lt;p&gt;LMDB does it, MongoDB used to do it, LevelDB did it, MonetDB did it, and SQLite has an mmap mode, so the idea is clearly attractive.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-transactional-safety-problem&quot;&gt;the transactional safety problem&lt;&#x2F;h2&gt;
&lt;p&gt;The OS can flush dirty pages to disk whenever it wants. You don&#x27;t control when.&lt;&#x2F;p&gt;
&lt;p&gt;If your DBMS modifies a page through the mmap&#x27;d region, that change can hit disk before the transaction commits. Crash at the wrong time and your database is inconsistent. You&#x27;ve violated durability, or atomicity, or both.&lt;&#x2F;p&gt;
&lt;p&gt;A buffer pool doesn&#x27;t have this problem because pages live in user-space memory and the DBMS decides when to write them to disk, always writing the WAL first then the data pages, so control flow is explicit.&lt;&#x2F;p&gt;
&lt;p&gt;With mmap, you need workarounds. MongoDB&#x27;s MMAPv1 engine used &lt;code&gt;MAP_PRIVATE&lt;&#x2F;code&gt; to create a copy-on-write workspace. Two copies of the database in memory. SQLite copies pages to user-space buffers before modifying them, which defeats the purpose of mmap. LMDB uses shadow paging, which forces single-writer concurrency.&lt;&#x2F;p&gt;
&lt;p&gt;All of these are complex. And all of them give back the simplicity that mmap was supposed to provide.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;i-o-stalls-you-can-t-control&quot;&gt;I&#x2F;O stalls you can&#x27;t control&lt;&#x2F;h2&gt;
&lt;p&gt;When you access an mmap&#x27;d page that&#x27;s been evicted you get a page fault and the thread blocks until the OS reads the page from disk.&lt;&#x2F;p&gt;
&lt;p&gt;You can&#x27;t do anything about this since there isn&#x27;t an async page-fault interface to say &quot;I&#x27;m going to need this page soon, start loading it,&quot; the thread just stops.&lt;&#x2F;p&gt;
&lt;p&gt;With a buffer pool, you control I&#x2F;O explicitly. You can use &lt;code&gt;io_uring&lt;&#x2F;code&gt; or &lt;code&gt;libaio&lt;&#x2F;code&gt; for async reads. You can prefetch pages you know you&#x27;ll need. A B+tree range scan can issue reads for the next few leaf pages ahead of time.&lt;&#x2F;p&gt;
&lt;p&gt;With mmap, a range scan hits a page fault on every cold page. Sequentially. Each one blocks. You can try &lt;code&gt;madvise(MADV_SEQUENTIAL)&lt;&#x2F;code&gt; but it&#x27;s a hint, not a guarantee, and it doesn&#x27;t help for non-sequential access patterns.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;madvise&lt;&#x2F;code&gt; behavior varies between kernel versions and is not standardized across operating systems. On Linux, &lt;code&gt;MADV_SEQUENTIAL&lt;&#x2F;code&gt; triggers aggressive readahead and drops pages behind, but the readahead window size and eviction behavior are kernel implementation details that can change. Don&#x27;t rely on specific behavior across versions.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;error-handling-gets-weird&quot;&gt;error handling gets weird&lt;&#x2F;h2&gt;
&lt;p&gt;With a buffer pool error handling is centralized: you read a page, check the checksum, handle I&#x2F;O errors, all in one place.&lt;&#x2F;p&gt;
&lt;p&gt;With mmap pages can be evicted and reloaded transparently, so you&#x27;d need to verify checksums on every access not just the first read. An I&#x2F;O error during transparent page-in doesn&#x27;t return an error code either, it raises SIGBUS, which means your error handling is now a signal handler scattered across the codebase.&lt;&#x2F;p&gt;
&lt;p&gt;If a page in your buffer gets corrupted you catch it before writing to disk, but with mmap the OS can flush a corrupted page without asking, which is silent data corruption.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-performance-collapse&quot;&gt;the performance collapse&lt;&#x2F;h2&gt;
&lt;p&gt;This is the part that surprised me. A CIDR 2022 paper by Crotty, Leis, and Pavlo benchmarked mmap against traditional I&#x2F;O and the results are bad.&lt;&#x2F;p&gt;
&lt;p&gt;Random reads on a 2TB dataset with 100GB of page cache (so ~95% of accesses are page faults):&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Traditional I&#x2F;O with &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt;: stable ~900K reads&#x2F;sec&lt;&#x2F;li&gt;
&lt;li&gt;mmap: starts fine, then collapses to near-zero when the page cache fills and eviction kicks in. Recovers to about half the throughput of traditional I&#x2F;O&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The collapse happens because of TLB shootdowns. When the kernel evicts a page, it has to invalidate the TLB entry on every CPU core that might have it cached. CPUs don&#x27;t keep TLB entries coherent automatically. The kernel sends inter-processor interrupts—thousands of cycles each. Under heavy eviction, TLB shootdowns hit 2 million per second.&lt;&#x2F;p&gt;
&lt;p&gt;Sequential scans on 10 NVMe SSDs in RAID 0: mmap gets ~3 GB&#x2F;s. Traditional I&#x2F;O gets ~60 GB&#x2F;s. That&#x27;s 20x worse. And mmap showed basically no improvement going from 1 SSD to 10. It can&#x27;t scale with modern storage bandwidth because the bottleneck is in the kernel&#x27;s page eviction path, not the drives.&lt;&#x2F;p&gt;
&lt;p&gt;The page eviction itself is another problem. Linux uses a single kswapd thread per NUMA node. Under high I&#x2F;O pressure it becomes the bottleneck. And the page table is a shared data structure that all threads hit during faults, creating contention.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-graveyard&quot;&gt;the graveyard&lt;&#x2F;h2&gt;
&lt;p&gt;The paper tracks which databases tried mmap and what happened:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MongoDB&lt;&#x2F;strong&gt; deprecated MMAPv1 in 2015, removed it in 2019. Couldn&#x27;t compress data, too complex to maintain.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;InfluxDB&lt;&#x2F;strong&gt; replaced mmap after severe I&#x2F;O spikes when databases exceeded a few GB.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;SingleStore&lt;&#x2F;strong&gt; found mmap calls took 10-20ms per query from write lock contention.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;RocksDB&lt;&#x2F;strong&gt; exists partly because LevelDB&#x27;s mmap usage had performance bottlenecks.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;TileDB, Scylla, VictoriaMetrics&lt;&#x2F;strong&gt; all evaluated mmap during development and rejected it.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;when-mmap-is-fine&quot;&gt;when mmap is fine&lt;&#x2F;h2&gt;
&lt;p&gt;If your entire dataset fits in memory and you&#x27;re read-only, mmap works: eviction never kicks in, so you avoid TLB shootdowns, and transactional write hazards don&#x27;t apply. LMDB operates in this sweet spot for some workloads.&lt;&#x2F;p&gt;
&lt;p&gt;But if your data exceeds memory, or you need writes with ACID guarantees, or you want to use fast storage at full bandwidth, mmap is the wrong tool.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-this-really-comes-down-to&quot;&gt;what this really comes down to&lt;&#x2F;h2&gt;
&lt;p&gt;For me this comes down to one thing: the OS page cache is general-purpose while databases need very specific control. General-purpose is fine for generic workloads, but databases have specific access patterns, durability rules, and error-handling paths that the OS cannot infer.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s similar to the tiered memory problem where the OS tries to manage page placement transparently, but transparency breaks down when the application knows something the kernel doesn&#x27;t. Buffer pool vs mmap is the same tension: do you trust the OS abstraction, or do you manage things yourself because you know your workload better?&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Paper: &lt;a rel=&quot;external&quot; href=&quot;https:&#x2F;&#x2F;db.cs.cmu.edu&#x2F;papers&#x2F;2022&#x2F;cidr2022-p13-crotty.pdf&quot;&gt;Crotty, Leis, Pavlo — &quot;Are You Sure You Want to Use MMAP in Your Database Management System?&quot;, CIDR 2022&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Andy Pavlo has a lecture on this topic too. Worth watching if you want the full rant.&lt;&#x2F;li&gt;
&lt;li&gt;TLB shootdowns are also a cost in page migration for tiered memory. Same mechanism, different context.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;O_DIRECT&lt;&#x2F;code&gt; bypasses the page cache entirely, which is why buffer pool implementations prefer it. The DBMS manages its own cache.&lt;&#x2F;li&gt;
&lt;li&gt;PostgreSQL has never used mmap for data access. It has its own buffer pool (shared_buffers). This was the right call.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>A Buffer Pool Is Just Paging in User Space</title>
        <published>2025-12-02T00:00:00+00:00</published>
        <updated>2025-12-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/buffer-pools/"/>
        <id>https://yazeed1s.github.io/posts/buffer-pools/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/buffer-pools/">&lt;p&gt;A database buffer pool manages fixed-size pages in memory, decides which ones to keep and which to evict, tracks dirty pages, and writes them back to disk on its own schedule.&lt;&#x2F;p&gt;
&lt;p&gt;That&#x27;s what the OS virtual memory system does. Page frames, page tables, eviction policies, dirty bit tracking, write-back. The database reimplements all of it. In user space.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-os-already-does-this&quot;&gt;the OS already does this&lt;&#x2F;h2&gt;
&lt;p&gt;The kernel manages physical memory in page frames (4KB). It maps virtual pages to physical frames through page tables. When memory is full, it evicts cold pages to disk. When a process touches an evicted page, it faults and the kernel loads it back. It tracks which pages are dirty and writes them back when it needs to.&lt;&#x2F;p&gt;
&lt;p&gt;This is the exact same problem a database has. The database has pages on disk. Some of them need to be in memory. Not all of them fit. The database needs to decide which pages to keep, which to evict, and when to write dirty ones back.&lt;&#x2F;p&gt;
&lt;p&gt;So why not just let the OS handle it? Map the database file with mmap and let the kernel manage everything. Some databases tried this. &lt;a href=&quot;https:&#x2F;&#x2F;yazeed1s.github.io&#x2F;posts&#x2F;mmap-databases&#x2F;&quot;&gt;It didn&#x27;t go well&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;why-databases-reimplement-it&quot;&gt;why databases reimplement it&lt;&#x2F;h2&gt;
&lt;p&gt;The OS page cache is general purpose. It has no concept of index pages vs temporary sort pages, no awareness that a range scan is about to need the next 50 pages, and no understanding that a dirty page has to hit the WAL before it hits the data file.&lt;&#x2F;p&gt;
&lt;p&gt;A buffer pool knows all of that.&lt;&#x2F;p&gt;
&lt;p&gt;The database builds its own page table: a hash map from &lt;code&gt;(file_id, page_number)&lt;&#x2F;code&gt; to a frame in the buffer pool. When a query needs a page, it looks up the hash map. If the page is there, it returns the pointer. If not, it picks a frame to evict, reads the page from disk into that frame, and updates the map.&lt;&#x2F;p&gt;
&lt;p&gt;Page fault, but in user space. Controlled entirely by the database.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-anatomy-of-a-buffer-pool&quot;&gt;the anatomy of a buffer pool&lt;&#x2F;h2&gt;
&lt;p&gt;The structure is simple:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Frame array&lt;&#x2F;strong&gt;: a fixed-size array of page-sized slots in memory (the &quot;RAM&quot; of the buffer pool).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Page table&lt;&#x2F;strong&gt;: a hash map from page ID to frame index (how the database translates a logical page reference into a memory location).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Eviction policy&lt;&#x2F;strong&gt;: decides which frame to reclaim when the pool is full (LRU, clock, LRU-K).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Dirty flag&lt;&#x2F;strong&gt;: each frame tracks whether its contents have been modified since it was read from disk.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Pin count&lt;&#x2F;strong&gt;: tracks how many operations are currently using a frame. A pinned page can&#x27;t be evicted (same idea as the kernel&#x27;s page reference count).&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;When a page is requested:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Check the page table. If the page is already in a frame, pin it and return the pointer.&lt;&#x2F;li&gt;
&lt;li&gt;If not, find a victim frame (eviction policy). If the victim is dirty, write it to disk first.&lt;&#x2F;li&gt;
&lt;li&gt;Read the requested page from disk into the victim frame.&lt;&#x2F;li&gt;
&lt;li&gt;Update the page table. Return the pointer.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;That&#x27;s page fault, find victim, write back if dirty, read page, update mapping. Same flow as an OS page fault handler, different layer.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;eviction-the-database-knows-more&quot;&gt;eviction: the database knows more&lt;&#x2F;h2&gt;
&lt;p&gt;The OS uses something like clock or a modified LRU. It works across all processes, all files, all pages. It has no application-level knowledge.&lt;&#x2F;p&gt;
&lt;p&gt;A database can do better because it knows the access patterns.&lt;&#x2F;p&gt;
&lt;p&gt;A sequential scan will touch every page once. An LRU policy would fill the cache with scan pages and evict hot index pages. PostgreSQL handles this by using a small ring buffer for sequential scans, so scan pages cycle through a handful of frames instead of polluting the whole pool.&lt;&#x2F;p&gt;
&lt;p&gt;A B+tree lookup traverses root, internal, then leaf. The root page is accessed on every lookup. It should basically never be evicted. LRU handles this naturally, but a database can also pin critical pages explicitly.&lt;&#x2F;p&gt;
&lt;p&gt;Prefetching works better too. The database knows it&#x27;s doing a range scan on a B+tree. It can issue async reads for the next few leaf pages before it needs them. The OS page cache can&#x27;t do this because it only sees physical file offsets, not logical access patterns.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;dirty-pages-and-write-back&quot;&gt;dirty pages and write-back&lt;&#x2F;h2&gt;
&lt;p&gt;This is where the difference matters most.&lt;&#x2F;p&gt;
&lt;p&gt;The OS can flush a dirty page to disk whenever it wants. That&#x27;s fine for normal files. For a database, it&#x27;s dangerous. If a modified data page hits disk before the corresponding WAL record, crash recovery breaks. This is the write-ahead logging rule: log first, data page second.&lt;&#x2F;p&gt;
&lt;p&gt;A buffer pool enforces this. Before writing a dirty page back to disk, it checks that the WAL has been flushed up to the page&#x27;s last modification LSN (Log Sequence Number). The page doesn&#x27;t go to disk until its log records are safe.&lt;&#x2F;p&gt;
&lt;p&gt;This is impossible with mmap. The kernel has no concept of WAL ordering or LSNs. It flushes when it feels like it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;o-direct-bypassing-the-os-page-cache&quot;&gt;O_DIRECT: bypassing the OS page cache&lt;&#x2F;h2&gt;
&lt;p&gt;Most serious databases open their files with &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt;. This tells the kernel to skip its own page cache entirely. Reads and writes go straight between the database&#x27;s buffer pool and the disk.&lt;&#x2F;p&gt;
&lt;p&gt;Without &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt;, you&#x27;d have the data in two places: once in the database&#x27;s buffer pool and once in the OS page cache. Double the memory usage for no benefit. The database already manages caching. The OS cache is redundant.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;O_DIRECT&lt;&#x2F;code&gt; also gives the database precise control over I&#x2F;O timing, no surprises from kernel write-back threads or memory pressure from the kernel evicting buffer pool pages.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;O_DIRECT&lt;&#x2F;code&gt; isn&#x27;t free to use. It requires buffers to be aligned to the filesystem block size (usually 512 bytes or 4KB), and I&#x2F;O sizes must also be aligned. If you get the alignment wrong, the syscall fails with EINVAL. This is why most databases that use &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt; implement their own aligned allocation routines.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;PostgreSQL is an exception. It uses the OS page cache (buffered I&#x2F;O) rather than &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt;, and relies on &lt;code&gt;fsync&lt;&#x2F;code&gt; to force data to disk. It simplifies some things but means PostgreSQL competes with the OS for memory management control.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;it-s-the-same-problem-at-a-different-layer&quot;&gt;it&#x27;s the same problem at a different layer&lt;&#x2F;h2&gt;
&lt;p&gt;The parallel is almost exact:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;OS Virtual Memory&lt;&#x2F;th&gt;&lt;th&gt;Database Buffer Pool&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Physical page frame&lt;&#x2F;td&gt;&lt;td&gt;Buffer pool frame&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Page table (virtual to physical)&lt;&#x2F;td&gt;&lt;td&gt;Page table (page ID to frame)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Page fault handler&lt;&#x2F;td&gt;&lt;td&gt;Buffer pool miss handler&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Dirty bit in PTE&lt;&#x2F;td&gt;&lt;td&gt;Dirty flag per frame&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Reference count&lt;&#x2F;td&gt;&lt;td&gt;Pin count&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;kswapd (page reclaim)&lt;&#x2F;td&gt;&lt;td&gt;Eviction policy (LRU, clock)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Swap file&lt;&#x2F;td&gt;&lt;td&gt;Data file on disk&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;code&gt;write-back&lt;&#x2F;code&gt; flush&lt;&#x2F;td&gt;&lt;td&gt;WAL-ordered write-back&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The database takes this responsibility away from the OS because general-purpose policies don&#x27;t work for database workloads. Eviction needs access-pattern awareness. Write-back needs WAL ordering. Prefetching needs query-plan knowledge. The OS has none of this context.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;InnoDB (MySQL) uses a buffer pool with an LRU that splits into &quot;young&quot; and &quot;old&quot; sublists. New pages enter the old sublist, and only move to the young sublist if accessed again. This handles the scan-pollution problem.&lt;&#x2F;li&gt;
&lt;li&gt;PostgreSQL&#x27;s shared_buffers is its buffer pool. It uses a clock-sweep eviction policy.&lt;&#x2F;li&gt;
&lt;li&gt;SQLite in WAL mode maintains its own page cache but sits on top of the OS page cache (no O_DIRECT). It works because SQLite targets small-to-medium databases where double-caching isn&#x27;t expensive.&lt;&#x2F;li&gt;
&lt;li&gt;The buffer pool is one of the first things a database student builds. It&#x27;s simple in concept and brutal in the details (concurrency, latch ordering, I&#x2F;O scheduling).&lt;&#x2F;li&gt;
&lt;li&gt;Some databases are experimenting with letting the buffer pool manage allocation at finer granularity than pages. But pages have stuck around because they align with disk I&#x2F;O boundaries.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
</feed>
