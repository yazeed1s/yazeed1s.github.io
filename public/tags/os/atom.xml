<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Yazeed&#x27;s Blog - OS</title>
    <subtitle>Notes on systems and low-level software.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://yazeed1s.github.io/tags/os/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://yazeed1s.github.io"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2026-02-16T00:00:00+00:00</updated>
    <id>https://yazeed1s.github.io/tags/os/atom.xml</id>
    <entry xml:lang="en">
        <title>Should malloc Know About Tiered Memory?</title>
        <published>2026-02-16T00:00:00+00:00</published>
        <updated>2026-02-16T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/malloc-to-teired/"/>
        <id>https://yazeed1s.github.io/posts/malloc-to-teired/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/malloc-to-teired/">&lt;p&gt;When you call &lt;code&gt;malloc()&lt;&#x2F;code&gt;, the allocator gives you a pointer. It doesn&#x27;t know or care whether the physical page behind it sits in fast local DRAM or slower CXL-attached memory. From user space, memory still looks flat. But it isn&#x27;t anymore. Machines now have 2â€“3x latency differences between memory tiers, and the allocator is completely blind to that.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-glibc-malloc-sees-the-world&quot;&gt;how glibc malloc sees the world&lt;&#x2F;h2&gt;
&lt;p&gt;glibc&#x27;s allocator (ptmalloc2) was designed in a mostly uniform DRAM world. It manages arenas, splits and coalesces chunks, decides when to use &lt;code&gt;brk&lt;&#x2F;code&gt; and when to use &lt;code&gt;mmap&lt;&#x2F;code&gt;, and tries to reduce lock contention between threads. But it doesn&#x27;t care about which NUMA node backs an allocation unless the application explicitly asks for it. In the common case, it just requests virtual memory and leaves physical placement to the kernel.&lt;&#x2F;p&gt;
&lt;p&gt;So from the allocator&#x27;s perspective, memory is virtual address space. It doesn&#x27;t know whether the physical pages will come from local DRAM, remote NUMA, CXL-attached memory, or something else. That blindness was perfectly reasonable when latency differences were small and mostly about bandwidth balancing. The allocator could afford to ignore placement because the hardware was close to uniform.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-tiered-memory-changes&quot;&gt;what tiered memory changes&lt;&#x2F;h2&gt;
&lt;p&gt;In tiered memory systems, the kernel often treats slow memory as another NUMA node. It may demote cold pages to slow memory and promote hot pages back to fast DRAM. Research systems like TPP migrate pages based on observed access frequency, and Memtis tries to improve classification by looking at access distribution and even splitting huge pages when access inside them is skewed.&lt;&#x2F;p&gt;
&lt;p&gt;But the pattern is always the same: allocate first, observe later, migrate if needed. The allocator places data somewhere, the kernel watches page faults or samples accesses, then corrects the placement. We&#x27;re always reacting.&lt;&#x2F;p&gt;
&lt;p&gt;Migration isn&#x27;t free. It involves copying 4KB pages, updating page tables, invalidating TLB entries, and potentially disturbing caches. Work like M5 shows that misclassification and migration overhead can actually hurt performance if not handled carefully. So you&#x27;re paying a correction cost because the initial allocation was blind. I keep wondering how much of this cost could be avoided if the allocator had any information at all about what&#x27;s hot.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-allocator-knows-nothing-about-temperature&quot;&gt;the allocator knows nothing about temperature&lt;&#x2F;h2&gt;
&lt;p&gt;Consider a simple program:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;c&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span&gt;hot_table &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt; malloc&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; &amp;lt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 20&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt;   &#x2F;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt; frequently accessed&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span&gt;log_buffer &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt; malloc&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; &amp;lt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 20&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt;  &#x2F;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt; rarely accessed&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span&gt;archive &lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt; malloc&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt;100&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; &amp;lt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #CA9D7D);&quot;&gt; 20&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt;   &#x2F;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7A7D7A, #7F7C77);font-style: italic;&quot;&gt; mostly cold&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;From glibc&#x27;s perspective, these are identical calls. Same API, same path. But their temperature is completely different. The allocator has no way to express or detect that difference.&lt;&#x2F;p&gt;
&lt;p&gt;The application often already knows which data is critical. A database knows its buffer pool is hot. A web server knows which structures sit in the request path. A compiler knows which tables are heavily reused. Yet we force the kernel to guess using access bits and heuristics.&lt;&#x2F;p&gt;
&lt;p&gt;That naturally leads to the question: are we solving the problem too late?&lt;&#x2F;p&gt;
&lt;h2 id=&quot;should-malloc-become-tier-aware&quot;&gt;should malloc become tier-aware?&lt;&#x2F;h2&gt;
&lt;p&gt;One idea, not that exotic: let the allocator express intent.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;c&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;malloc_hot&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;size_t&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7D4242, #D4D4C0);&quot;&gt; size&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;void&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #CB8B8B);&quot;&gt; *&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#3C6362, #A2BD90);&quot;&gt;malloc_cold&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#784367, #D4B399);&quot;&gt;size_t&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#7D4242, #D4D4C0);&quot;&gt; size&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color: light-dark(#4B4B48, #7F7C77);&quot;&gt;;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Internally, &lt;code&gt;malloc_hot()&lt;&#x2F;code&gt; could bind memory to the fast NUMA node using mechanisms that already exist, like &lt;code&gt;mbind()&lt;&#x2F;code&gt; or &lt;code&gt;set_mempolicy()&lt;&#x2F;code&gt;. &lt;code&gt;malloc_cold()&lt;&#x2F;code&gt; could allocate directly on the slow tier. Instead of allocate -&amp;gt; detect -&amp;gt; migrate, you&#x27;d allocate correctly from the start.&lt;&#x2F;p&gt;
&lt;p&gt;This avoids some migration entirely. Fewer TLB shootdowns, less page copying. Placement becomes a proactive decision rather than a reactive correction.&lt;&#x2F;p&gt;
&lt;p&gt;But now the deeper question comes up.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;is-os-transparency-still-sacred&quot;&gt;is OS transparency still sacred?&lt;&#x2F;h2&gt;
&lt;p&gt;Virtual memory was designed to hide physical placement. That abstraction is powerful because developers don&#x27;t need to care where bytes live. They just allocate and use.&lt;&#x2F;p&gt;
&lt;p&gt;Tiered memory challenges that. When latency differences become large enough, placement starts to matter again.&lt;&#x2F;p&gt;
&lt;p&gt;You can keep full transparency. The kernel observes access patterns and tries to infer temperature. Developers stay insulated. The system grows more complex internally, with more sampling and migration.&lt;&#x2F;p&gt;
&lt;p&gt;Or you can leak some abstraction. Let developers label allocations as hot or cold. Trust applications to express intent, and let the allocator participate in placement.&lt;&#x2F;p&gt;
&lt;p&gt;I honestly don&#x27;t know which is better. The second approach sounds cleaner until you think about what happens when developers misclassify. What if everything gets labeled &quot;fast&quot;? Do you override their hints? Ignore them? Once you expose placement, you also expose responsibility, and most application developers probably don&#x27;t want that.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;huge-pages-make-it-worse&quot;&gt;huge pages make it worse&lt;&#x2F;h2&gt;
&lt;p&gt;Memtis shows that access inside a 2MB huge page can be highly skewed. Promoting an entire huge page to fast memory because a small region is hot wastes precious capacity. The allocator doesn&#x27;t know how its allocations align with huge pages. The kernel may split or merge them later.&lt;&#x2F;p&gt;
&lt;p&gt;So page size, allocation strategy, and tier placement are all interacting and I&#x27;m not sure anyone has a clean model for how they should interact. The original layering between allocator and kernel assumed these things were independent. They&#x27;re not anymore.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;a-possible-middle-ground&quot;&gt;a possible middle ground&lt;&#x2F;h2&gt;
&lt;p&gt;I don&#x27;t think we should abandon OS transparency completely. It&#x27;s still valuable, especially for most applications that don&#x27;t care about deep performance tuning. But maybe transparency should become adjustable.&lt;&#x2F;p&gt;
&lt;p&gt;By default, memory stays abstract. The kernel handles tiering. But for performance-critical systems, the allocator could expose controlled hints, and the kernel could enforce limits so that misclassification doesn&#x27;t destabilize things.&lt;&#x2F;p&gt;
&lt;p&gt;I don&#x27;t know if this would hold up in real systems. It depends on how well the kernel can override bad hints, and on whether developers will bother annotating allocations. My guess is most people ignore it and a small group gets real value out of it.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>A Buffer Pool Is Just Paging in User Space</title>
        <published>2025-12-02T00:00:00+00:00</published>
        <updated>2025-12-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/buffer-pools/"/>
        <id>https://yazeed1s.github.io/posts/buffer-pools/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/buffer-pools/">&lt;p&gt;A database buffer pool manages fixed-size pages in memory, decides which ones to keep and which to evict, tracks dirty pages, and writes them back to disk on its own schedule.&lt;&#x2F;p&gt;
&lt;p&gt;That&#x27;s what the OS virtual memory system does. Page frames, page tables, eviction policies, dirty bit tracking, write-back. The database reimplements all of it. In user space.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-os-already-does-this&quot;&gt;the OS already does this&lt;&#x2F;h2&gt;
&lt;p&gt;The kernel manages physical memory in page frames (4KB). It maps virtual pages to physical frames through page tables. When memory is full, it evicts cold pages to disk. When a process touches an evicted page, it faults and the kernel loads it back. It tracks which pages are dirty and writes them back when it needs to.&lt;&#x2F;p&gt;
&lt;p&gt;This is the exact same problem a database has. The database has pages on disk. Some of them need to be in memory. Not all of them fit. The database needs to decide which pages to keep, which to evict, and when to write dirty ones back.&lt;&#x2F;p&gt;
&lt;p&gt;So why not just let the OS handle it? Map the database file with mmap and let the kernel manage everything. Some databases tried this. &lt;a href=&quot;https:&#x2F;&#x2F;yazeed1s.github.io&#x2F;posts&#x2F;mmap-databases&#x2F;&quot;&gt;It didn&#x27;t go well&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;why-databases-reimplement-it&quot;&gt;why databases reimplement it&lt;&#x2F;h2&gt;
&lt;p&gt;The OS page cache is general purpose. It has no concept of index pages vs temporary sort pages, no awareness that a range scan is about to need the next 50 pages, and no understanding that a dirty page has to hit the WAL before it hits the data file.&lt;&#x2F;p&gt;
&lt;p&gt;A buffer pool knows all of that.&lt;&#x2F;p&gt;
&lt;p&gt;The database builds its own page table: a hash map from &lt;code&gt;(file_id, page_number)&lt;&#x2F;code&gt; to a frame in the buffer pool. When a query needs a page, it looks up the hash map. If the page is there, it returns the pointer. If not, it picks a frame to evict, reads the page from disk into that frame, and updates the map.&lt;&#x2F;p&gt;
&lt;p&gt;Page fault, but in user space. Controlled entirely by the database.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-anatomy-of-a-buffer-pool&quot;&gt;the anatomy of a buffer pool&lt;&#x2F;h2&gt;
&lt;p&gt;The structure is simple:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Frame array&lt;&#x2F;strong&gt;: a fixed-size array of page-sized slots in memory (the &quot;RAM&quot; of the buffer pool).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Page table&lt;&#x2F;strong&gt;: a hash map from page ID to frame index (how the database translates a logical page reference into a memory location).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Eviction policy&lt;&#x2F;strong&gt;: decides which frame to reclaim when the pool is full (LRU, clock, LRU-K).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Dirty flag&lt;&#x2F;strong&gt;: each frame tracks whether its contents have been modified since it was read from disk.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Pin count&lt;&#x2F;strong&gt;: tracks how many operations are currently using a frame. A pinned page can&#x27;t be evicted (same idea as the kernel&#x27;s page reference count).&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;When a page is requested:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Check the page table. If the page is already in a frame, pin it and return the pointer.&lt;&#x2F;li&gt;
&lt;li&gt;If not, find a victim frame (eviction policy). If the victim is dirty, write it to disk first.&lt;&#x2F;li&gt;
&lt;li&gt;Read the requested page from disk into the victim frame.&lt;&#x2F;li&gt;
&lt;li&gt;Update the page table. Return the pointer.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;That&#x27;s page fault, find victim, write back if dirty, read page, update mapping. Same flow as an OS page fault handler, different layer.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;eviction-the-database-knows-more&quot;&gt;eviction: the database knows more&lt;&#x2F;h2&gt;
&lt;p&gt;The OS uses something like clock or a modified LRU. It works across all processes, all files, all pages. It has no application-level knowledge.&lt;&#x2F;p&gt;
&lt;p&gt;A database can do better because it knows the access patterns.&lt;&#x2F;p&gt;
&lt;p&gt;A sequential scan will touch every page once. An LRU policy would fill the cache with scan pages and evict hot index pages. PostgreSQL handles this by using a small ring buffer for sequential scans, so scan pages cycle through a handful of frames instead of polluting the whole pool.&lt;&#x2F;p&gt;
&lt;p&gt;A B+tree lookup traverses root, internal, then leaf. The root page is accessed on every lookup. It should basically never be evicted. LRU handles this naturally, but a database can also pin critical pages explicitly.&lt;&#x2F;p&gt;
&lt;p&gt;Prefetching works better too. The database knows it&#x27;s doing a range scan on a B+tree. It can issue async reads for the next few leaf pages before it needs them. The OS page cache can&#x27;t do this because it only sees physical file offsets, not logical access patterns.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;dirty-pages-and-write-back&quot;&gt;dirty pages and write-back&lt;&#x2F;h2&gt;
&lt;p&gt;This is where the difference matters most.&lt;&#x2F;p&gt;
&lt;p&gt;The OS can flush a dirty page to disk whenever it wants. That&#x27;s fine for normal files. For a database, it&#x27;s dangerous. If a modified data page hits disk before the corresponding WAL record, crash recovery breaks. This is the write-ahead logging rule: log first, data page second.&lt;&#x2F;p&gt;
&lt;p&gt;A buffer pool enforces this. Before writing a dirty page back to disk, it checks that the WAL has been flushed up to the page&#x27;s last modification LSN (Log Sequence Number). The page doesn&#x27;t go to disk until its log records are safe.&lt;&#x2F;p&gt;
&lt;p&gt;This is impossible with mmap. The kernel has no concept of WAL ordering or LSNs. It flushes when it feels like it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;o-direct-bypassing-the-os-page-cache&quot;&gt;O_DIRECT: bypassing the OS page cache&lt;&#x2F;h2&gt;
&lt;p&gt;Most serious databases open their files with &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt;. This tells the kernel to skip its own page cache entirely. Reads and writes go straight between the database&#x27;s buffer pool and the disk.&lt;&#x2F;p&gt;
&lt;p&gt;Without &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt;, you&#x27;d have the data in two places: once in the database&#x27;s buffer pool and once in the OS page cache. Double the memory usage for no benefit. The database already manages caching. The OS cache is redundant.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;O_DIRECT&lt;&#x2F;code&gt; also gives the database precise control over I&#x2F;O timing, no surprises from kernel write-back threads or memory pressure from the kernel evicting buffer pool pages.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;O_DIRECT&lt;&#x2F;code&gt; isn&#x27;t free to use. It requires buffers to be aligned to the filesystem block size (usually 512 bytes or 4KB), and I&#x2F;O sizes must also be aligned. If you get the alignment wrong, the syscall fails with EINVAL. This is why most databases that use &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt; implement their own aligned allocation routines.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;PostgreSQL is an exception. It uses the OS page cache (buffered I&#x2F;O) rather than &lt;code&gt;O_DIRECT&lt;&#x2F;code&gt;, and relies on &lt;code&gt;fsync&lt;&#x2F;code&gt; to force data to disk. It simplifies some things but means PostgreSQL competes with the OS for memory management control.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;it-s-the-same-problem-at-a-different-layer&quot;&gt;it&#x27;s the same problem at a different layer&lt;&#x2F;h2&gt;
&lt;p&gt;The parallel is almost exact:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;OS Virtual Memory&lt;&#x2F;th&gt;&lt;th&gt;Database Buffer Pool&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Physical page frame&lt;&#x2F;td&gt;&lt;td&gt;Buffer pool frame&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Page table (virtual to physical)&lt;&#x2F;td&gt;&lt;td&gt;Page table (page ID to frame)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Page fault handler&lt;&#x2F;td&gt;&lt;td&gt;Buffer pool miss handler&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Dirty bit in PTE&lt;&#x2F;td&gt;&lt;td&gt;Dirty flag per frame&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Reference count&lt;&#x2F;td&gt;&lt;td&gt;Pin count&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;kswapd (page reclaim)&lt;&#x2F;td&gt;&lt;td&gt;Eviction policy (LRU, clock)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Swap file&lt;&#x2F;td&gt;&lt;td&gt;Data file on disk&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;code&gt;write-back&lt;&#x2F;code&gt; flush&lt;&#x2F;td&gt;&lt;td&gt;WAL-ordered write-back&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The database takes this responsibility away from the OS because general-purpose policies don&#x27;t work for database workloads. Eviction needs access-pattern awareness. Write-back needs WAL ordering. Prefetching needs query-plan knowledge. The OS has none of this context.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;InnoDB (MySQL) uses a buffer pool with an LRU that splits into &quot;young&quot; and &quot;old&quot; sublists. New pages enter the old sublist, and only move to the young sublist if accessed again. This handles the scan-pollution problem.&lt;&#x2F;li&gt;
&lt;li&gt;PostgreSQL&#x27;s shared_buffers is its buffer pool. It uses a clock-sweep eviction policy.&lt;&#x2F;li&gt;
&lt;li&gt;SQLite in WAL mode maintains its own page cache but sits on top of the OS page cache (no O_DIRECT). It works because SQLite targets small-to-medium databases where double-caching isn&#x27;t expensive.&lt;&#x2F;li&gt;
&lt;li&gt;The buffer pool is one of the first things a database student builds. It&#x27;s simple in concept and brutal in the details (concurrency, latch ordering, I&#x2F;O scheduling).&lt;&#x2F;li&gt;
&lt;li&gt;Some databases are experimenting with letting the buffer pool manage allocation at finer granularity than pages. But pages have stuck around because they align with disk I&#x2F;O boundaries.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>What a Process Owns</title>
        <published>2025-10-19T00:00:00+00:00</published>
        <updated>2025-10-19T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/what-a-process-owns/"/>
        <id>https://yazeed1s.github.io/posts/what-a-process-owns/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/what-a-process-owns/">&lt;p&gt;A process isn&#x27;t just &quot;running code.&quot; It&#x27;s a container that owns a set of resources the kernel manages on its behalf, and understanding what those resources are helps explain why certain operations behave the way they do.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;address-space&quot;&gt;address space&lt;&#x2F;h2&gt;
&lt;p&gt;Each process gets its own virtual address space, which is a complete illusion of having memory all to itself. On 64-bit Linux a process gets a 128TB virtual space (48-bit addressing), split into user space (lower half) and kernel space (upper half, shared across all processes but inaccessible from user mode).&lt;&#x2F;p&gt;
&lt;p&gt;Inside this space you have the text segment (your compiled code, read-only and executable), the data segment for initialized globals, BSS for uninitialized globals (zeroed by the kernel), the heap growing upward from the end of BSS (managed by malloc&#x2F;brk&#x2F;mmap), the stack growing downward from near the top of user space, and memory-mapped regions scattered in between for shared libraries, mmap&#x27;d files, and anonymous mappings.&lt;&#x2F;p&gt;
&lt;p&gt;Two processes can have the same virtual address pointing to completely different physical memory because each process has its own page table. The kernel switches page tables on every context switch by writing a new value to CR3 on x86.&lt;&#x2F;p&gt;
&lt;p&gt;You can look at a process&#x27;s memory map through &lt;code&gt;&#x2F;proc&#x2F;&amp;lt;pid&amp;gt;&#x2F;maps&lt;&#x2F;code&gt;, which shows each VMA (Virtual Memory Area) with its address range, permissions, offset, device, inode, and pathname.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;file-descriptors&quot;&gt;file descriptors&lt;&#x2F;h2&gt;
&lt;p&gt;A file descriptor is an integer index into a per-process table of open files. When you call &lt;code&gt;open()&lt;&#x2F;code&gt;, the kernel creates an entry pointing to the underlying file object and returns the lowest available integer, which is why &lt;code&gt;stdin&lt;&#x2F;code&gt; is 0, &lt;code&gt;stdout&lt;&#x2F;code&gt; is 1, and &lt;code&gt;stderr&lt;&#x2F;code&gt; is 2 (they&#x27;re opened first).&lt;&#x2F;p&gt;
&lt;p&gt;Each process has its own file descriptor table, so fd 5 in one process is completely unrelated to fd 5 in another. The file descriptor carries the current read&#x2F;write position, flags (like &lt;code&gt;O_NONBLOCK&lt;&#x2F;code&gt;), and a reference to the underlying inode.&lt;&#x2F;p&gt;
&lt;p&gt;File descriptors aren&#x27;t just for files on disk: they represent pipes, sockets, eventfd, timerfd, signalfd, epoll instances, and even directories. The &quot;everything is a file&quot; philosophy means one interface for many types of I&#x2F;O.&lt;&#x2F;p&gt;
&lt;p&gt;There&#x27;s a per-process limit on open file descriptors (check with &lt;code&gt;ulimit -n&lt;&#x2F;code&gt;, default often 1024) and a system-wide limit. Leaking file descriptors is a common bug, especially in long-running services that open connections and forget to close them.&lt;&#x2F;p&gt;
&lt;p&gt;You can see all open file descriptors for a process in &lt;code&gt;&#x2F;proc&#x2F;&amp;lt;pid&amp;gt;&#x2F;fd&#x2F;&lt;&#x2F;code&gt;, where each entry is a symlink to the actual file, socket, or pipe.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;signals&quot;&gt;signals&lt;&#x2F;h2&gt;
&lt;p&gt;Each process has a signal disposition table describing what happens for each signal. The default action depends on the signal: &lt;code&gt;SIGTERM&lt;&#x2F;code&gt; terminates, &lt;code&gt;SIGSTOP&lt;&#x2F;code&gt; suspends, &lt;code&gt;SIGCHLD&lt;&#x2F;code&gt; is ignored by default. You can override most defaults with signal handlers, except for &lt;code&gt;SIGKILL&lt;&#x2F;code&gt; and &lt;code&gt;SIGSTOP&lt;&#x2F;code&gt; which can&#x27;t be caught or ignored.&lt;&#x2F;p&gt;
&lt;p&gt;A process also has a signal mask that blocks specific signals from being delivered (they queue up until unblocked), and a set of pending signals that have been sent but not yet handled.&lt;&#x2F;p&gt;
&lt;p&gt;Signal handling is per-process (signal disposition) but delivery can target specific threads. &lt;code&gt;kill(pid, sig)&lt;&#x2F;code&gt; sends to the process, &lt;code&gt;pthread_kill(tid, sig)&lt;&#x2F;code&gt; targets a thread.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;credentials&quot;&gt;credentials&lt;&#x2F;h2&gt;
&lt;p&gt;Every process has a set of UIDs and GIDs that determine what it can access. There are three sets: &lt;strong&gt;real&lt;&#x2F;strong&gt; (who actually started the process), &lt;strong&gt;effective&lt;&#x2F;strong&gt; (what&#x27;s checked for access), and &lt;strong&gt;saved&lt;&#x2F;strong&gt; (the previous effective, so you can drop and regain privileges).&lt;&#x2F;p&gt;
&lt;p&gt;Usually real and effective are the same, but setuid programs differ. When you run &lt;code&gt;passwd&lt;&#x2F;code&gt;, the file has the setuid bit set, so the effective UID becomes root (owner of the file) while the real UID stays yours. The process can access &lt;code&gt;&#x2F;etc&#x2F;shadow&lt;&#x2F;code&gt; because effective UID is root, but the program knows who actually called it through the real UID.&lt;&#x2F;p&gt;
&lt;p&gt;Groups work the same way, and there&#x27;s also a supplementary group list for all the groups you belong to. All of this is stored in the process&#x27;s credential structure in the kernel.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;scheduling-context&quot;&gt;scheduling context&lt;&#x2F;h2&gt;
&lt;p&gt;The kernel tracks scheduling metadata for each process: its priority (nice value from -20 to 19, and real-time priority if applicable), which scheduling class it belongs to (CFS for normal processes, FIFO or round-robin for real-time), how much CPU time it has consumed, when it last ran, and which CPU it ran on.&lt;&#x2F;p&gt;
&lt;p&gt;CFS (Completely Fair Scheduler) on Linux uses a virtual runtime to track how much CPU time a process has received relative to others, and it always picks the process with the lowest virtual runtime to run next. The nice value adjusts the weight: a lower nice value means more weight, which means the virtual runtime advances more slowly, which means more CPU time.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;namespaces-and-cgroups&quot;&gt;namespaces and cgroups&lt;&#x2F;h2&gt;
&lt;p&gt;In containerized environments, processes also own namespace memberships and cgroup associations. Namespaces give a process an isolated view of system resources: PID namespace makes it think its PID is 1, network namespace gives it a separate network stack, mount namespace gives it a different filesystem view.&lt;&#x2F;p&gt;
&lt;p&gt;Cgroups limit how much resource a process (or group of processes) can use: CPU quota, memory limit, I&#x2F;O bandwidth, and number of PIDs. These are the building blocks of containers.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-task-struct&quot;&gt;the task_struct&lt;&#x2F;h2&gt;
&lt;p&gt;Internally, the kernel represents all of this in a single structure called &lt;code&gt;task_struct&lt;&#x2F;code&gt;, which on Linux is a large struct (several kilobytes) containing or pointing to everything described above: the memory descriptor for address space, the files struct for file descriptors, the signal handler table, credentials, scheduling state, namespace pointers, cgroup references, and many other fields.&lt;&#x2F;p&gt;
&lt;p&gt;Every running thread has a &lt;code&gt;task_struct&lt;&#x2F;code&gt;. A single-threaded process has one. A multi-threaded process has one per thread, and they share the memory descriptor, file table, and signal handlers through pointers to the same structures.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&#x2F;proc&#x2F;&amp;lt;pid&amp;gt;&#x2F;status&lt;&#x2F;code&gt; gives a summary of most of these: UIDs, GIDs, threads, memory usage, signal masks, context switch counts&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;&#x2F;proc&#x2F;&amp;lt;pid&amp;gt;&#x2F;maps&lt;&#x2F;code&gt; for address space layout&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;&#x2F;proc&#x2F;&amp;lt;pid&amp;gt;&#x2F;fd&#x2F;&lt;&#x2F;code&gt; for open file descriptors&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;&#x2F;proc&#x2F;&amp;lt;pid&amp;gt;&#x2F;cgroup&lt;&#x2F;code&gt; for cgroup membership&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;ls -la &#x2F;proc&#x2F;&amp;lt;pid&amp;gt;&#x2F;ns&#x2F;&lt;&#x2F;code&gt; shows namespace memberships&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;strace -p &amp;lt;pid&amp;gt;&lt;&#x2F;code&gt; lets you watch syscalls in real time&lt;&#x2F;li&gt;
&lt;li&gt;A zombie process has exited but its parent hasn&#x27;t called &lt;code&gt;wait()&lt;&#x2F;code&gt;, so the task_struct stays around holding the exit status&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Interrupts, Traps, and the Kernel Boundary</title>
        <published>2025-02-18T00:00:00+00:00</published>
        <updated>2025-02-18T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/interrupts-traps/"/>
        <id>https://yazeed1s.github.io/posts/interrupts-traps/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/interrupts-traps/">&lt;p&gt;Your app might call a syscall, a packet might arrive on the network card, or the timer might fire; all of these interrupt normal execution, but they&#x27;re not the same thing. The terminology gets confusing because people use interrupt, trap, and exception loosely.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;user-space-vs-kernel-space&quot;&gt;user space vs kernel space&lt;&#x2F;h2&gt;
&lt;p&gt;First the basics: modern CPUs have privilege levels called rings on x86, where ring 0 is the most privileged (kernel) and ring 3 is least privileged (user applications).&lt;&#x2F;p&gt;
&lt;p&gt;Your application runs in ring 3 and can execute normal instructions, access its own memory, and do math, but it cannot access hardware directly, read&#x2F;write arbitrary memory addresses, or execute privileged instructions (like changing page tables or disabling interrupts). The kernel runs in ring 0 and can do all of those things.&lt;&#x2F;p&gt;
&lt;p&gt;When you call &lt;code&gt;read()&lt;&#x2F;code&gt; to read from a file, your code can&#x27;t just talk to the disk controller, it has to ask the kernel. The CPU has to switch from ring 3 to ring 0, do the privileged work, then switch back.&lt;&#x2F;p&gt;
&lt;p&gt;This switch is the kernel boundary, and crossing it costs you: register save&#x2F;restore, privilege change, and sometimes cache disruption. That&#x27;s why syscalls aren&#x27;t free.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;interrupts&quot;&gt;interrupts&lt;&#x2F;h2&gt;
&lt;p&gt;An interrupt is a signal from hardware that says &quot;stop what you&#x27;re doing, I need attention.&quot; Examples are a keyboard telling you a key was pressed, a network card saying a packet arrived, a timer saying your time slice is up, or a disk controller saying that read you asked for is done.&lt;&#x2F;p&gt;
&lt;p&gt;Interrupts are asynchronous, they happen whenever the hardware needs attention regardless of what the CPU is currently doing. You could be in the middle of a for loop and suddenly an interrupt fires.&lt;&#x2F;p&gt;
&lt;p&gt;When an interrupt happens, the CPU stops executing the current instruction stream, saves the current state (registers, instruction pointer, flags), looks up the interrupt handler in the Interrupt Descriptor Table (IDT), jumps to that handler (now in ring 0), the handler does its work, and then the handler returns and the CPU restores state and continues where it left off. The key thing is that the currently running process doesn&#x27;t trigger this, it just happens to it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;traps&quot;&gt;traps&lt;&#x2F;h2&gt;
&lt;p&gt;A trap is a synchronous exception triggered by the currently running code, and it&#x27;s intentional. The main example is syscalls: when you call &lt;code&gt;read()&lt;&#x2F;code&gt;, the C library eventually executes a special instruction (&lt;code&gt;syscall&lt;&#x2F;code&gt; on x86-64, &lt;code&gt;int 0x80&lt;&#x2F;code&gt; on older x86) that deliberately triggers a trap.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;User code calls read()&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;  -&amp;gt; libc wrapper&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    -&amp;gt; syscall instruction (trap into kernel)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;      -&amp;gt; kernel syscall handler&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        -&amp;gt; returns to user space&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The difference from interrupts is that you asked for this, the code executing triggered it, and it happens at a specific point in your instruction stream, not randomly.&lt;&#x2F;p&gt;
&lt;p&gt;Other traps and exceptions include page faults (you accessed memory that isn&#x27;t mapped, which could be a bug or could be demand paging doing its job), division by zero (arithmetic error), invalid opcode (tried to execute garbage), and breakpoint traps (int 3 for debuggers). Some of these are errors that kill your process (division by zero), and some are handled so execution continues (page fault loads the page and then your load instruction retries).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-naming-confusion&quot;&gt;the naming confusion&lt;&#x2F;h2&gt;
&lt;p&gt;Different sources use these terms differently. Here&#x27;s how I think about it: &lt;strong&gt;interrupt&lt;&#x2F;strong&gt; means external, async, from hardware. &lt;strong&gt;Trap&lt;&#x2F;strong&gt; means internal, sync, intentional (syscalls). &lt;strong&gt;Exception&lt;&#x2F;strong&gt; means internal, sync, usually an error (page fault, div by zero). &lt;strong&gt;Fault&lt;&#x2F;strong&gt; is an exception that can be corrected (page fault) where the instruction retries. &lt;strong&gt;Abort&lt;&#x2F;strong&gt; is an unrecoverable error.&lt;&#x2F;p&gt;
&lt;p&gt;Some people use &quot;exception&quot; as the umbrella term for everything, some use &quot;interrupt&quot; for everything, and the Intel manual has its own definitions. It&#x27;s messy. What matters is understanding whether the trigger is external (hardware) or internal (executing code), and whether it&#x27;s expected (syscall) or unexpected (error).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-interrupt-descriptor-table&quot;&gt;the interrupt descriptor table&lt;&#x2F;h2&gt;
&lt;p&gt;The CPU needs to know where to jump for each interrupt or exception, and this is stored in the IDT, a table in memory that the kernel sets up at boot. Each entry has a handler address, what privilege level can trigger it, and a gate type (interrupt gate, trap gate).&lt;&#x2F;p&gt;
&lt;p&gt;For hardware interrupts, the entries point to kernel interrupt handlers, and for the syscall trap, it points to the syscall entry point. When an interrupt fires, the CPU uses the interrupt number as an index into the IDT, checks privilege, switches to ring 0 if needed, and jumps to the handler address.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;hardware-interrupts-in-more-detail&quot;&gt;hardware interrupts in more detail&lt;&#x2F;h2&gt;
&lt;p&gt;When a device needs attention, it signals an interrupt request (IRQ), and on modern systems this goes through an interrupt controller (APIC). The kernel has to acknowledge the interrupt, figure out which device caused it, call the right driver&#x27;s handler, and tell the interrupt controller we&#x27;re done. Handling needs to be fast because interrupts are disabled (or that IRQ is masked) while you&#x27;re in the handler, and if you take too long you miss other interrupts.&lt;&#x2F;p&gt;
&lt;p&gt;Linux splits this into top half and bottom half: the &lt;strong&gt;top half&lt;&#x2F;strong&gt; runs in interrupt context, does the minimum work, and schedules the bottom half. The &lt;strong&gt;bottom half&lt;&#x2F;strong&gt; runs later with interrupts enabled and does the real work (softirqs, tasklets, workqueues). For example, a network card interrupt&#x27;s top half grabs the packet from hardware, queues it, and schedules the bottom half, which then processes the packet up the network stack.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;syscall-cost&quot;&gt;syscall cost&lt;&#x2F;h2&gt;
&lt;p&gt;Crossing the kernel boundary isn&#x27;t free. You pay for saving and restoring registers, switching stacks (user stack to kernel stack), TLB and cache effects, and Spectre mitigations on modern kernels (KPTI, retpolines).&lt;&#x2F;p&gt;
&lt;p&gt;On a modern system, a syscall might take a few hundred nanoseconds, which doesn&#x27;t sound like much, but if you&#x27;re doing thousands per second it adds up. That cost is why people use batching (fewer syscalls, more work per call), io_uring (submit many I&#x2F;O requests with one syscall), and mmap (access files without read() syscalls).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;On x86-64, &lt;code&gt;syscall&lt;&#x2F;code&gt;&#x2F;&lt;code&gt;sysret&lt;&#x2F;code&gt; are faster than the old &lt;code&gt;int 0x80&lt;&#x2F;code&gt; method&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;&#x2F;proc&#x2F;interrupts&lt;&#x2F;code&gt; shows interrupt counts per CPU&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;perf stat&lt;&#x2F;code&gt; can count context switches and syscalls&lt;&#x2F;li&gt;
&lt;li&gt;NMI (Non-Maskable Interrupt) can&#x27;t be disabled, used for profiling and panic&lt;&#x2F;li&gt;
&lt;li&gt;The timer interrupt is what makes preemptive multitasking work&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Context Switches: What Actually Happens</title>
        <published>2024-11-18T00:00:00+00:00</published>
        <updated>2024-11-18T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://yazeed1s.github.io/posts/context-switches/"/>
        <id>https://yazeed1s.github.io/posts/context-switches/</id>
        
        <content type="html" xml:base="https://yazeed1s.github.io/posts/context-switches/">&lt;p&gt;A context switch is what happens when the OS takes one process off the CPU and puts another one on. It happens constantly, thousands of times per second, and your programs never notice because the whole point is that it looks seamless.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;why-context-switches-exist&quot;&gt;why context switches exist&lt;&#x2F;h2&gt;
&lt;p&gt;CPUs don&#x27;t multitask on their own, they execute one instruction stream at a time (per core). The OS creates the illusion of parallelism by rapidly switching between processes, giving each one a time slice of maybe 1-10 milliseconds, executing its instructions, then saving its state and loading the next process&#x27;s state. Do this fast enough and it looks like everything runs at the same time.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-gets-saved&quot;&gt;what gets saved&lt;&#x2F;h2&gt;
&lt;p&gt;A process&#x27;s execution state lives in CPU registers, and when the OS switches away from a process it needs to save all of them: general purpose registers (rax, rbx, rcx on x86-64), the instruction pointer (rip, which says where execution was), the stack pointer (rsp), flags register, floating point and SIMD registers (SSE, AVX), and the memory mappings reference (CR3 on x86, which points to the page tables).&lt;&#x2F;p&gt;
&lt;p&gt;All of this gets saved to the process&#x27;s kernel data structure (the task_struct on Linux), and the incoming process&#x27;s saved state gets loaded into the CPU registers. The CPU then continues executing from wherever the new process left off.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-actually-happens-step-by-step&quot;&gt;what actually happens step by step&lt;&#x2F;h2&gt;
&lt;p&gt;The timer interrupt fires (or the process yields, or it blocks on I&#x2F;O), the CPU traps to the kernel, the scheduler picks the next process, the kernel saves current registers to the outgoing task_struct, loads registers from the incoming task_struct, switches the page tables by writing CR3 (which changes the entire virtual memory mapping), flushes TLB entries that are no longer valid, and returns to user space where the new process resumes as if nothing happened.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color-scheme: light dark; color: light-dark(#4B4B48, #D4D4C0); background-color: light-dark(#D7D5C3, #212121);&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;Process A running&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;  -&amp;gt; timer interrupt fires&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;  -&amp;gt; save A&amp;#39;s registers to A&amp;#39;s task_struct&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;  -&amp;gt; scheduler picks Process B&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;  -&amp;gt; load B&amp;#39;s registers from B&amp;#39;s task_struct&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;  -&amp;gt; switch page tables (CR3)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;  -&amp;gt; flush TLB&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;  -&amp;gt; Process B is now running&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The key thing is that the process doesn&#x27;t know. It saved no state, it called no function. The kernel did everything while the process wasn&#x27;t looking.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-cost&quot;&gt;the cost&lt;&#x2F;h2&gt;
&lt;p&gt;Context switches aren&#x27;t free. The direct cost is saving and restoring register state, which is maybe a few hundred nanoseconds. But the indirect cost is worse: the TLB gets flushed (partially or fully) because the new process has different page tables, so the first memory accesses after the switch take page table walks instead of TLB hits. The CPU caches (L1, L2) are now full of the old process&#x27;s data, and the new process suffers cache misses until it warms them up. Branch predictors trained on the old process&#x27;s code are useless for the new process.&lt;&#x2F;p&gt;
&lt;p&gt;These indirect costs can add up to several microseconds of effective penalty, and on workloads with many short-lived operations it can matter a lot.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;thread-switches-vs-process-switches&quot;&gt;thread switches vs process switches&lt;&#x2F;h2&gt;
&lt;p&gt;Threads within the same process share address space, so switching between them doesn&#x27;t require changing CR3 or flushing the TLB. That makes thread switches cheaper: you still save&#x2F;restore registers, but you skip the expensive page table swap and TLB invalidation.&lt;&#x2F;p&gt;
&lt;p&gt;This is one reason why multi-threaded servers outperform multi-process ones for high-concurrency workloads, fewer and cheaper context switches.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;voluntary-vs-involuntary&quot;&gt;voluntary vs involuntary&lt;&#x2F;h2&gt;
&lt;p&gt;A &lt;strong&gt;voluntary&lt;&#x2F;strong&gt; context switch happens when a process can&#x27;t continue: it calls &lt;code&gt;read()&lt;&#x2F;code&gt; and waits for disk, calls &lt;code&gt;sleep()&lt;&#x2F;code&gt;, waits on a mutex, or does any blocking operation. The process essentially says &quot;I have nothing to do, give the CPU to someone else.&quot;&lt;&#x2F;p&gt;
&lt;p&gt;An &lt;strong&gt;involuntary&lt;&#x2F;strong&gt; context switch happens when the scheduler preempts the process because its time slice expired (the timer interrupt fires and the scheduler decides it&#x27;s someone else&#x27;s turn), a higher-priority process becomes runnable, or load balancing moves the process to another core. You can see both types in &lt;code&gt;&#x2F;proc&#x2F;&amp;lt;pid&amp;gt;&#x2F;status&lt;&#x2F;code&gt; under &lt;code&gt;voluntary_ctxt_switches&lt;&#x2F;code&gt; and &lt;code&gt;nonvoluntary_ctxt_switches&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-triggers-a-switch&quot;&gt;what triggers a switch&lt;&#x2F;h2&gt;
&lt;p&gt;Most context switches come from I&#x2F;O waits (process blocks, voluntary), timer expiry (time slice used up, involuntary), synchronization (mutex, semaphore, condition variable), and inter-process communication (pipe, signal). A busy process that never blocks still gets switched out involuntarily when its time slice expires.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;notes&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;perf stat&lt;&#x2F;code&gt; shows context switch counts for a command&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;&#x2F;proc&#x2F;&amp;lt;pid&amp;gt;&#x2F;status&lt;&#x2F;code&gt; shows voluntary and involuntary counts per process&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;vmstat&lt;&#x2F;code&gt; shows system-wide context switches per second (cs column)&lt;&#x2F;li&gt;
&lt;li&gt;On a typical desktop system you might see 10,000-50,000 context switches per second&lt;&#x2F;li&gt;
&lt;li&gt;PCID (Process Context ID) on modern x86 lets the TLB keep entries from multiple processes, reducing the flush cost&lt;&#x2F;li&gt;
&lt;li&gt;Kernel preemption means the kernel itself can be context-switched mid-operation (when configured with &lt;code&gt;PREEMPT&lt;&#x2F;code&gt;)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
</feed>
